

            <!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html lang='zh-CN' xml:lang='zh-CN' xmlns='http://www.w3.org/1999/xhtml'>
<head>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"/>
    <meta http-equiv="Content-Language" content="zh-CN"/>
    <title>naughty的博客</title>
    <style>
        body {font:12px/21px Tahoma,"Microsoft Yahei",Geneva,sans-serif;}
        h1 a {color:#40AA53;}
        h1 a,h2 a{text-decoration:none;}
        .backupDesc em {font-style:normal;}
        .blog {margin:0 0 20px 0;border:1px solid #ccc;background:#ffc;padding:10px;}
        .blog h2 {border-bottom:1px solid #ccc;padding:0 0 10px 0;}
        .blog h2 a.top {float:right;font-weight:normal;font-size:9pt;}
        .blog .outline {background:#fff;border:1px solid #eee;padding:10px;color:#666;}

        .commentList {margin:20px 0 0 0;}
        .comment {margin:0 0 10px 0; border:1px solid #ddd; background:#ffe;padding:5px;}
        .comment .author {float:left;font-weight:bold;}
        .comment .date {float:left;margin:0 0 0 10px;}
        .comment .content {clear:left;margin:5px 0 0 0;}
    </style>
</head>
<body>
<h1><a name='top' href="https://my.oschina.net/taogang/blog">naughty的博客@OSCHINA</a></h1>
<p class='backupDesc'>共有<em>66</em>篇文章，备份时间：2019-04-04 06:09:16</p>
<ol>
            <li><a href="#blog_217616">程序员量子力学-海森堡式BUG</a></li>
            <li><a href="#blog_217715">用编程的思路模拟解决脑筋急转弯问题</a></li>
            <li><a href="#blog_220058">用可视化来讲故事</a></li>
            <li><a href="#blog_220552">数据可视化中的视觉属性</a></li>
            <li><a href="#blog_224658">软件中的质量属性（一）</a></li>
            <li><a href="#blog_225513">软件中的质量属性（二）</a></li>
            <li><a href="#blog_264351">Python 与 Javascript 之比较</a></li>
            <li><a href="#blog_267707">探索Javascript异步编程</a></li>
            <li><a href="#blog_268505">神奇的阿基米德螺线</a></li>
            <li><a href="#blog_271060">使用Python抓取欧洲足球联赛数据</a></li>
            <li><a href="#blog_271193">这些年，我穿过的那些队服</a></li>
            <li><a href="#blog_279402">用Python做单变量数据集的异常点分析</a></li>
            <li><a href="#blog_285605">在Mac OS X上构建wget来抓取静态网站内容</a></li>
            <li><a href="#blog_286955">在ipython notebook中调用ggplot的三种不同的方法</a></li>
            <li><a href="#blog_299955">利用Splunk做应用程序的性能分析</a></li>
            <li><a href="#blog_350255">用HTML5构建一个流程图绘制工具</a></li>
            <li><a href="#blog_383241">使用sphinx快速生成Python API 文档</a></li>
            <li><a href="#blog_386077">Python 并行分布式框架之 Celery</a></li>
            <li><a href="#blog_386512">Python 并行分布式框架之 PP</a></li>
            <li><a href="#blog_389293">使用Python进行并发编程</a></li>
            <li><a href="#blog_403939">用Python抓取亚马逊云（AWS）的日志（CloudTrail）数据</a></li>
            <li><a href="#blog_410864">使用Python进行分布式系统协调 （ZooKeeper，Consul， etcd ）</a></li>
            <li><a href="#blog_485586">谈谈程序员面试之刷题</a></li>
            <li><a href="#blog_497932">美丽的曼陀罗曲线</a></li>
            <li><a href="#blog_524385">大数据系统数据采集产品的架构分析</a></li>
            <li><a href="#blog_630382">在云上的机器学习 </a></li>
            <li><a href="#blog_630632">使用开源软件快速搭建数据分析平台</a></li>
            <li><a href="#blog_652287">Spark Python 快速体验</a></li>
            <li><a href="#blog_652501">100 open source Big Data architecture papers</a></li>
            <li><a href="#blog_657253">Spark 机器学习实践 ：Iris数据集的分类</a></li>
            <li><a href="#blog_686568">用JS实现简单的神经网络算法</a></li>
            <li><a href="#blog_693877"> 一道据说是苹果的面试题</a></li>
            <li><a href="#blog_777780">Go语言的类IPython 交互式编程界面</a></li>
            <li><a href="#blog_778136">容器集群管理平台的比较</a></li>
            <li><a href="#blog_868767">持续交付的架构成熟度模型</a></li>
            <li><a href="#blog_877437">微服务中的模式和反模式</a></li>
            <li><a href="#blog_877726">微服务部署方式的演进</a></li>
            <li><a href="#blog_885550">开源中国用户分析</a></li>
            <li><a href="#blog_890269">使用开源Echarts为Splunk打造类似语法驱动的分析可视化</a></li>
            <li><a href="#blog_983586">大数据搜索选开源还是商业软件？ElasticSearch 对比 Splunk </a></li>
            <li><a href="#blog_1530258">把代码执行演示嵌在你的PPT中</a></li>
            <li><a href="#blog_1544709">图解机器学习</a></li>
            <li><a href="#blog_1579204">用Python实现一个大数据搜索引擎</a></li>
            <li><a href="#blog_1608835">使用Heapster和Splunk监控Kubernetes运行性能</a></li>
            <li><a href="#blog_1627590">一个利用Tensorflow求解几何问题的例子</a></li>
            <li><a href="#blog_1790365">在浏览器中进行深度学习：TensorFlow.js (一）基本概念</a></li>
            <li><a href="#blog_1793835">在浏览器中进行深度学习：TensorFlow.js (二）第一个模型，线性回归</a></li>
            <li><a href="#blog_1797230">在浏览器中进行深度学习：TensorFlow.js (三）更多的基本模型</a></li>
            <li><a href="#blog_1803051">在浏览器中进行深度学习：TensorFlow.js (四）用基本模型对MNIST数据进行识别</a></li>
            <li><a href="#blog_1807936">在浏览器中进行深度学习：TensorFlow.js (五）构建一个神经网络</a></li>
            <li><a href="#blog_1809904">基于容器应用设计的原则，模式和反模式</a></li>
            <li><a href="#blog_1811573">用500行纯前端代码在浏览器中构建一个Tableau</a></li>
            <li><a href="#blog_1812524">在浏览器中进行深度学习：TensorFlow.js (六）构建一个卷积网络 Convolutional Network </a></li>
            <li><a href="#blog_1819665">Pyflow : 一个基于工作流的编程模型（Flow Based Programing) 工具</a></li>
            <li><a href="#blog_1822915">用KOps在AWS上部署和管理Kubernetes</a></li>
            <li><a href="#blog_1860421">机器学习管理平台 MLFlow </a></li>
            <li><a href="#blog_1940597">读书笔记：A Philosophy of Software Design （一）</a></li>
            <li><a href="#blog_1940954">读书笔记：A Philosophy of Software Design （二）</a></li>
            <li><a href="#blog_2032324">在浏览器中进行深度学习：TensorFlow.js (七）递归神经网络 （RNN）</a></li>
            <li><a href="#blog_2052152">轻松扩展你的机器学习能力 ： Kubeflow</a></li>
            <li><a href="#blog_2222908">谈谈机器学习模型的部署</a></li>
            <li><a href="#blog_2248506">在浏览器中进行深度学习：TensorFlow.js (八）生成对抗网络 （GAN）</a></li>
            <li><a href="#blog_2990742">重构机器学习算法的知识体系 - 《终极算法》读书笔记</a></li>
            <li><a href="#blog_3011686">自动机器学习简述（AutoML）</a></li>
            <li><a href="#blog_3014119">是时候把你的Python2应用迁移到Python3了</a></li>
            <li><a href="#blog_3023996">在浏览器中进行深度学习：TensorFlow.js (九）训练词向量 Word Embedding</a></li>
    </ol>
<div class='blogList'>
    
        <div class='blog'>
            <h2><a href="#top" class='top'>回到顶部</a><a name="blog_217616" href="https://my.oschina.net/taogang/blog/217616">程序员量子力学-海森堡式BUG</a></h2>
            <div class='outline'>
                <div class='date'>时间：2014-04-04 09:26:45</div>
                <div class='catalog'>分类：工作日志</div>
                                                                            </div>
            <div class='content'>今天在阅读《The Pragmatic Programmer》的时候发现书中提到了Heisenbug让我想起来了多年以前在开发中碰到的一个海森堡式的BUG。海森堡是德国著名的物理学家,量子力学的创始人之一，“哥本哈根学派”的代表人物。如果大家对这段历史或者物理原理不清楚的话，推荐大家阅读《上帝掷骰子吗》,非常好看的一本关于量子力学历史的科普书。海森堡对量子力学的一个重要贡献是提出了著名的“不确定性原理”（又称“海森堡测不准原理”），在一个量子力学系统中，一个运动粒子的位置和它的动量不可被同时确定。这是因为要观察就必须用光击中被观察的粒子，通过反射的光波来确定粒子的位置。然而观测所发出的光必然会影响被观测的粒子。海森堡式的BUG是该物理原理在软件开发中的一个表现，当然这是一个类比，它和量子力学没有半毛钱的关系。举个例子吧，当程序出现了一个bug，程序猿为了找到bug出现的原因，在出现bug的代码处加入了一条打印语句，想要了解在出现bug的时候本地变量和参数的值以便确定bug出现的原因。然而奇迹出现了，当加入打印语句后，bug消失了。哦，买糕的，这条打印语句不就是射向粒子的那束光么？下面，我来给大家分享一下我遇到的那个Heisenbug。当时我们团队做一个商用软件从Windows平台到Linux平台的移植，因为该软件早期是以Mac为平台编写的（那是Mac还没有像现在这样风靡），所以我们的移植平没有遇到太大的困难，主要的工作是包括：用QT来实现所有的UI组件支持Linux下的数据库连接移植C++非标准的使用Unicode的支持项目的前期非常顺利，然而就是当项目接近尾声的时候，一个可怕的海森堡式bug出现了，产品的release版本会出现一些随机的错误，这些错误都很奇怪，并没有一个统一的表现。而这些错误在debug版本中完全不会出现。为了解决这个问题，在boss的率领下我们开始加班除虫。在当时公司加班还是很少见的，一方面欧洲公司以人为本，并不鼓励加班；另一方面我们团队，大家能力都很强，不需要加班来解决问题。然而这个bug属于必须要解决的问题，加班在所难免。可是要解决问题这个问题还真是不容易，因为问题不会出现在debug版本中，所以用gdb加断点单步调试的方式根本没有用。所以调试的方式就只剩下了打日志。经过我一天不断的加入日志，运行测试，加入日志，运行测试…… 最终终于找到了问题的原因。一般情况下，找到问题的原因比解决问题要困难的多，当你发现问题的原因后，解决的方法就像秃子头上的虱子。原来，这个问题是由于我们生成软件版本的方法所造成的。为了在build的时候自动的生成软件的版本，我们的天才工程师想了一个好主意，那就是把版本信息写入Linux的可执行文件中（大家可以参考ELF的规范）。我们的软件可以从文件中读出这个版本，让后通过API告诉任何组件或者使用者当前的软件版本是什么。不得不承认这是一个非常有创意的好主意，但是这也是引起错误的主要原因。由于疏忽，在写入版本信息的时候，并没有严格按照规范来写，所以其实版本信息超出了本应写入的位置，也就是说写越界了，破坏了程序数据，所以造成了release版本中的错误。可是为什么debug版本没有问题呢？原因大概是debug版本由于加入了大量的代码和调试信息，版本信息很可能只是覆盖了调试信息而没有影响程序的正常运行。这么多年过去了，很多细节我已经记不清楚啦，然而对于这个海森堡式的bug，现在想想，我们应该可以做的更好：设计评审我们鼓励创新，当引入一个新的解决方案，尤其是别人很少或根本没有使用过得方案时，我们必须非常小心。因为没有人碰到同样的问题。认真的设计评审或许可以有所帮助。代码审查如果当时我们能够认真的进行代码审查，也许我们能发现这个问题。但也许即使进行代码审查，也不一定能发现这个错误。团队中没有几个人能够搞懂ELF的规范单元测试单元测试能帮助发现这个问题么？也许可以，当时我们并没有就这个功能引入任何的单元测试。最后引用一个我发现的软件开发中的测不准原理：软件的需求和完成时间不可能同时测准，如果你清楚需求是什么，那么你就不知道什么时候能完成；如果你知道什么时候要做完，那你根本就不知道需求是什么。&nbsp;</div>
                    </div>
    
        <div class='blog'>
            <h2><a href="#top" class='top'>回到顶部</a><a name="blog_217715" href="https://my.oschina.net/taogang/blog/217715">用编程的思路模拟解决脑筋急转弯问题</a></h2>
            <div class='outline'>
                <div class='date'>时间：2014-04-04 14:15:19</div>
                <div class='catalog'>分类：工作日志</div>
                                                                            </div>
            <div class='content'>前些日子看了可汗学院的这个关于诚实人和说谎者的脑筋急转弯问题，我觉得如果能用程序来模拟，那一定很有趣。  这个题目是这样的，有两扇门，一扇通往天堂，一扇通往地狱，你要做出选择打开那扇门。门口各有一个人，他们都知道门后面的情况，其中一个总是说实话，而另一个总是撒谎。你可以问他们问题。要怎样问问题，才能做出正确的选择？  在揭开答案之前，我们先看看我是如何用JavaScript来模拟这个问题的。 function&nbsp;Game()&nbsp;{&nbsp;&nbsp;var&nbsp;destinations&nbsp;=&nbsp;[&quot;Heaven&quot;,&nbsp;&quot;Hell&quot;];&nbsp;&nbsp;var&nbsp;gates&nbsp;=&nbsp;{},&nbsp;persons&nbsp;=&nbsp;{};&nbsp;&nbsp;function&nbsp;Gate()&nbsp;{&nbsp;&nbsp;&nbsp;&nbsp;var&nbsp;val&nbsp;=&nbsp;undefined;&nbsp;&nbsp;&nbsp;&nbsp;return&nbsp;{&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;value:&nbsp;function(_)&nbsp;{&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;if&nbsp;(!arguments.length)&nbsp;{&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;return&nbsp;val;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;}&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;val&nbsp;=&nbsp;_;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;},&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;open:&nbsp;function()&nbsp;{&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;if&nbsp;(val&nbsp;===&nbsp;destinations[0])&nbsp;{&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;console.log(&quot;Happy&nbsp;Ending!&quot;);&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;}&nbsp;else&nbsp;{&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;console.log(&quot;You&nbsp;Die!&quot;);&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;}&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;}&nbsp;&nbsp;&nbsp;&nbsp;}&nbsp;&nbsp;}&nbsp;&nbsp;function&nbsp;Behavior(isLier)&nbsp;{&nbsp;&nbsp;&nbsp;&nbsp;var&nbsp;tellTruth&nbsp;=&nbsp;function(info)&nbsp;{&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;return&nbsp;info;&nbsp;&nbsp;&nbsp;&nbsp;};&nbsp;&nbsp;&nbsp;&nbsp;var&nbsp;tellLie&nbsp;=&nbsp;function(info)&nbsp;{&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;var&nbsp;index&nbsp;=&nbsp;destinations.indexOf(info);&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;if&nbsp;(index&nbsp;===&nbsp;0)&nbsp;{&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;return&nbsp;destinations[1];&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;}&nbsp;else&nbsp;{&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;return&nbsp;destinations[0];&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;}&nbsp;&nbsp;&nbsp;&nbsp;};&nbsp;&nbsp;&nbsp;&nbsp;return&nbsp;isLier&nbsp;?&nbsp;tellLie&nbsp;:&nbsp;tellTruth;&nbsp;&nbsp;}&nbsp;&nbsp;function&nbsp;Person(gate,&nbsp;behavior)&nbsp;{&nbsp;&nbsp;&nbsp;&nbsp;return&nbsp;{&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;ask:&nbsp;function()&nbsp;{&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;return&nbsp;behavior.call(this,&nbsp;gate.value());&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;}&nbsp;&nbsp;&nbsp;&nbsp;}&nbsp;&nbsp;}&nbsp;&nbsp;function&nbsp;Init()&nbsp;{&nbsp;&nbsp;&nbsp;&nbsp;gates.A&nbsp;=&nbsp;Gate();&nbsp;&nbsp;&nbsp;&nbsp;gates.B&nbsp;=&nbsp;Gate();&nbsp;&nbsp;&nbsp;&nbsp;if&nbsp;(Math.random()&nbsp;&gt;&nbsp;0.5)&nbsp;{&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;gates.A.value(destinations[0]);&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;gates.B.value(destinations[1]);&nbsp;&nbsp;&nbsp;&nbsp;}&nbsp;else&nbsp;{&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;gates.A.value(destinations[1]);&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;gates.B.value(destinations[0]);&nbsp;&nbsp;&nbsp;&nbsp;}&nbsp;&nbsp;&nbsp;&nbsp;if&nbsp;(Math.random()&nbsp;&gt;&nbsp;0.5)&nbsp;{&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;persons.A&nbsp;=&nbsp;Person(gates.A,&nbsp;Behavior(true));&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;persons.B&nbsp;=&nbsp;Person(gates.B,&nbsp;Behavior(false));&nbsp;&nbsp;&nbsp;&nbsp;}&nbsp;else&nbsp;{&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;persons.A&nbsp;=&nbsp;Person(gates.A,&nbsp;Behavior(false));&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;persons.B&nbsp;=&nbsp;Person(gates.B,&nbsp;Behavior(true));&nbsp;&nbsp;&nbsp;&nbsp;}&nbsp;&nbsp;}&nbsp;&nbsp;Init();&nbsp;&nbsp;return&nbsp;{&nbsp;&nbsp;&nbsp;&nbsp;open:&nbsp;function(gateName)&nbsp;{&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;gates[gateName].open();&nbsp;&nbsp;&nbsp;&nbsp;},&nbsp;&nbsp;&nbsp;&nbsp;ask:&nbsp;function(personName)&nbsp;{&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;return&nbsp;persons[personName].ask();&nbsp;&nbsp;&nbsp;&nbsp;}&nbsp;&nbsp;}}  程序主要有以下方法：   GateGate方法负责创建们，每扇门提供两个方法， value方法负责读写们的内容，open方法检查开门后的结果。如果是天堂，则打印“Happy Ending”， 游戏成功；如果是地狱则打印“You Die”，游戏失败。  BehaviorBehavior是对说谎和说实话这两种行为的抽象。返回两个不同的方法，说谎的行为总是返回和原本信息不一致的信息，而说实话则返回原本的信息。  PersonPerson方法负责创建人，并关联人所在的门，指定人的行为。  InitInit方法随机初始化门和人  GameGame方法是最外层的闭包，他暴露出游戏者可以调用的两个方法，问问题和开门。因为Person和Gate都在闭包里所以游戏者无法获得任何相关的信息，只能通过ask方法来问问题。  调用以下方法来运行游戏： var&nbsp;g&nbsp;=&nbsp;Game();console.log(g.ask(&quot;A&quot;));console.log(g.ask(&quot;B&quot;));  因为A和B有一个人说谎，所以条用的结果是他们都说自己后面的门是“天堂”，或者都说是“地狱”。 于是无论打开A还是B，游戏者都只有50%的概率获胜。 好了，现在是揭晓答案的时候了，这道题，游戏者需要这样问，“如果问另外一个人你后面的门后是什么，他会怎么说？”会有两种情况：   问诚实人说谎者怎么回答他后面的门是什么，因为说谎者会说谎话，而诚实的人会原封不动的返回这个回答，所以提问者会得到错误的答案  问说谎者诚实人怎么回答他后面的门是什么，因为说谎者会说谎话，他会把诚实人的正确答案变成谎话回答，所以答案还是错误的。  那么我们要怎样修改我们的程序呢？  其实改动很简单，首先要增强Person方法  function&nbsp;Person(gate,&nbsp;behavior)&nbsp;{&nbsp;&nbsp;&nbsp;&nbsp;return&nbsp;{&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;ask:&nbsp;function(aGate)&nbsp;{&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;if(!aGate)&nbsp;{&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;return&nbsp;behavior.call(this,&nbsp;gate.value());&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;}&nbsp;else&nbsp;{&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;return&nbsp;behavior.call(this,&nbsp;aGate.value());&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;}&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;},&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;askBy:&nbsp;function(accordingTo)&nbsp;{&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;return&nbsp;behavior.call(this,&nbsp;accordingTo.ask(gate));&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;}&nbsp;&nbsp;&nbsp;&nbsp;}&nbsp;&nbsp;}  在Person方法中增强ask方法，暴露门的信息，也就是说每个人不但知道自己的门后的东西是什么，也能回答另一扇门后是什么。所以通过参数传递门的引用。这里我们可以看出，在前一版中假定人和门之间的耦合关系是不成立的，其实完全应该可以帮人和门解耦。 另外增加了askBy方法，这个方法会去问另一个人，我给你的门后面的信息，然后用自己的行为返回出去。 除了扩展Person，我们还要扩展Game返回的对象，增加新的提问方式： &nbsp;&nbsp;return&nbsp;{&nbsp;&nbsp;&nbsp;&nbsp;open:&nbsp;function(gateName)&nbsp;{&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;gates[gateName].open();&nbsp;&nbsp;&nbsp;&nbsp;},&nbsp;&nbsp;&nbsp;&nbsp;ask:&nbsp;function(personName)&nbsp;{&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;return&nbsp;persons[personName].ask();&nbsp;&nbsp;&nbsp;&nbsp;},&nbsp;&nbsp;&nbsp;&nbsp;askBy&nbsp;:&nbsp;function(personName,&nbsp;byName)&nbsp;{&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;return&nbsp;persons[personName].askBy(persons[byName]);&nbsp;&nbsp;&nbsp;&nbsp;}&nbsp;&nbsp;}  这样，新暴露的askBy方法就可以正确的找到错误的门了（听上去很绕口） 增强后的代码如下： function&nbsp;Game()&nbsp;{&nbsp;&nbsp;var&nbsp;destinations&nbsp;=&nbsp;[&quot;Heaven&quot;,&nbsp;&quot;Hell&quot;];&nbsp;&nbsp;var&nbsp;gates&nbsp;=&nbsp;{},&nbsp;persons&nbsp;=&nbsp;{};&nbsp;&nbsp;function&nbsp;Gate()&nbsp;{&nbsp;&nbsp;&nbsp;&nbsp;var&nbsp;val&nbsp;=&nbsp;undefined;&nbsp;&nbsp;&nbsp;&nbsp;return&nbsp;{&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;value:&nbsp;function(_)&nbsp;{&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;if&nbsp;(!arguments.length)&nbsp;{&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;return&nbsp;val;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;}&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;val&nbsp;=&nbsp;_;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;},&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;open:&nbsp;function()&nbsp;{&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;if&nbsp;(val&nbsp;===&nbsp;destinations[0])&nbsp;{&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;console.log(&quot;Happy&nbsp;Ending!&quot;);&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;}&nbsp;else&nbsp;{&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;console.log(&quot;You&nbsp;Die!&quot;);&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;}&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;}&nbsp;&nbsp;&nbsp;&nbsp;}&nbsp;&nbsp;}&nbsp;&nbsp;function&nbsp;Behavior(isLier)&nbsp;{&nbsp;&nbsp;&nbsp;&nbsp;var&nbsp;tellTruth&nbsp;=&nbsp;function(info)&nbsp;{&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;return&nbsp;info;&nbsp;&nbsp;&nbsp;&nbsp;};&nbsp;&nbsp;&nbsp;&nbsp;var&nbsp;tellLie&nbsp;=&nbsp;function(info)&nbsp;{&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;var&nbsp;index&nbsp;=&nbsp;destinations.indexOf(info);&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;if&nbsp;(index&nbsp;===&nbsp;0)&nbsp;{&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;return&nbsp;destinations[1];&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;}&nbsp;else&nbsp;{&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;return&nbsp;destinations[0];&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;}&nbsp;&nbsp;&nbsp;&nbsp;};&nbsp;&nbsp;&nbsp;&nbsp;return&nbsp;isLier&nbsp;?&nbsp;tellLie&nbsp;:&nbsp;tellTruth;&nbsp;&nbsp;}&nbsp;&nbsp;function&nbsp;Person(gate,&nbsp;behavior)&nbsp;{&nbsp;&nbsp;&nbsp;&nbsp;return&nbsp;{&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;ask:&nbsp;function(aGate)&nbsp;{&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;if(!aGate)&nbsp;{&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;return&nbsp;behavior.call(this,&nbsp;gate.value());&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;}&nbsp;else&nbsp;{&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;return&nbsp;behavior.call(this,&nbsp;aGate.value());&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;}&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;},&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;askBy:&nbsp;function(accordingTo)&nbsp;{&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;return&nbsp;behavior.call(this,&nbsp;accordingTo.ask(gate));&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;}&nbsp;&nbsp;&nbsp;&nbsp;}&nbsp;&nbsp;}&nbsp;&nbsp;function&nbsp;Init()&nbsp;{&nbsp;&nbsp;&nbsp;&nbsp;gates.A&nbsp;=&nbsp;Gate();&nbsp;&nbsp;&nbsp;&nbsp;gates.B&nbsp;=&nbsp;Gate();&nbsp;&nbsp;&nbsp;&nbsp;if&nbsp;(Math.random()&nbsp;&gt;&nbsp;0.5)&nbsp;{&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;gates.A.value(destinations[0]);&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;gates.B.value(destinations[1]);&nbsp;&nbsp;&nbsp;&nbsp;}&nbsp;else&nbsp;{&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;gates.A.value(destinations[1]);&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;gates.B.value(destinations[0]);&nbsp;&nbsp;&nbsp;&nbsp;}&nbsp;&nbsp;&nbsp;&nbsp;if&nbsp;(Math.random()&nbsp;&gt;&nbsp;0.5)&nbsp;{&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;persons.A&nbsp;=&nbsp;Person(gates.A,&nbsp;Behavior(true));&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;persons.B&nbsp;=&nbsp;Person(gates.B,&nbsp;Behavior(false));&nbsp;&nbsp;&nbsp;&nbsp;}&nbsp;else&nbsp;{&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;persons.A&nbsp;=&nbsp;Person(gates.A,&nbsp;Behavior(false));&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;persons.B&nbsp;=&nbsp;Person(gates.B,&nbsp;Behavior(true));&nbsp;&nbsp;&nbsp;&nbsp;}&nbsp;&nbsp;}&nbsp;&nbsp;Init();&nbsp;&nbsp;return&nbsp;{&nbsp;&nbsp;&nbsp;&nbsp;open:&nbsp;function(gateName)&nbsp;{&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;gates[gateName].open();&nbsp;&nbsp;&nbsp;&nbsp;},&nbsp;&nbsp;&nbsp;&nbsp;ask:&nbsp;function(personName)&nbsp;{&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;return&nbsp;persons[personName].ask();&nbsp;&nbsp;&nbsp;&nbsp;},&nbsp;&nbsp;&nbsp;&nbsp;askBy&nbsp;:&nbsp;function(personName,&nbsp;byName)&nbsp;{&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;return&nbsp;persons[personName].askBy(persons[byName]);&nbsp;&nbsp;&nbsp;&nbsp;}&nbsp;&nbsp;}}  调用以下代码来问新的问题：  var&nbsp;g&nbsp;=&nbsp;Game();console.log(g.askBy(&quot;A&quot;,&quot;B&quot;));console.log(g.askBy(&quot;B&quot;,&quot;A&quot;))；  游戏者只要打开和回答不一致的门就可以马上升入天堂！ 通过这个例子，我们发现对Behavior的抽象对进一步问题的扩展起到了很大的帮助。这也展现了基于Function编程的灵活性，不难想象，如果我们把说谎者和诚实人按照面向对象的方式来构造，而不是把说谎还是说真话作为行为注入到一般的人上，对新的问题的扩展将会比较困难。</div>
                            <div class='commentList'>
                    <h3>评论列表</h3>
                                    </div>
                    </div>
    
        <div class='blog'>
            <h2><a href="#top" class='top'>回到顶部</a><a name="blog_220058" href="https://my.oschina.net/taogang/blog/220058">用可视化来讲故事</a></h2>
            <div class='outline'>
                <div class='date'>时间：2014-04-08 17:22:27</div>
                <div class='catalog'>分类：数据可视化</div>
                                                                            </div>
            <div class='content'>在科学松鼠会的网站上有一篇漫画，来自imgur.com，讲述了 科学理论是如何建立的我用到d3.js把这个静态的图改成了动态效果，代码可见githubSee the Pen Occam's Razor cn by gangtao (@gangtao) on CodePen.注：这个其实主要就是奥卡姆剃刀原理，“若无必要，勿增实体”。附原图 ：http://marccortez.com/2012/06/28/evidence-theories-and-truth/</div>
                    </div>
    
        <div class='blog'>
            <h2><a href="#top" class='top'>回到顶部</a><a name="blog_220552" href="https://my.oschina.net/taogang/blog/220552">数据可视化中的视觉属性</a></h2>
            <div class='outline'>
                <div class='date'>时间：2014-04-10 10:04:33</div>
                <div class='catalog'>分类：数据可视化</div>
                                                                            </div>
            <div class='content'>Stephan Few&nbsp;是数据可视化领域里面数一数二的专家，他的几本书《Show Me The Numbers》,《Information Dashboard Design》和《Now You See It》都是非常优秀的关于数据可视化的书。 Stephan Few 和 Tableau 的合作非常紧密，在 Tableau 的数据可视化的设计中，处处可见 Stephan Few 的思想的影子。Stephan Few 在他的博客中毫不掩饰的提到 Tableau 是他唯一欣赏和敬重的公司。  以下是我阅读《Now You See It》的读书笔记：   Speaker Deck  SlideShare&nbsp;（国内需要翻墙）  在书中提到了视觉属性在可视化中的作用，视觉属性包含了以下分类：      形式     长，宽, 方向, 大小, 形状, 弯曲度, 围绕, 模糊    颜色     色调, 强度    空间位置     2-D 位置, 空间分组    动画     方向    每一种属性在表达数据时的表现力度是不一样的，大家可以通过这个图(Design By Stephane Few)来体会一下，点这里看动态图（Implemented By JavaScript + D3.js）</div>
                    </div>
    
        <div class='blog'>
            <h2><a href="#top" class='top'>回到顶部</a><a name="blog_224658" href="https://my.oschina.net/taogang/blog/224658">软件中的质量属性（一）</a></h2>
            <div class='outline'>
                <div class='date'>时间：2014-04-21 15:10:29</div>
                <div class='catalog'>分类：架构设计</div>
                                                                            </div>
            <div class='content'>开发高质量的软件是一件极具挑战的工作。其中一个重要的原因就是对于“质量”的定义各不相同，变化莫测。   杰拉尔德温伯格在他的四部曲巨作《质量软件管理》的第一卷第一章中就谈到了什么是质量以及质量的重要性。温伯格在书中讲了一个很有趣的故事。某软件企业每年都会根据所开发软件的质量对开发团队进行奖励，质量好的团队将会获得相应的嘉奖。可是如何评价软件的质量是一个令人头疼问题，于是他们采用了量化指标，根据用户反馈的defect的数量来定。某款软件质量超群，自推出至市场以来，只收到了一个defect。于是开发相应软件的团队获得了年度质量大奖！可是这款软件真的是一款高质量的软件么？看看这个defect的内容吧：“该软件无法安装！”   为了了解软件的质量是否满足要求，我们必须定义软件的质量属性（Quality Attributes）。同时质量属性也是影响软件架构的重要因素。软件架构主要由需求决定，需求有功能性的和非功能性的，其中非功能性的需求主要就是指质量属性，即各种“性”（“-ilities“）。wikipedia上列出了大概80种不同的质量属性，下面我们讨论一下其中最常见的属性。      可用性（Availability）   可用性是指系统正常工作的时间所占的比例。可用性会遇到系统错误，恶意攻击，高负载等问题的影响。   当年在一家存储公司开发管理软件，&quot;HA&quot;（High Availability）是一个经常被提及的性能属性。&quot;DU&quot;(Data Unavailable)和&quot;DL&quot;(Data Lost)都是非常严重的可用性问题。   可用性面临的主要问题有：         物理层失效：比如数据库服务器宕机，停电， 网络欠费被中国电信断网       恶意攻击：例如DOS（Deny of Service）攻击       软件的设计问题或BUG：比如错误的资源控制锁导致某个资源长期被占用，各种core dump， out of memery, out of stack。       升级或日常维护       针对这些问题，为了增加可用性，需要考虑        如何设计故障转移（failover），一般可以使用冗余来消除系统中的单点故障。可以是各种冷热备份，分布式系统。       如何设计在线升级。我当年在那家存储公司的一个主要责任就是做在线升级，因为存储设备有两个同时工作的单元构成，所以升级的过程简单说就是先升级第一个单元，然后再升级第二个单元。听上去非常简单，然而实际的升级过程非常复杂，在升级之前，会做非常多的健康检查，比如检查两个单元是不是都在正常工作，有没有坏的磁盘，存储设备的版本是不是满足要求，等等等等。特别是由于存储设备的软硬件型号复杂，还要考虑各种不同的运行环境，各种软件BUG，健康检查非常非常的复杂。当然大多数情况下，用户可以跳过健康检查，然而由此引发的升级失败，后果自负！       如何设计异常处理。异常处理是一个很大的话题，为了支持高可用性，我们应该如何处理异常呢？举个例子，一个网站要处理用户的订单，然而由于数据库服务器的故障，虽然前端的服务一切正常，可是订单无法处理。你会怎么处理这个数据库服务器异常的情况呢？大多数的程序员会在捕获异常的时候写日志，把异常状记录下来，然后在客户端的UI上显示一段谁也看不懂的异常代码。这样的异常处理其实跟没做差不多，甚至更糟。那么把异常代码翻译成用户可以看懂的语言是不是会好一点呢？也许会，如果你告诉用户因为你的数据库故障，请明天再来，可想而知用户会多么失望！好的异常处理是把用户的订单请求记录下来，发给人工处理，或者等待数据库恢复后自动处理，并告诉用户订单已经处理，并有可能迟延，请求得到用户的谅解。       如何应对不稳定的网络连接。            灵活性 （Flexibility）   灵活性是指系统是否能够很容易的适应环境和需求的变化。   例如现在需求是返回10以内的所有质数。我们可以使用以下程序：  function prime(){    var result = [2,3,5,7];    return result;}  这段程序非常好，性能也非常高。然而非常的不具备灵活性，通过对需求的分析我们似乎可以大胆的预见10是一个非常有可能会改变的需求，于是提高灵活性的方式就是把10变成可变参数：  function prime(range){    var result = [];    var i,k;    for(i=2; i&lt;=range; i++){      result.push(i);    }    for(i=0; i&lt;result.length; i++){      for(k=i+1; k&lt;result.length; k++){        if(result[k]%result[i]==0){          result.splice(k,1);        }      }    }    return result;}  我们看到第二段代码为了增加灵活性，代码变的更复杂，运行时间变长。当然第一段代码中的质数根本就没有经过计算验证，完全是我自己计算出来的，因为10以内的质数这样简单的运算根本不需要计算机。   在软件开发中有哪些问题会引起灵活性下降呢？         由于各种原因而造成的数量惊人的代码       过于复杂的代码       不断重复的代码       糟糕的软件中这三点往往是一起出现的。    软件系统通常可以通过以分层，组件化等方式来提高灵活性。我在实践中的原则是”用简单构造复杂“。软件系统本身是非常复杂的，然而构建软件系统的基本单元却应该是非常简单的。例如计算机的基本组成单元是门电路，每一个门电路都非常简单，然而计算机系统却是如此的复杂和灵活。凯文凯利在他的《失控》一书中，也有同样的观点。    构建灵活软件系统的关键在于找到那个简单单元的边界，每一个单元应该足够的简单，但是不能够过于简单，爱因斯坦说过“Simple，but not Simpler！”     概念一致性 （Conceptual Integrity）   我们很少在软件设计的时候谈论概念一致性，也许我们认为概念一致是一个共同的假定，然而实际上，在软件开发的过程中，往往会出现很多概念不一致的情况。   概念一致性的问题主要表现在以下的方面：&nbsp; &nbsp;&nbsp;         在一个模块的设计中混杂不同的问题域       不同的组织或者团队负责系统中的同一个功能       没有统一的代码规范       为了满足后向兼容，系统中存在新旧两套不同的代码栈       我们可以看出，这些问题大多是管理的问题。我个人认为，为了实现概念一致性，在软件的设计过程中应该尽可能少的引入新的概念。软件设计的过程是一个抽象的过程，我们把复杂的软件系统抽象为一个个的层，问题域，过程，模块，服务，接口，这些都是非常必要的。但是这些东西都应该是越少越好，能在一两层解决的问题，绝不要划分成四五层，能提供一个API接口，绝不要给三个。人类能够同时掌握的概念是有限的，大部分人可能也就三四个把，当你设计的系统中有五六个或者七八个陌生的概念需要同时掌握的时候，对你的团队中其它需要使用你的设计的开发人员来说绝对是一个巨大的挑战。大部分人不会费力气去搞懂你设计的高大上的新概念。他们很有可能会设计出一套对他们自己更容易理解的并行的方案来解决同样的问题。    在以后我们将接着讨论软件中其它主要的质量属性。   当我们设计软件的时候，需要定义哪些质量属性是我们希望实现的，切记，质量属性并非越多越好。一般来说找到最重要的三个来构建软件就好了，而且&quot;鱼和熊掌不能得兼&quot;，各个质量属性之间有可能是互相矛盾或者互相影响的。分布式数据库中的CAP理论就是一个典型的例子。对于一个分布式的计算系统可不能同时满足一致性（Consistency），可用性（Availability），分区容忍性（Partition Tolerance）。</div>
                    </div>
    
        <div class='blog'>
            <h2><a href="#top" class='top'>回到顶部</a><a name="blog_225513" href="https://my.oschina.net/taogang/blog/225513">软件中的质量属性（二）</a></h2>
            <div class='outline'>
                <div class='date'>时间：2014-04-23 14:49:21</div>
                <div class='catalog'>分类：架构设计</div>
                                                                            </div>
            <div class='content'>现在我们接着上一次的话题来看看其它的质量属性。 互操作性 （Interoperability） 互操作性指的是系统内或者系统之间不同的组件可以有效地进行信息交换，通常是以服务（Service）的形式来进行的。互操作性的关键因素包括通信协议，接口定义，数据格式的定义等等，而标准化是实现互操作性的重要手段。 实现互操作性的主要挑战有以下这些方面：   系统内部或者和已有的旧系统（legacy system）之间的数据定义不一致  系统的边界模糊，模块之间耦合严重，导致数据冗余  缺乏标准，或者各方对标准的实现和认识不一致  我现在所在的商务智能团队的总架构师（Chief Architect）就一直在部门间推动对数据文档统一格式标准的定义和实现。这本身对于我们产品内部的互操作性是非常有必要的，然而BI的团队分布在各个大洲（主要是德国，法国，加拿大，中国和印度），每个部门对各自产品优先级的认识不一致，在加上对旧系统兼容性的要求，这项工作的进展非常非常的缓慢。BI的各个产品仍然很难互操作。 我之前开发过通信网管，当时做的产品是统一网管平台，就是把各个厂商（华为，中兴，朗讯等等）的电信设备统一的管理起来。当时已经有了相当成熟的电信网管理标准（TMF，ISO等标准）和技术标准（Q3,Corba）。然而理解的不同，厂商对标准的实现千奇百怪，所以实际上需要给每一个厂商定制不同的接口适配器。我当时就在负责一些这样的接口开发。 面向服务的架构（SOA）曾经是一个非常热门的词汇，现在似乎不怎么提起了。我司当年曾经大张旗鼓的宣传SOA。其实这样的架构能够解决的一个主要的问题就在能够把企业内部各种已有系统通过暴露标准服务接口的方式有效的协调起来。 为了实现互操作性，我们一般需要   定义标准的数据格式和语义  定义标准的服务接口，并使用基于服务的架构  设计高内聚，低耦合的模块以获得最大的灵活性和可重用性  可维护性 （Maintainability） 可维护性有两个不同的角度，一个是指从软件用户和运维人员的角度，另一个是从软件开发人员的角度。 从用户和运维人员的角度，软件的可维护性是指软件是不是容易安装，升级，打补丁，有了问题是不是容易修复，能不能很容易的获得支持。 从开发人员的角度，软件的可维护性是指软件的架构是不是清楚简单，代码是不是容易阅读，有了问题是不是容易定位错误的原因，有没有可以提供帮助的文档，等等。 软件系统可维护性的主要问题有：   模块之间紧耦合导致无法或很难对单独的模块进行修改，替换和升级  在高层直接使用底层协议和接口，导致无法替换物理设备实体  没有有效的分层和责任的划分，导致一个肿大的模块以及巨大的代码出现在同一个文件甚至函数中。  没有帮助和设计文档  为了兼容旧系统而不得不同时存在两个以上的复杂的代码栈甚至技术不同的实现  知道了问题，改进的建议是非常明显的，我要讨论的是一个关于代码可读性的有趣话题。 很多人都认为，代码是写给人看的，它碰巧也能被计算机读懂。所以我们应该像写文学作品那样写代码。碰巧我也非常的赞同这样的观点。然而，最近的一篇文章提到另一种观点，代码不是文学作品，我们不是阅读而是评审。这篇文章也许能够帮助你改善代码评审。不管代码是不是文学作品，写出容易阅读的代码对于提高软件的可维护性的好处是不言而喻的。 性能（Performance） 性能也许是软件开发中最被重视的质量属性，也是最特殊的一个，从它的英文名字中不以（-ility）为后缀，我们可以看到他的特殊性。我们通常以系统执行某操作所需要的响应时间（latency）或者在某单位时间所能完成的任务的数量（throughput）来定义性能指标。 性能和其它的质量属性的相关性很高，有一些会对性能产生正面影响，有一些则是负面的。 记得我当年参加一个关于C++性能优化的培训，有一道算法，要求大家试着用最快的方式实现。因为C++中的指针操作按数组索引访问要快，于是当时最快的一个解决方案是用了一大推复杂指针访问来实现算法。然而这样的代码很难维护，而且容易出错。所以为提高性能就牺牲了可维护性。一般而言，计算机提供了许多的资源，包括CPU，内存，硬盘等等，提高性能的核心就是充分利用这些资源。要保证对资源的使用是正确和有效的。通常提高性能的考虑包括：   利用缓存（空间换时间）  设计高效的资源共享，多线程，多进程，锁  异步  减少模块见得信息传递  使接口设计传递最小所需的信息  增加系统的可伸缩性，是系统能够有效的部署在分布式的资源上  另外我们还需要考虑另一个性能，就是程序员实现功能的性能。随着软件的发展，现在的程序员可以更高效的实现功能需求。一方面编程语言和方法在不断进步，另一方面大量的可重复使用的组件，服务，开源的库使得想在实现同样的功能的时间和需要的开发人员的数量比以前极大的缩小了。whatsapp以区区55人的团队开发出价值190亿美元，拥有4.5亿用户的软件产品，这在以前是难以想象的。所以软件架构设计者应该把软件的开发效率看成是更重要的性能指标。  可重用性 （Reusability） 老板一般都非常重视可重用性，因为如果把软件代码看成产品，那么如果该产品如果只能用一次就扔掉，那他显然是很不开心的，因为这是一笔很糟糕的投入。在我的软件开发生涯这么多年以来，我体验的软件的可重用性都不是很高，也许是我大多在大型的软件企业服务有关。大企业的特点就是团队非常多，产品非常丰富，老板经常更换。每一个新的老板上台后，面临一大堆功能技术各异的系统都非常的不happy，于是整合在所难免，如何重用已有的系统来实现一个新的，大一统的新产品成了重中之重。然而这并不比找到宇宙中的终极大一统理论更简单。最终的整合结果往往并不能有效的重用已有的系统，当老板因为各种原因离开时，我们会发现，对新的老板来说，整合的任务变得更加艰巨了，因为，又有一个新的系统需要整合了。 提高可重用性的一个最主要的原则就是避免重复，“Don't repeat yourself!”我想大家应该非常熟悉了，这里就不多说了。 在成熟度不高的开发团队中（很不幸，我们大多数人都处在这样的团队中），对代码的重用很难，其实只有人才是最可靠的可重用组件，人离开了，所有的可重用性也就跟着离开了。  伸缩性（Scalability） 伸缩性要求软件系统能够跟着所需处理的工作量相应的伸缩。例如如果计算机是多CPU多核心的，软件是否能够相应的利用到这些计算资源。另一个方面就是软件是不是能够部署到分布式的网络，有效的利用网络中的每一个节点的资源。 有两个方向的伸缩，垂直和水平。 在垂直方向的伸缩（scale up）是指提高单节点的处理能力，比如提高CPU主频和内核数，增大内存，增大磁盘容量等等。SAP的HANA就是一个典型的垂直方向的伸缩。 在水平方向的伸缩（scale out）通常是指通过并发和分布的方式来增加节点以提高处理能力。Hadoop就是一个很好地水平伸缩的例子。 设计高伸缩性的软件时，我们可以考虑   避免有状态的服务或组件  使用配置文件决定组件的的部署和关系  考虑支持数据的分区  避免统一层的责任跨越不同的物理实体  在云时代，软件的伸缩性越发重要。  可测试性（Testability） 可测试性顾名思义就是指软件是否容易测试。 什么样的软件是不可测试的呢？举个例子来说，我们曾经开发过一个数据可视化的组件，就是把数据以图表的形式展现出来。有一种数据可视化的类型使用力导向的算法（force-directed）把数据以网络拓扑图的形式展现出来，该算法使用一些随机的参数来模拟节点的初始位置，并通过迭代计算生成最终layout的结果。也就是说每次的layout结果都是不一样的。测试团队对这样的算法很不满意，他们认为这样的实现是无法测试的，因为当时我们的测试主要以比对图形为基础，也就是生成一个正确的图形为基准，每次测试都会把输出的图形和基准图形进行比较，如果不一致则认为出错或者要修改基准。随机算法虽然从功能上讲并没有错误，但是在这样的测试方法下是无法满足可测试性的要求的。最后，开发团队修改了算法，使得每次的初始位置未固定位置来解决这个问题。 David Catlett提出了一个SOCK模型可以有效地帮助我们了解可测试性的要素   Simple代码越简单越容易测试。  Observable软件系统应该是可观测，无法观测也就无法衡量  Control软件系统应该是可以控制的，尽可能多的把控制权暴露给测试模块。  Knowledge测试人员或者模块需要更多地理解被测试模块，理解的越多也就越容易测试（白盒测试）  软件质量属性的每一个方面都有很多的内容，我们只能浅尝而止，而且仍然有许多重要的质量属性我们还没有涉及到，有时间的话，我们以后再说。</div>
                    </div>
    
        <div class='blog'>
            <h2><a href="#top" class='top'>回到顶部</a><a name="blog_264351" href="https://my.oschina.net/taogang/blog/264351">Python 与 Javascript 之比较</a></h2>
            <div class='outline'>
                <div class='date'>时间：2014-05-13 18:03:54</div>
                <div class='catalog'>分类：编程语言</div>
                                                                            </div>
            <div class='content'>最近由于工作的需要开始开发一些Python的东西，由于之前一直在使用Javascript，所以会不自觉的使用一些Javascript的概念，语法什么的，经常掉到坑里。我觉得对于从Javascript转到Python，有必要总结一下它们之间的差异。 基本概念  Python和Javascript都是脚本语言，所以它们有很多共同的特性，都需要解释器来运行，都是动态类型，都支持自动内存管理,都可以调用eval（）来执行脚本等等脚本语言所共有的特性。 然而它们也有很大的区别，Javascript这设计之初是一种客户端的脚本语言，主要应用于浏览器，它的语法主要借鉴了C，而Python由于其“优雅”，“明确”，“简单”的设计而广受欢迎，被应用于教育，科学计算，web开发等不同的场景中。 编程范式 Python和Javascript都支持多种不同的编程范式，在面向对象的编程上面，它们有很大的区别。Javascript的面向对象是基于原型（prototype）的， 对象的继承是由原型（也是对象）创建出来的，由原型对象创建出来的对象继承了原型链上的方法。而Python则是中规中矩的基于类（class）的继承，并天然的支持多态（polymophine）。 OO in Pyhton&nbsp; class&nbsp;Employee:&nbsp;&nbsp;&nbsp;'Common&nbsp;base&nbsp;class&nbsp;for&nbsp;all&nbsp;employees'&nbsp;&nbsp;&nbsp;empCount&nbsp;=&nbsp;0&nbsp;##类成员&nbsp;&nbsp;&nbsp;def&nbsp;__init__(self,&nbsp;name,&nbsp;salary):&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;self.name&nbsp;=&nbsp;name&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;self.salary&nbsp;=&nbsp;salary&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Employee.empCount&nbsp;+=&nbsp;1&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;def&nbsp;displayCount(self):&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;print&nbsp;&quot;Total&nbsp;Employee&nbsp;%d&quot;&nbsp;%&nbsp;Employee.empCount&nbsp;&nbsp;&nbsp;def&nbsp;displayEmployee(self):&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;print&nbsp;&quot;Name&nbsp;:&nbsp;&quot;,&nbsp;self.name,&nbsp;&nbsp;&quot;,&nbsp;Salary:&nbsp;&quot;,&nbsp;self.salary##&nbsp;创建实例ea&nbsp;=&nbsp;Employee(&quot;a&quot;,1000)eb&nbsp;=&nbsp;Employee(&quot;b&quot;,2000)  OO in Javascript var&nbsp;empCount&nbsp;=&nbsp;0;//构造函数function&nbsp;Employee(name,&nbsp;salary){&nbsp;&nbsp;&nbsp;&nbsp;this.name&nbsp;=&nbsp;name;&nbsp;&nbsp;&nbsp;&nbsp;this.salary&nbsp;=&nbsp;salary;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;this.empCount&nbsp;+=&nbsp;1;}Employee.prototype.displayCount&nbsp;=&nbsp;function(){&nbsp;&nbsp;&nbsp;&nbsp;console.log(&quot;Total&nbsp;Employee&nbsp;&quot;&nbsp;+&nbsp;empCount&nbsp;);}Employee.prototype.displayEmployee&nbsp;=&nbsp;function(){&nbsp;&nbsp;&nbsp;&nbsp;console.log(&quot;Name&nbsp;&quot;&nbsp;+&nbsp;this.name&nbsp;+&nbsp;&quot;,&nbsp;Salary&nbsp;&quot;&nbsp;+&nbsp;this.salary&nbsp;);}//创建实例var&nbsp;ea&nbsp;=&nbsp;new&nbsp;Employee(&quot;a&quot;,1000);var&nbsp;eb&nbsp;=&nbsp;new&nbsp;Employee(&quot;b&quot;,2000);  因为是基于对象的继承，在Javascript中，我们没有办法使用类成员empCount，只好声明了一个全局变量，当然实际开发中我们会用更合适的scope。注意Javascript创建对象需要使用new关键字，而Python不需要。 除了原生的基于原型的继承，还有很多利用闭包或者原型来模拟类继承的Javascript OO工具，因为不是语言本身的属性，我们就不讨论了。 线程模型 在Javascript的世界中是没有多线程的概念的，并发使用过使用事件驱动的方式来进行的， 所有的JavaScript程序都运行在一个线程中。在HTML5中引入web worker可以并发的处理任务，但没有改变Javascript单线程的限制。 Python通过thread包支持多线程。  不可改变类型 （immutable type）  在Python中，有的数据类型是不可改变的，也就意味着这种类型的数据不能被修改，所有的修改都会返回新的对象。而在Javascript中所有的数据类型都是可以改变的。Python引入不可改变类型我认为是为了支持线程安全，而因为Javascript是单线程模型，所以没有必要引入不可改变类型。 当然在Javascript可以定义一个对象的属性为只读。 var&nbsp;obj&nbsp;=&nbsp;{};Object.defineProperty(obj,&nbsp;&quot;prop&quot;,&nbsp;{&nbsp;&nbsp;&nbsp;&nbsp;value:&nbsp;&quot;test&quot;,&nbsp;&nbsp;&nbsp;&nbsp;writable:&nbsp;false});  在ECMAScript5的支持中，也可以调用Object的freeze方法来是对象变得不可修改。  Object.freeze(obj)    数据类型 Javascript的数据类型比较简单，有object、string、boolean、number、null和undefined，总共六种 Python中一切均为对象，像module、function、class等等都是。  Python有五个内置的简单数据类型bool、int、long、float和complex，另外还有容器类型，代码类型，内部类型等等。  布尔 Javascript有true和false。Python有True和False。它们除了大小写没有什么区别。  字符串 Javascript采用UTF16编码。 Python使用ASCII码。需要调用encode、decode来进行编码转换。使用u作为前缀可以指定字符串使用Unicode编码。  数值 Javascript中所有的数值类型都是实现为64位浮点数。支持NaN(Not a number)，正负无穷大（+/-Infiity）。  Python拥有诸多的数值类型，其中的复数类型非常方便，所以在Python在科研和教育领域很受欢迎。这应该也是其中一个原因吧。Python中没有定义NaN，除零操作会引发异常。 列表 Javascript内置了array类型（array也是object）  Python的列表（List）和Javascript的Array比较接近，而元组（Tuple）可以理解为不可改变的列表。  除了求长度在Python中是使用内置方法len外，基本上Javascript和Python都提供了类似的方法来操作列表。Python中对列表下标的操作非常灵活也非常方便，这是Javascript所没有的。例如l[5:-1],l[:6]等等。  字典、哈希表、对象 Javascript中大量的使用{}来创建对象，这些对象和字典没有什么区别，可以使用[]或者.来访问对象的成员。可以动态的添加，修改和删除成员。可以认为对象就是Javascript的字典或者哈希表。对象的key必须是字符串。 Python内置了哈希表（dictS），和Javascript不同的是，dictS可以有各种类型的key值。  空值 Javascript定义了两种空值。 undefined表示变量没有被初始化，null表示变量已经初始化但是值为空。  Python中不存在未初始化的值，如果一个变量值为空，Python使用None来表示。 Javascript中变量的声明和初始化 v1;v2&nbsp;=&nbsp;null;var&nbsp;v3;var&nbsp;v4&nbsp;=&nbsp;null;var&nbsp;v5&nbsp;=&nbsp;'something';  在如上的代码中v1是全局变量，未初始化，值为undefined；v2是全局变量，初始化为空值；v3为局部未初始化变量，v4是局部初始化为空值的变量；v5是局部已初始化为一个字符处的变量。 Python中变量的声明和初始化  v1&nbsp;=&nbsp;Nonev2&nbsp;=&nbsp;'someting'  Python中的变量声明和初始化就简单了许多。当在Python中访问一个不存在的变量时，会抛出NameError的异常。当访问对象或者字典的值不存在的时候，会抛出AttributeError或者KeyError。因此判断一个值是否存在在Javascript和Python中需要不一样的方式。 Javascript中检查某变量的存在性： if&nbsp;(!v&nbsp;)&nbsp;{&nbsp;&nbsp;&nbsp;&nbsp;//&nbsp;do&nbsp;something&nbsp;if&nbsp;v&nbsp;does&nbsp;not&nbsp;exist&nbsp;or&nbsp;is&nbsp;null&nbsp;or&nbsp;is&nbsp;false}if&nbsp;(v&nbsp;===&nbsp;undefined)&nbsp;{&nbsp;&nbsp;&nbsp;&nbsp;//&nbsp;do&nbsp;something&nbsp;if&nbsp;v&nbsp;does&nbsp;not&nbsp;initialized}  注意使用!v来检查v是否初始化是有歧义的因为有许多种情况!v都会返回true Python中检查某变量的存在性： try:&nbsp;&nbsp;&nbsp;&nbsp;vexcept&nbsp;NameError&nbsp;&nbsp;&nbsp;&nbsp;##&nbsp;do&nbsp;something&nbsp;if&nbsp;v&nbsp;does&nbsp;not&nbsp;exist  在Python中也可以通过检查变量是不是存在于局部locals（）或者全局globals（）来判断是否存在该变量。 类型检查 Javascript可以通过typeof来获得某个变量的类型： typeof in Javascript 的例子： typeof&nbsp;3&nbsp;//&nbsp;&quot;number&quot;typeof&nbsp;&quot;abc&quot;&nbsp;//&nbsp;&quot;string&quot;typeof&nbsp;{}&nbsp;//&nbsp;&quot;object&quot;typeof&nbsp;true&nbsp;//&nbsp;&quot;boolean&quot;typeof&nbsp;undefined&nbsp;//&nbsp;&quot;undefined&quot;typeof&nbsp;function(){}&nbsp;//&nbsp;&quot;function&quot;typeof&nbsp;[]&nbsp;//&nbsp;&quot;object&quot;typeof&nbsp;null&nbsp;//&nbsp;&quot;object&quot;  要非常小心的使用typeof，从上面的例子你可以看到，typeof null居然是object。因为javscript的弱类型特性，想要获得更实际的类型，还需要结合使用instanceof，constructor等概念。具体请参考这篇文章 Python提供内置方法type来获得数据的类型。 &gt;&gt;&gt;&nbsp;type([])&nbsp;is&nbsp;listTrue&gt;&gt;&gt;&nbsp;type({})&nbsp;is&nbsp;dictTrue&gt;&gt;&gt;&nbsp;type('')&nbsp;is&nbsp;strTrue&gt;&gt;&gt;&nbsp;type(0)&nbsp;is&nbsp;intTrue  同时也可以通过isinstance()来判断类的类型 class&nbsp;A:&nbsp;&nbsp;&nbsp;&nbsp;passclass&nbsp;B(A):&nbsp;&nbsp;&nbsp;&nbsp;passisinstance(A(),&nbsp;A)&nbsp;&nbsp;#&nbsp;returns&nbsp;Truetype(A())&nbsp;==&nbsp;A&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;#&nbsp;returns&nbsp;Trueisinstance(B(),&nbsp;A)&nbsp;&nbsp;&nbsp;&nbsp;#&nbsp;returns&nbsp;Truetype(B())&nbsp;==&nbsp;A&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;#&nbsp;returns&nbsp;False  但是注意Python的class style发生过一次变化，不是每个版本的Python运行上述代码的行为都一样，在old style中，所有的实例的type都是‘instance’，所以用type方法来检查也不是一个好的方法。这一点和Javascript很类似。 自动类型转换 当操作不同类型一起进行运算的时候，Javascript总是尽可能的进行自动的类型转换，这很方便，当然也很容易出错。尤其是在进行数值和字符串操作的时候，一不小心就会出错。我以前经常会计算SVG中的各种数值属性，诸如x，y坐标之类的，当你一不小心把一个字符串加到数值上的时候，Javascript会自动转换出一个数值，往往是NaN，这样SVG就完全画不出来啦，因为自动转化是合法的，找到出错的地方也非常困难。  Python在这一点上就非常的谨慎，一般不会在不同的类型之间做自动的转换。 语法 风格  Python使用缩进来决定逻辑行的结束非常具有创造性，这也许是Python最独特的属性了，当然也有人对此颇具微词，尤其是需要修改重构代码的时候，修改缩进往往会引起不小的麻烦。  Javascript虽然名字里有Java，它的风格也有那么一点像Java，可是它和Java就好比雷峰塔和雷锋一样，真的没有半毛钱的关系。到时语法上和C比较类似。这里必须要提到的是coffeescript作为构建与Javascript之上的一种语言，采用了类似Python的语法风格，也是用缩进来决定逻辑行。 Python风格  def&nbsp;func(list):&nbsp;&nbsp;&nbsp;&nbsp;for&nbsp;i&nbsp;in&nbsp;range(0,len(list)):&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;print&nbsp;list[i]  Javascript风格  function&nbsp;funcs(list)&nbsp;{&nbsp;&nbsp;&nbsp;&nbsp;for(var&nbsp;i=0,&nbsp;len&nbsp;=&nbsp;list.length();&nbsp;i&nbsp;&lt;&nbsp;len;&nbsp;i++)&nbsp;{&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;console.log(list[i]);&nbsp;&nbsp;&nbsp;&nbsp;}}  从以上的两个代码的例子可以看出，Python确实非常简洁。 作用范围和包管理 Javascript的作用域是由方法function来定义的，也就是说同一个方法内部拥有相同的作用域。这个严重区别与C语言使用{}来定义的作用域。Closure是Javascript最有用的一个特性。 Python的作用域是由module，function，class来定义的。 Python的import可以很好的管理依赖和作用域，而Javascript没有原生的包管理机制，需要借助AMD来异步的加载依赖的js文件，requirejs是一个常用的工具。 赋值逻辑操作符 Javascript使用=赋值，拥有判断相等（==）和全等（===）两种相等的判断。其它的逻辑运算符有&amp;&amp; 和||，和C语言类似。 Python中没有全等，或和与使用的时and 和 or，更接近自然语言。Python中没有三元运算符 A ：B ？C，通常的写法是 （A&nbsp;and&nbsp;B)&nbsp;or&nbsp;C  因为这样写有一定的缺陷，也可以写作 &nbsp;B&nbsp;if&nbsp;A&nbsp;else&nbsp;C  Python对赋值操作的一个重要的改进是不允许赋值操作返回赋值的结果，这样做的好处是避免出现在应该使用相等判断的时候错误的使用了赋值操作。因为这两个操作符实在太像了，而且从自然语言上来说它们也没有区别。 ++运算符 Python不支持++运算符，没错你再也不需要根据++符号在变量的左右位置来思考到底是先加一再赋值呢还是先赋值再加一。  连续赋值 利用元组（tuple），Python可以一次性的给多个变量赋值  (MONDAY,&nbsp;TUESDAY,&nbsp;WEDNESDAY,&nbsp;THURSDAY,&nbsp;FRIDAY,&nbsp;SATURDAY,&nbsp;SUNDAY)&nbsp;=&nbsp;range(7)  函数参数 Python的函数参数支持命名参数和可选参数（提供默认值），使用起来很方便，Javascript不支持可选参数和默认值（可以通过对arguments的解析来支持）  def&nbsp;info(object,&nbsp;spacing=10,&nbsp;collapse=1):&nbsp;&nbsp;&nbsp;&nbsp;...&nbsp;...  其它 立即调用函数表达式 （IIFE）  Javascript的一个方便的特性是可以立即调用一个刚刚声明的匿名函数。也有人称之为自调用匿名函数。  下面的代码是一个module模式的例子，使用闭包来保存状态实现良好的封装。这样的代码可以用在无需重用的场合。 var&nbsp;counter&nbsp;=&nbsp;(function(){&nbsp;&nbsp;&nbsp;&nbsp;var&nbsp;i&nbsp;=&nbsp;0;&nbsp;&nbsp;&nbsp;&nbsp;return&nbsp;{&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;get:&nbsp;function(){&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;return&nbsp;i;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;},&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;set:&nbsp;function(&nbsp;val&nbsp;){&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;i&nbsp;=&nbsp;val;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;},&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;increment:&nbsp;function()&nbsp;{&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;return&nbsp;++i;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;}&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;};&nbsp;&nbsp;&nbsp;&nbsp;}());  Python没有相应的支持。 生成器和迭代器（Generators &amp; Iterator） 在我接触到的Python代码中，大量的使用这样的生成器的模式。 Python生成器的例子 #&nbsp;a&nbsp;generator&nbsp;that&nbsp;yields&nbsp;items&nbsp;instead&nbsp;of&nbsp;returning&nbsp;a&nbsp;listdef&nbsp;firstn(n):&nbsp;&nbsp;&nbsp;&nbsp;num&nbsp;=&nbsp;0&nbsp;&nbsp;&nbsp;&nbsp;while&nbsp;num&nbsp;&lt;&nbsp;n:&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;yield&nbsp;num&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;num&nbsp;+=&nbsp;1&nbsp;&nbsp;sum_of_first_n&nbsp;=&nbsp;sum(firstn(1000000))  Javascript1.7中引入了一些列的新特性，其中就包括生成器和迭代器。然而大部分的浏览器除了Mozilla（Mozilla基本上是在自己玩，下一代的Javascript标准应该是ECMAScript5）都不支持这些特性 Javascript1.7 迭代器和生成器的例子。 function&nbsp;fib()&nbsp;{&nbsp;&nbsp;var&nbsp;i&nbsp;=&nbsp;0,&nbsp;j&nbsp;=&nbsp;1;&nbsp;&nbsp;while&nbsp;(true)&nbsp;{&nbsp;&nbsp;&nbsp;&nbsp;yield&nbsp;i;&nbsp;&nbsp;&nbsp;&nbsp;var&nbsp;t&nbsp;=&nbsp;i;&nbsp;&nbsp;&nbsp;&nbsp;i&nbsp;=&nbsp;j;&nbsp;&nbsp;&nbsp;&nbsp;j&nbsp;+=&nbsp;t;&nbsp;&nbsp;}}；var&nbsp;g&nbsp;=&nbsp;fib();for&nbsp;(var&nbsp;i&nbsp;=&nbsp;0;&nbsp;i&nbsp;&lt;&nbsp;10;&nbsp;i++)&nbsp;{&nbsp;&nbsp;console.log(g.next());}  列表（字典、集合）映射表达式 （List、Dict、Set Comprehension） Python的映射表达式可以非常方便的帮助用户构造列表、字典、集合等内置数据类型。  下面是列表映射表达式使用的例子： &gt;&gt;&gt;&nbsp;[x&nbsp;+&nbsp;3&nbsp;for&nbsp;x&nbsp;in&nbsp;range(4)][3,&nbsp;4,&nbsp;5,&nbsp;6]&gt;&gt;&gt;&nbsp;{x&nbsp;+&nbsp;3&nbsp;for&nbsp;x&nbsp;in&nbsp;range(4)}{3,&nbsp;4,&nbsp;5,&nbsp;6}&gt;&gt;&gt;&nbsp;{x:&nbsp;x&nbsp;+&nbsp;3&nbsp;for&nbsp;x&nbsp;in&nbsp;range(4)}{0:&nbsp;3,&nbsp;1:&nbsp;4,&nbsp;2:&nbsp;5,&nbsp;3:&nbsp;6}  Javascript1.7开始也引入了Array Comprehension var&nbsp;numbers&nbsp;=&nbsp;[1,&nbsp;2,&nbsp;3,&nbsp;4];var&nbsp;doubled&nbsp;=&nbsp;[i&nbsp;*&nbsp;2&nbsp;for&nbsp;(i&nbsp;of&nbsp;numbers)];  Lamda表达式 （Lamda Expression ） Lamda表达式是一种匿名函数，基于著名的λ演算。许多语言诸如C#，Java都提供了对lamda的支持。Pyhton就是其中之一。Javascript没有提供原生的Lamda支持。但是有第三方的Lamda包。 g&nbsp;=&nbsp;lambda&nbsp;x&nbsp;:&nbsp;x*3  装饰器（Decorators）  Decorator是一种设计模式，大部分语言都可以支持这样的模式，Python提供了原生的对该模式的支持，算是一种对程序员的便利把。  Decorator的用法如下。 @classmethoddef&nbsp;foo&nbsp;(arg1,&nbsp;arg2):&nbsp;&nbsp;&nbsp;&nbsp;....  更多decorator的内容，请参考https://wiki.python.org/moin/PythonDecorators  本人对Javascript和Python的认识有限，欢迎大家提出宝贵意见。</div>
                            <div class='commentList'>
                    <h3>评论列表</h3>
                                    </div>
                    </div>
    
        <div class='blog'>
            <h2><a href="#top" class='top'>回到顶部</a><a name="blog_267707" href="https://my.oschina.net/taogang/blog/267707">探索Javascript异步编程</a></h2>
            <div class='outline'>
                <div class='date'>时间：2014-05-22 12:49:13</div>
                <div class='catalog'>分类：编程语言</div>
                                                                            </div>
            <div class='content'>笔者在之前的一片博客中简单的讨论了Python和Javascript的异同，其实作为一种编程语言Javascript的异步编程是一个非常值得讨论的有趣话题。   JavaScript 异步编程简介   回调函数和异步执行   所谓的异步指的是函数的调用并不直接返回执行的结果，而往往是通过回调函数异步的执行。   我们先看看回调函数是什么：  var fn = function(callback) {    // do something here    ...    callback.apply(this, para);}；var mycallback = function(parameter) {    // do someting in customer callback}；// call the fn with callback as parameterfn(mycallback)；   回调函数，其实就是调用用户提供的函数，该函数往往是以参数的形式提供的。回调函数并不一定是异步执行的。比如上述的例子中，回调函数是被同步执行的。大部分语言都支持回调，C++可用通过函数指针或者回调对象，Java一般也是使用回调对象。   在Javascript中有很多通过回调函数来执行的异步调用，例如setTimeout（）或者setInterval（）。  setTimeout(function(){    console.log(&quot;this will be exectued after 1 second!&quot;);},1000);   在以上的例子中，setTimeout直接返回，匿名函数会在1000毫秒（不一定能保证是1000毫秒）后异步触发并执行，完成打印控制台的操作。也就是说在异步操作的情境下，函数直接返回，把控制权交给回调函数，回调函数会在以后的某一个时间片被调度执行。那么为什么需要异步呢？为什么不能直接在当前函数中完成操作呢？这就需要了解Javascript的线程模型了。   Javascript线程模型和事件驱动   Javascript最初是被设计成在浏览器中辅助提供HTML的交互功能。在浏览器中都包含一个Javascript引擎，Javscript程序就运行在这个引擎之中，并且只有一个线程。单线程能都带来很多优点，程序员们可以很开心的不用去考虑诸如资源同步，死锁等多线程阻塞式编程所需要面对的恼人的问题。但是很多人会问，既然Javascript是单线程的，那它又如何能够异步的执行呢？   这就需要了解到Javascript在浏览器中的事件驱动（event driven）机制。事件驱动一般通过事件循环（event loop）和事件队列（event queue）来实现的。假定浏览器中有一个专门用于事件调度的实例（该实例可以是一个线程，我们可以称之为事件分发线程event dispatch thread），该实例的工作就是一个不结束的循环，从事件队列中取出事件，处理所有很事件关联的回调函数（event handler）。注意回调函数是在Javascript的主线程中运行的，而非事件分发线程中，以保证事件处理不会发生阻塞。   Event Loop Code:  while(true) { var event = eventQueue.pop(); if(event &amp;&amp; event.handler) {     event.handler.execute(); // execute the callback in Javascript thread } else {     sleep(); //sleep some time to release the CPU do other stuff }}   通过事件驱动机制，我们可以想象Javascript的编程模型就是响应一系列的事件，执行对应的回调函数。很多UI框架都采用这样的模型（例如Java Swing）。   那为什要异步呢，同步不是很好么？   异步的主要目的是处理非阻塞，在和HTML交互的过程中，会需要一些IO操作（典型的就是Ajax请求，脚本文件加载），如果这些操作是同步的，就会阻塞其它操作，用户的体验就是页面失去了响应。   综上所述Javascript通过事件驱动机制，在单线程模型下，以异步回调函数的形式来实现非阻塞的IO操作。   Javascript异步编程带来的挑战   Javascript的单线程模型有很多好处，但同时也带来了很多挑战。   代码可读性   想象一下，如果某个操作需要经过多个非阻塞的IO操作，每一个结果都是通过回调，程序有可能会看上去像这个样子。  operation1(function(err, result) {    operation2(function(err, result) {        operation3(function(err, result) {            operation4(function(err, result) {                operation5(function(err, result) {                    // do something useful                })            })        })    })})   我们称之为意大利面条式（spaghetti）的代码。这样的代码很难维护。这样的情况更多的会发生在server side的情况下。   流程控制   异步带来的另一个问题是流程控制，举个例子，我要访问三个网站的内容，当三个网站的内容都得到后，合并处理，然后发给后台。代码可以这样写：  var urls = ['url1','url2','url3'];var result = [];for (var i = 0, len = urls.length(); i &lt; len; i++ ) {    $.ajax({        url: urls[i],        context: document.body,        success: function(){          //do something on success          result.push(&quot;one of the request done successfully&quot;);          if (result.length === urls.length()) {              //do something when all the request is completed successfully          }        }});}   上述代码通过检查result的长度的方式来决定是否所有的请求都处理完成，这是一个很丑陋方法，也很不可靠。   异常和错误处理   通过上一个例子，我们还可以看出，为了使程序更健壮，我们还需要加入异常处理。 在异步的方式下，异常处理分布在不同的回调函数中，我们无法在调用的时候通过try...catch的方式来处理异常， 所以很难做到有效，清楚。   更好的Javascript异步编程方式   “这是最好的时代，也是最糟糕的时代”   为了解决Javascript异步编程带来的问题，很多的开发者做出了不同程度的努力，提供了很多不同的解决方案。然而面对如此众多的方案应该如何选择呢？我们这就来看看都有哪些可供选择的方案吧。   Promise   Promise 对象曾经以多种形式存在于很多语言中。这个词最先由C++工程师用在Xanadu 项目中，Xanadu 项目是Web 应用项目的先驱。随后Promise 被用在E编程语言中，这又激发了Python 开发人员的灵感，将它实现成了Twisted 框架的Deferred 对象。   2007 年，Promise 赶上了JavaScript 大潮，那时Dojo 框架刚从Twisted框架汲取灵感，新增了一个叫做dojo.Deferred 的对象。也就在那个时候，相对成熟的Dojo 框架与初出茅庐的jQuery 框架激烈地争夺着人气和名望。2009 年，Kris Zyp 有感于dojo.Deferred 的影响力提出了CommonJS 之Promises/A 规范。同年，Node.js 首次亮相。   在编程的概念中，future，promise，和delay表示同一个概念。Promise翻译成中文是“承诺”，也就是说给你一个东西，我保证未来能够做到，但现在什么都没有。它用来表示异步操作返回的一个对象，该对象是用来获取未来的执行结果的一个代理，初始值不确定。许多语言都有对Promise的支持。   Promise的核心是它的then方法，我们可以使用这个方法从异步操作中得到返回值，或者是异常。then有两个可选参数（有的实现是三个），分别处理成功和失败的情景。  var promise = doSomethingAync()promise.then(onFulfilled, onRejected)   异步调用doSomethingAync返回一个Promise对象promise，调用promise的then方法来处理成功和失败。这看上去似乎并没有很大的改进。仍然需要回调。但是和以前的区别在于，首先异步操作有了返回值，虽然该值只是一个对未来的承诺；其次通过使用then，程序员可以有效的控制流程异常处理，决定如何使用这个来自未来的值。   对于嵌套的异步操作，有了Promise的支持，可以写成这样的链式操作：  operation1().then(function (result1) {    return operation2(result1)}).then(function (result2) {    return operation3(result2);}).then(function (result3) {    return operation4(result3);}).then(function (result4) {    return operation5(result4)}).then(function (result5) {    //And so on});   Promise提供更便捷的流程控制，例如Promise.all()可以解决需要并发的执行若干个异步操作，等所有操作完成后进行处理。  var p1 = async1();var p2 = async2();var p3 = async3();Promise.all([p1,p2,p3]).then(function(){    // do something when all three asychronized operation finished});   对于异常处理，  doA()  .then(doB)  .then(null,function(error){      // error handling here  })   如果doA失败，它的Promise会被拒绝，处理链上的下一个onRejected会被调用，在这个例子中就是匿名函数function（error）{}。比起原始的回调方式，不需要在每一步都对异常进行处理。这生了不少事。   以上只是对于Promise概念的简单陈述，Promise拥有许多不同规范建议（A,A+,B,KISS,C,D等），名字（Future，Promise，Defer），和开源实现。大家可以参考一下的这些链接。      jQuery's Deferred Object      YUI Promise Class      Dojo Promises      Q      RSVP.js      When.js      MochiKit.Async      FutureJS      node-promise      WinJS        如果你有选择困难综合症，面对这么多的开源库不知道如何决断，先不要急，这还只是一部分，还有一些库没有或者不完全采用Promise的概念   Non-Promise   下面列出了其它的一些开源的库，也可以帮助解决Javascript中异步编程所遇到的诸多问题，它们的解决方案各不相同，我这里就不一一介绍了。大家有兴趣可以去看看或者试用一下。      Node-fibers      Streamlinejs      Step      Flow-js      Async      Async.js      slide-flow-control     Non-3rd Party   其实，为了解决Javascript异步编程带来的问题，不一定非要使用Promise或者其它的开源库，这些库提供了很好的模式，但是你也可以通过有针对性的设计来解决。   比如，对于层层回调的模式，可以利用消息机制来改写，假定你的系统中已经实现了消息机制，你的code可以写成这样：  eventbus.on(&quot;init&quot;, function(){    operationA(function(err,result){        eventbus.dispatch(&quot;ACompleted&quot;);    });});eventbus.on(&quot;ACompleted&quot;, function(){    operationB(function(err,result){        eventbus.dispatch(&quot;BCompleted&quot;);    });});eventbus.on(&quot;BCompleted&quot;, function(){    operationC(function(err,result){        eventbus.dispatch(&quot;CCompleted&quot;);    });});eventbus.on(&quot;CCompleted&quot;, function(){    // do something when all operation completed});   这样我们就把嵌套的异步调用，改写成了顺序执行的事件处理。   更多的方式，请大家参考这篇文章，它提出了解决异步的五种模式：回调、观察者模式（事件）、消息、Promise和有限状态机（FSM）。   下一代Javscript对异步编程的增强   ECMAScript6   下一代的Javascript标准Harmony，也就是ECMAScript6正在酝酿中，它提出了许多新的语言特性，比如箭头函数、类（Class）、生成器（Generator）、Promise等等。其中Generator和Promise都可以被用于对异步调用的增强。   Nodejs的开发版V0.11已经可以支持ES6的一些新的特性，使用node --harmony命令来运行对ES6的支持。   co、Thunk、Koa   koa是由Express原班人马（主要是TJ）打造，希望提供一个更精简健壮的nodejs框架。koa依赖ES6中的Generator等新特性，所以必须运行在相应的Nodejs版本上。   利用Generator、co、Thunk，可以在Koa中有效的解决Javascript异步调用的各种问题。   co是一个异步流程简化的工具，它利用Generator把一层层嵌套的调用变成同步的写法。  var co = require('co');var fs = require('fs');var stat = function(path) {  return function(cb){    fs.stat(path,cb);  }};var readFile = function(filename) {  return function(cb){    fs.readFile(filename,cb);  }};co(function *() {  var stat = yield stat('./README.md');  var content = yield readFile('./README.md');})();   通过co可以把异步的fs.readFile当成同步一样调用，只需要把异步函数fs.readFile用闭包的方式封装。   利用Thunk可以进一步简化为如下的code, 这里Thunk的作用就是用闭包封装异步函数，返回一个生成函数的函数，供生成器来调用。  var thunkify = require('thunkify');var co = require('co');var fs = require('fs');var stat = thunkify(fs.stat);var readFile = thunkify(fs.readFile);co(function *() {  var stat = yield stat('./README.md');  var content = yield readFile('./README.md');})();   利用co可以串行或者并行的执行异步调用。   串行  co(function *() {  var a = yield request(a);  var b = yield request(b);})();   并行  co(function *() { var res = yield [request(a), request(b)];})();   更多详细的内容，大家可以参考这两篇文章1，2。   总结   异步编程带来的问题在客户端Javascript中并不明显，但随着服务器端Javascript越来越广的被使用，大量的异步IO操作使得该问题变得明显。许多不同的方法都可以解决这个问题，本文讨论了一些方法，但并不深入。大家需要根据自己的情况选择一个适于自己的方法。   同时，随着ES6的定义，Javascript的语法变得越来越丰富，更多的功能带来了很多便利，然而原本简洁，单一目的的Javascript变得复杂，也要承担更多的任务。Javascript何去何从，让我们拭目以待。</div>
                            <div class='commentList'>
                    <h3>评论列表</h3>
                                    </div>
                    </div>
    
        <div class='blog'>
            <h2><a href="#top" class='top'>回到顶部</a><a name="blog_268505" href="https://my.oschina.net/taogang/blog/268505">神奇的阿基米德螺线</a></h2>
            <div class='outline'>
                <div class='date'>时间：2014-05-24 14:01:30</div>
                <div class='catalog'>分类：数学</div>
                                                                            </div>
            <div class='content'>今天在读数学史，正巧读到阿基米德螺线，于是写了一段js代码，生成螺线。更多的有趣内容请参考这篇文章 代码如下（需要jquery和d3）： HTML &lt;div&nbsp;id=&quot;chart&quot;&gt;&lt;/div&gt;  CSS body&nbsp;{background-color:#000000;}  JS function&nbsp;drawCircile(pc,&nbsp;r,&nbsp;container)&nbsp;{&nbsp;&nbsp;&nbsp;&nbsp;var&nbsp;circle&nbsp;=&nbsp;container.append(&quot;circle&quot;).attr(&quot;cx&quot;,&nbsp;pc.x).attr(&quot;cy&quot;,&nbsp;pc.y).attr(&quot;r&quot;,&nbsp;r)&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;.attr(&quot;stroke&quot;,&nbsp;&quot;#fa6900&quot;).attr(&quot;stroke-width&quot;,&nbsp;1).attr(&quot;fill&quot;,&nbsp;&quot;none&quot;);&nbsp;&nbsp;&nbsp;&nbsp;return&nbsp;circle;}function&nbsp;drawLine(p1,&nbsp;p2,&nbsp;c,&nbsp;isDash)&nbsp;{&nbsp;&nbsp;&nbsp;&nbsp;var&nbsp;line&nbsp;=&nbsp;c.append(&quot;line&quot;).attr(&quot;x1&quot;,&nbsp;p1.x).attr(&quot;y1&quot;,&nbsp;p1.y)&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;.attr(&quot;x2&quot;,&nbsp;p2.x).attr(&quot;y2&quot;,&nbsp;p2.y)&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;.style(&quot;stroke&quot;,&nbsp;&quot;#ccc&quot;).style(&quot;stroke-width&quot;,&nbsp;1);&nbsp;&nbsp;&nbsp;&nbsp;if&nbsp;(isDash)&nbsp;{&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;line.style(&quot;stroke-dasharray&quot;,&nbsp;&quot;5,5&quot;);&nbsp;&nbsp;&nbsp;&nbsp;}&nbsp;&nbsp;&nbsp;&nbsp;return&nbsp;line;}$(function&nbsp;()&nbsp;{&nbsp;&nbsp;&nbsp;&nbsp;var&nbsp;center&nbsp;=&nbsp;{x:&nbsp;300,&nbsp;y&nbsp;:300};&nbsp;&nbsp;&nbsp;&nbsp;var&nbsp;l&nbsp;=&nbsp;250;&nbsp;&nbsp;&nbsp;&nbsp;var&nbsp;a&nbsp;=&nbsp;0,&nbsp;b&nbsp;=&nbsp;0,&nbsp;p0,&nbsp;p1,&nbsp;line;&nbsp;&nbsp;&nbsp;&nbsp;var&nbsp;v&nbsp;=&nbsp;0.3;&nbsp;//直线移动速度&nbsp;&nbsp;&nbsp;&nbsp;var&nbsp;f&nbsp;=&nbsp;360;&nbsp;//转动速度&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;var&nbsp;root&nbsp;=&nbsp;d3.select(&quot;#chart&quot;).append(&quot;svg&quot;).append(&quot;g&quot;);&nbsp;&nbsp;&nbsp;&nbsp;drawCircile(center,3,root)&nbsp;&nbsp;&nbsp;&nbsp;var&nbsp;timer&nbsp;=&nbsp;setInterval(function(){&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;a&nbsp;=&nbsp;a&nbsp;+&nbsp;Math.PI&nbsp;/&nbsp;f;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;b&nbsp;=&nbsp;b&nbsp;+&nbsp;v;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;p0&nbsp;=&nbsp;{x:center.x+&nbsp;l&nbsp;*&nbsp;Math.sin(a),&nbsp;y:center.y&nbsp;-&nbsp;l&nbsp;*&nbsp;Math.cos(a)&nbsp;};&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;p1&nbsp;=&nbsp;{x:center.x&nbsp;+&nbsp;b&nbsp;*&nbsp;Math.sin(a),&nbsp;y:center.y&nbsp;-&nbsp;b&nbsp;*&nbsp;Math.cos(a)&nbsp;};&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;drawCircile(p1,1,root)&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;if(&nbsp;b&nbsp;&gt;&nbsp;l&nbsp;)&nbsp;{&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;clearInterval(timer);&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;}&nbsp;else&nbsp;{&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;if(!line)&nbsp;{&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;line&nbsp;=&nbsp;&nbsp;drawLine(center,&nbsp;p0,&nbsp;root);&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;}&nbsp;else&nbsp;{&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;line.transition().duration(10).attr(&quot;x2&quot;,p0.x).attr(&quot;y2&quot;,p0.y);&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;}&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;}&nbsp;&nbsp;&nbsp;&nbsp;},10);});  以上代码会生成以下的图形：  改变v和f的值分别改变点在直线上的移动过速度和直线转动的角速度，会得到不同的结果。 我拿给我还在读一年级的女儿看，她很是入迷，不断要求我修改参数，结果得到许多有趣的图形，大家可以通过这个链接自己试试看。 f=2.5 海星  f=3 雪花  f=1.414  f = 0.618  f=8.8  f = 12.12  更多的结果，大家可以自己去探索。 感叹于数学的神奇和美丽，感叹于小孩子的探索精神， 感叹于计算机和编程使得数学的探索变得简单和有趣！</div>
                    </div>
    
        <div class='blog'>
            <h2><a href="#top" class='top'>回到顶部</a><a name="blog_271060" href="https://my.oschina.net/taogang/blog/271060">使用Python抓取欧洲足球联赛数据</a></h2>
            <div class='outline'>
                <div class='date'>时间：2014-05-30 18:18:31</div>
                <div class='catalog'>分类：编程语言</div>
                                                                            </div>
            <div class='content'>背景 Web Scraping  在大数据时代，一切都要用数据来说话，大数据处理的过程一般需要经过以下的几个步骤   数据的采集和获取  数据的清洗，抽取，变形和装载  数据的分析，探索和预测  数据的展现  其中首先要做的就是获取数据，并提炼出有效地数据，为下一步的分析做好准备。 数据的来源多种多样，以为我本身是足球爱好者，而世界杯就要来了，所以我就想提取欧洲联赛的数据来做一个分析。许多的网站都提供了详细的足球数据，例如：   网易&nbsp;http://goal.sports.163.com/&nbsp;  腾讯体育&nbsp;http://soccerdata.sports.qq.com/&nbsp;  虎扑体育&nbsp;http://soccer.hupu.com/&nbsp;  http://www.football-data.co.uk/&nbsp;   这些网站都提供了详细的足球数据，然而为了进一步的分析，我们希望数据以格式化的形式存储，那么如何把这些网站提供的网页数据转换成格式化的数据呢？这就要用到Web scraping的技术了。简单地说，Web Scraping就是从网站抽取信息， 通常利用程序来模拟人浏览网页的过程，发送http请求，从http响应中获得结果。 Web Scraping 注意事项 在抓取数据之前，要注意以下几点：   阅读网站有关数据的条款和约束条件，搞清楚数据的拥有权和使用限制  友好而礼貌，使用计算机发送请求的速度飞人类阅读可比，不要发送非常密集的大量请求以免造成服务器压力过大  因为网站经常会调整网页的结构，所以你之前写的Scraping代码，并不总是能够工作，可能需要经常调整  因为从网站抓取的数据可能存在不一致的情况，所以很有可能需要手工调整  Python Web Scraping 相关的库 Python提供了很便利的Web Scraping基础，有很多支持的库。这里列出一小部分   BeautifulSoup&nbsp;http://www.crummy.com/software/BeautifulSoup/&nbsp;  Scrapy&nbsp;http://scrapy.org/&nbsp;   webscraping&nbsp;https://code.google.com/p/webscraping/&nbsp;  pyquery&nbsp;https://pypi.python.org/pypi/pyquery&nbsp;  当然也不一定要用Python或者不一定要自己写代码，推荐关注import.io  Web Scraping 代码 下面，我们就一步步地用Python，从腾讯体育来抓取欧洲联赛13/14赛季的数据。  首先要安装Beautifulsoup pip&nbsp;install&nbsp;beautifulsoup4  我们先从球员的数据开始抓取。 球员数据的Web请求是http://soccerdata.sports.qq.com/playerSearch.aspx?lega=epl&amp;pn=2&nbsp;，返回的内容如下图所示：   该web服务有两个参数，lega表示是哪一个联赛，pn表示的是分页的页数。 首先我们先做一些初始化的准备工作 from&nbsp;urllib2&nbsp;import&nbsp;urlopenimport&nbsp;urlparseimport&nbsp;bs4BASE_URL&nbsp;=&nbsp;&quot;http://soccerdata.sports.qq.com&quot;PLAYER_LIST_QUERY&nbsp;=&nbsp;&quot;/playerSearch.aspx?lega=%s&amp;pn=%d&quot;league&nbsp;=&nbsp;['epl','seri','bund','liga','fran','scot','holl','belg']page_number_limit&nbsp;=&nbsp;100player_fields&nbsp;=&nbsp;['league_cn','img','name_cn','name','team','age','position_cn','nation','birth','query','id','teamid','league']  urlopen,urlparse,bs4是我们将要使用的Python库。 BASE_URL,PLAYER_LIST_QUERY,league,page_number_limit和player_fields是我们会用到的一些常量。 下面是抓取球员数据的具体代码： def&nbsp;get_players(baseurl):&nbsp;&nbsp;&nbsp;&nbsp;html&nbsp;=&nbsp;urlopen(baseurl).read()&nbsp;&nbsp;&nbsp;&nbsp;soup&nbsp;=&nbsp;bs4.BeautifulSoup(html,&nbsp;&quot;lxml&quot;)&nbsp;&nbsp;&nbsp;&nbsp;players&nbsp;=&nbsp;[&nbsp;dd&nbsp;for&nbsp;dd&nbsp;in&nbsp;soup.select('.searchResult&nbsp;tr')&nbsp;if&nbsp;dd.contents[1].name&nbsp;!=&nbsp;'th']&nbsp;&nbsp;&nbsp;&nbsp;result&nbsp;=&nbsp;[]&nbsp;&nbsp;&nbsp;&nbsp;for&nbsp;player&nbsp;in&nbsp;players:&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;record&nbsp;=&nbsp;[]&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;link&nbsp;=&nbsp;''&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;query&nbsp;=&nbsp;[]&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;for&nbsp;item&nbsp;in&nbsp;player.contents:&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;if&nbsp;type(item)&nbsp;is&nbsp;bs4.element.Tag:&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;if&nbsp;not&nbsp;item.string&nbsp;and&nbsp;item.img:&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;record.append(item.img['src'])&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;else&nbsp;:&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;record.append(item.string&nbsp;and&nbsp;item.string.strip()&nbsp;or&nbsp;'na')&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;try:&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;o&nbsp;=&nbsp;urlparse.urlparse(item.a['href']).query&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;if&nbsp;len(link)&nbsp;==&nbsp;0:&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;link&nbsp;=&nbsp;o&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;query&nbsp;=&nbsp;dict([(k,v[0])&nbsp;for&nbsp;k,v&nbsp;in&nbsp;urlparse.parse_qs(o).items()])&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;except:&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;pass&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;if&nbsp;len(record)&nbsp;!=&nbsp;10:&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;for&nbsp;i&nbsp;in&nbsp;range(0,&nbsp;10&nbsp;-&nbsp;len(record)):&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;record.append('na')&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;record.append(unicode(link,'utf-8'))&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;record.append(unicode(query[&quot;id&quot;],'utf-8'))&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;record.append(unicode(query[&quot;teamid&quot;],'utf-8'))&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;record.append(unicode(query[&quot;lega&quot;],'utf-8'))&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;result.append(record)&nbsp;&nbsp;&nbsp;&nbsp;return&nbsp;result&nbsp;&nbsp;&nbsp;&nbsp;result&nbsp;=&nbsp;[]for&nbsp;url&nbsp;in&nbsp;[&nbsp;BASE_URL&nbsp;+&nbsp;PLAYER_LIST_QUERY&nbsp;%&nbsp;(l,n)&nbsp;for&nbsp;l&nbsp;in&nbsp;league&nbsp;for&nbsp;n&nbsp;in&nbsp;range(page_number_limit)&nbsp;]:&nbsp;&nbsp;&nbsp;&nbsp;result&nbsp;=&nbsp;result&nbsp;+&nbsp;&nbsp;get_players(url)  我们来看看抓取球员数据的详细过程： 首先我们定义了一个get_players方法，该方法会返回某一请求页面上所有球员的数据。为了得到所有的数据，我们通过一个for循环，因为要循环各个联赛，每个联赛又有多个分页，一般情况下是需要一个双重循环的： for&nbsp;i&nbsp;in&nbsp;league:&nbsp;&nbsp;&nbsp;&nbsp;for&nbsp;j&nbsp;in&nbsp;range(0,&nbsp;100):&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;url&nbsp;=&nbsp;BASE_URL&nbsp;+&nbsp;PLAYER_LIST_QUERY&nbsp;%&nbsp;(l,n)&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;##&nbsp;send&nbsp;request&nbsp;to&nbsp;url&nbsp;and&nbsp;do&nbsp;scraping  Python的list comprehension可以很方便的通过构造一个列表的方式来减少循环的层次。 另外Python还有一个很方便的语法来合并连个列表： list = list1 + list2 好我们再看看如何使用BeautifulSoup来抓取网页中我们需要的内容。 首先调用urlopen读取对应url的内容，通常是一个html，用该html构造一个beautifulsoup对象。 beautifulsoup对象支持很多查找功能，也支持类似css的selector。通常如果有一个DOM对象是&lt;xx class='cc'&gt;,我们使用以下方式来查找： obj&nbsp;=&nbsp;soup.find(&quot;xx&quot;,&quot;cc&quot;)  另外一种常见的方式就是通过CSS的selector方式，在上述代码中，我们选择class=searchResult元素里面，所有的tr元素，过滤掉th也就是表头元素。 for&nbsp;dd&nbsp;in&nbsp;soup.select('.searchResult&nbsp;tr')&nbsp;if&nbsp;dd.contents[1].name&nbsp;!=&nbsp;'th'    对于每一行记录tr，生成一条球员记录，并存放在一个列表中。所以我们就循环tr的内容tr.contents,获得对应的field内容。  对于每一个tr的content，我们先检查其类型是不是一个Tag,对于Tag类型有几种情况，一种是包含img的情况，我们需要取出球员的头像图片的网址。   另一种是包含了一个链接，指向其他数据内容   所以在代码中要分别处理这些不同的情况。 对于一个Tag对象，Tag.x可以获得他的子对象，Tag['x']可以获得Tag的attribute的值。 所以用item.img['src']可以获得item的子元素img的src属性。 对已包含链接的情况，我们通过urlparse来获取查询url中的参数。这里我们利用了dict comprehension的把查询参数放入一个dict中，然后添加到列表中。 dict([(k,v[0])&nbsp;for&nbsp;k,v&nbsp;in&nbsp;urlparse.parse_qs(o).items()])  对于其它情况，我们使用Python 的and or表达式以确保当Tag的内容为空时，我们写入‘na’，该表达式类似C/C++或Java中的三元操作符 X ? A : B 然后有一段代码判断当前记录的长度是否大于10，不大于10则用空值填充，目的是避免一些不一致的地方。 if&nbsp;len(record)&nbsp;!=&nbsp;10:&nbsp;&nbsp;&nbsp;&nbsp;for&nbsp;i&nbsp;in&nbsp;range(0,&nbsp;10&nbsp;-&nbsp;len(record)):&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;record.append('na')  最后，我们把query中的一些相关的参数如球员的id，球队的id，所在的联赛代码等加入到列表。 record.append(unicode(link,'utf-8'))record.append(unicode(query[&quot;id&quot;],'utf-8'))record.append(unicode(query[&quot;teamid&quot;],'utf-8'))record.append(unicode(query[&quot;lega&quot;],'utf-8'))  最后我们把本页面所有球员的列表放入一个列表返回。 好了，现在我们拥有了一个包含所有球员的信息的列表，我们需要把它存下来，以进一步的处理，分析。通常，csv格式是一个常见的选择。 import&nbsp;csvdef&nbsp;write_csv(filename,&nbsp;content,&nbsp;header&nbsp;=&nbsp;None):&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;file&nbsp;=&nbsp;open(filename,&nbsp;&quot;wb&quot;)&nbsp;&nbsp;&nbsp;&nbsp;file.write('\xEF\xBB\xBF')&nbsp;&nbsp;&nbsp;&nbsp;writer&nbsp;=&nbsp;csv.writer(file,&nbsp;delimiter=',')&nbsp;&nbsp;&nbsp;&nbsp;if&nbsp;header:&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;writer.writerow(header)&nbsp;&nbsp;&nbsp;&nbsp;for&nbsp;row&nbsp;in&nbsp;content:&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;encoderow&nbsp;=&nbsp;[dd.encode('utf8')&nbsp;for&nbsp;dd&nbsp;in&nbsp;row]&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;writer.writerow(encoderow)write_csv('players.csv',result,player_fields)  这里需要注意的就是关于encode的问题。因为我们使用的时utf-8的编码方式，在csv的文件头，需要写入\xEF\xBB\xBF，详见这篇文章  好了现在大功告成，抓取的csv如下图：   因为之前我们还抓取了球员本赛季的比赛详情，所以我们可以进一步的抓取所有球员每一场比赛的记录   抓取的代码如下 def&nbsp;get_player_match(url):&nbsp;&nbsp;&nbsp;&nbsp;html&nbsp;=&nbsp;urlopen(url).read()&nbsp;&nbsp;&nbsp;&nbsp;soup&nbsp;=&nbsp;bs4.BeautifulSoup(html,&nbsp;&quot;lxml&quot;)&nbsp;&nbsp;&nbsp;&nbsp;matches&nbsp;=&nbsp;[&nbsp;dd&nbsp;for&nbsp;dd&nbsp;in&nbsp;soup.select('.shtdm&nbsp;tr')&nbsp;if&nbsp;dd.contents[1].name&nbsp;!=&nbsp;'th']&nbsp;&nbsp;&nbsp;&nbsp;records&nbsp;=&nbsp;[]&nbsp;&nbsp;&nbsp;&nbsp;for&nbsp;item&nbsp;in&nbsp;[&nbsp;dd&nbsp;for&nbsp;dd&nbsp;in&nbsp;matches&nbsp;if&nbsp;len(dd.contents)&nbsp;&gt;&nbsp;11]:&nbsp;##&nbsp;filter&nbsp;out&nbsp;the&nbsp;personal&nbsp;part&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;record&nbsp;=&nbsp;[]&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;for&nbsp;match&nbsp;in&nbsp;[&nbsp;dd&nbsp;for&nbsp;dd&nbsp;in&nbsp;item.contents&nbsp;if&nbsp;type(dd)&nbsp;is&nbsp;bs4.element.Tag]:&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;if&nbsp;match.string:&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;record.append(match.string)&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;else:&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;for&nbsp;d&nbsp;in&nbsp;[&nbsp;dd&nbsp;for&nbsp;dd&nbsp;in&nbsp;match.contents&nbsp;if&nbsp;type(dd)&nbsp;is&nbsp;bs4.element.Tag]:&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;query&nbsp;=&nbsp;dict([(k,v[0])&nbsp;for&nbsp;k,v&nbsp;in&nbsp;urlparse.parse_qs(d['href']).items()])&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;record.append('teamid'&nbsp;in&nbsp;query&nbsp;and&nbsp;query['teamid']&nbsp;or&nbsp;query['id'])&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;record.append(d.string&nbsp;and&nbsp;d.string&nbsp;or&nbsp;'na')&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;records.append(record)&nbsp;&nbsp;&nbsp;&nbsp;return&nbsp;records[1:]&nbsp;&nbsp;##remove&nbsp;the&nbsp;first&nbsp;record&nbsp;as&nbsp;the&nbsp;headerdef&nbsp;get_players_match(playerlist,&nbsp;baseurl&nbsp;=&nbsp;BASE_URL&nbsp;+&nbsp;'/player.aspx?'):&nbsp;&nbsp;&nbsp;&nbsp;result&nbsp;=&nbsp;[]&nbsp;&nbsp;&nbsp;&nbsp;for&nbsp;item&nbsp;in&nbsp;playerlist:&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;url&nbsp;=&nbsp;&nbsp;baseurl&nbsp;+&nbsp;item[10]&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;print&nbsp;url&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;result&nbsp;=&nbsp;result&nbsp;+&nbsp;get_player_match(url)&nbsp;&nbsp;&nbsp;&nbsp;return&nbsp;resultmatch_fields&nbsp;=&nbsp;['date_cn','homeid','homename_cn','matchid','score','awayid','awayname_cn','league_cn','firstteam','playtime','goal','assist','shoot','run','corner','offside','foul','violation','yellowcard','redcard','save']&nbsp;&nbsp;&nbsp;&nbsp;write_csv('m.csv',get_players_match(result),match_fields)  抓取的过程和之前类似。 下一步做什么 现在我们拥有了详细的欧洲联赛的数据，那么下一步要怎么做呢，我推荐大家把数据导入BI工具来做进一步的分析。有两个比较好的选择：   Tableau Public  Tableau在数据可视化领域可谓无出其右，Tableau Public完全免费，用数据可视化来驱动数据的探索和分析，拥有非常好的用户体验     Splunk  Splunk提供一个大数据的平台，主要面向机器数据。支持每天免费导入500M的数据，如果是个人学习，应该足够了。 当然你也可以用Excel。 另外大家如果有什么好的免费的数据分析的平台，欢迎交流。</div>
                            <div class='commentList'>
                    <h3>评论列表</h3>
                                    </div>
                    </div>
    
        <div class='blog'>
            <h2><a href="#top" class='top'>回到顶部</a><a name="blog_271193" href="https://my.oschina.net/taogang/blog/271193">这些年，我穿过的那些队服</a></h2>
            <div class='outline'>
                <div class='date'>时间：2014-05-31 00:03:04</div>
                <div class='catalog'>分类：日常记录</div>
                                                                            </div>
            <div class='content'>世界杯将近，晒一下毕业后穿过球衣，缅怀一下青春。1997~ 首先映入眼帘的这件阿根廷队服，这是仍然保存的年代最久的一件球衣了，依稀是研究生时的队服，15，6年了，现在我仍然有机会就会穿上这件球衣。2003~2007 这是在SAP B1时代的队服，英格兰。从那时起，我更多的使用14号球衣  这是队友同事在欧洲带的米兰队服，卡卡不错，我更喜欢巴斯滕。2003~  这是我在小区俱乐部队的球衣，那时我是17号，那支球队很不错，大家都很有热情，很多队员还来我的婚礼帮忙，然而后来不知为什么，活动少了，我和球队也失去了联系。至今，我还能在qq群上看到球队的消息，队伍似乎壮大了不少，可是物是人非，当年的小伙伴们，已经没有剩下几个了。 2011~  2011年重归SAP，加入BOBJ足球队，我仍然是14号。那一年我们选择了曼城；那一年，我们勇夺公司联赛冠军；曼城也随后夺得英超的冠军。后来我们继续蝉联了一届冠军。可惜两次夺冠，我的贡献都不多，只有一些替补登场，只记得两次替补登场，都是在第一脚触球打入进球。后来由于关键球星的离去，球队未能继续辉煌。 大家注意，贴的字不能机洗，字会掉！ 这是夺冠后的奖励2013~  去年，加入了一个新的球队-上海万里老男孩足球俱乐部，有组织真好，这是球队主要赞助商林特赞助的恒大队服。我们球队的气氛非常好，我很喜欢和大家一起踢球，赞！  球队另一主要赞助商味它赞助的意大利队服。同时，还联系上了大学的校友会，在我们的世界杯抽签，我们签到了伊朗，不过太忙，还么来得及和队友合练。  最后是我现在公司球队，还是14号。顺便插播广告，splunk是业内领先的机器数据大数据平台提供商，公司待遇优厚，球队实力一流。有愿意加入的联系我：）23是我和我女儿的学号 ：）容颜易老，青春永恒~</div>
                            <div class='commentList'>
                    <h3>评论列表</h3>
                                    </div>
                    </div>
    
        <div class='blog'>
            <h2><a href="#top" class='top'>回到顶部</a><a name="blog_279402" href="https://my.oschina.net/taogang/blog/279402">用Python做单变量数据集的异常点分析</a></h2>
            <div class='outline'>
                <div class='date'>时间：2014-06-13 12:59:55</div>
                <div class='catalog'>分类：机器学习</div>
                                                                            </div>
            <div class='content'>大数据时代，数据的异常分析被广泛的用于各个场合。 今天我们就来看一看其中的一种场景，对于单变量数据集的异常检测。 所谓单变量，就是指数据集中只有一个变化的值，下面我们来看看今天我们要分析的的数据，点击这里数据文件下载数据文件。 分析数据的第一步是要加载文件， 本文使用了numpy，pandas，scikit learn等常见的数据分析要用到的Python库。 import&nbsp;numpy&nbsp;as&nbsp;npimport&nbsp;pandas&nbsp;as&nbsp;pddf&nbsp;=&nbsp;pd.read_csv("farequote.csv") &nbsp; Pandas 是一个常用的数据分析的Python库，提供对数据的加载，清洗，抽取，变形等操作。Pandas依赖numpy，numpy提供了基于列/多维数组（List/N-D Array）的数据结构的操作。许多科学计算和数据分析的库都依赖于numpy。 df 是Pandas中常用的数据类型dataframe，dataframe类似与一个数据库的表，使用 df.head()可以得到数据的头几行，以便了解数据的概貌。  该数据结构中，第一列式Pandas添加的索引，第一行是每一列数据的名字，除了第一列，每一列数据可以看成是一个变量，所以该数据集共有三个变量，时间（_time）、航空公司名称(airline)、响应时间（responsetime）。我们可以这样理解，该数据集记录了一段时间内，各个航空公司飞机延误的时间。我们希望通过分析找出是否存在异常的情况。 注意，我们是要分析单变量，所以所有的分析都是基于某一个航空公司的数据，所以就需要对该数据集做一个查询，找出要分析的航空公司。首先要知道有哪些航空公司，使用np.unique(df.airline)可以找到所有的航空公司代码，类似SQL的Unique命令 array(['AAL',&nbsp;'ACA',&nbsp;'AMX',&nbsp;'ASA',&nbsp;'AWE',&nbsp;'BAW',&nbsp;'DAL',&nbsp;'EGF',&nbsp;'FFT',&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;'JAL',&nbsp;'JBU',&nbsp;'JZA',&nbsp;'KLM',&nbsp;'NKS',&nbsp;'SWA',&nbsp;'SWR',&nbsp;'TRS',&nbsp;'UAL',&nbsp;'VRD'],&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;dtype='|S3') &nbsp; 查询某个航空公司的数据使用dataframe的query方法，类似SQL的select。Query返回的结果仍然是一个dataframe对象。 dd&nbsp;=&nbsp;df.query('airline=="KLM"')&nbsp;##&nbsp;得到法航的数据 &nbsp; 我们先了解一下数据的大致信息，使用describe方法 dd.responsetime.describe() &nbsp; 得到如下的结果： count&nbsp;&nbsp;&nbsp;&nbsp;1724.000000mean&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;1500.613766std&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;100.085320min&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;1209.76680025%&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;1434.08462550%&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;1499.13500075%&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;1567.831025max&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;1818.774100Name:&nbsp;responsetime,&nbsp;dtype:&nbsp;float64 &nbsp; 该结果返回了数据集responsetime维度上的主要统计指标，个数，均值，方差，最大最小值等等，也可以调用单独的方法例如min（），mean（）等来获得某一个指标。 基于标准差得异常检测 下面我们就可以开始异常点的分析了，对于单变量的异常点分析，最容易想到的就是基于标准差（Standard Deviation）的方法了。我们假定数据的正态分布的，利用概率密度函数，我们知道    95.449974面积在平均数左右两个标准差的范围内    99.730020%的面积在平均数左右三个标准差的范围内    99.993666的面积在平均数左右三个标准差的范围内    所以我们95%也就是大概两个标准差为门限，凡是落在门限外的都认为是异常点。代码如下 def&nbsp;a1(dataframe,&nbsp;threshold=.95):&nbsp;&nbsp;&nbsp;&nbsp;d&nbsp;=&nbsp;dataframe['responsetime']&nbsp;&nbsp;&nbsp;&nbsp;dataframe['isAnomaly']&nbsp;=&nbsp;d&nbsp;&gt;&nbsp;d.quantile(threshold)&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;return&nbsp;dataframeprint&nbsp;a1(dd) &nbsp; 运行以上程序我们得到如下结果 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;_time&nbsp;airline&nbsp;&nbsp;responsetime&nbsp;isAnomaly20&nbsp;&nbsp;&nbsp;&nbsp;2013-02-01T23:57:59.000-0700&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;KLM&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;1481.4945&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;False76&nbsp;&nbsp;&nbsp;&nbsp;2013-02-01T23:52:34.000-0700&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;KLM&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;1400.9050&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;False124&nbsp;&nbsp;&nbsp;2013-02-01T23:47:10.000-0700&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;KLM&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;1501.4313&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;False203&nbsp;&nbsp;&nbsp;2013-02-01T23:39:08.000-0700&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;KLM&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;1278.9509&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;False281&nbsp;&nbsp;&nbsp;2013-02-01T23:32:27.000-0700&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;KLM&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;1386.4157&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;False336&nbsp;&nbsp;&nbsp;2013-02-01T23:26:09.000-0700&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;KLM&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;1629.9589&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;False364&nbsp;&nbsp;&nbsp;2013-02-01T23:23:52.000-0700&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;KLM&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;1482.5900&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;False448&nbsp;&nbsp;&nbsp;2013-02-01T23:16:08.000-0700&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;KLM&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;1553.4988&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;False511&nbsp;&nbsp;&nbsp;2013-02-01T23:10:39.000-0700&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;KLM&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;1555.1894&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;False516&nbsp;&nbsp;&nbsp;2013-02-01T23:10:08.000-0700&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;KLM&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;1720.7862&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;True553&nbsp;&nbsp;&nbsp;2013-02-01T23:06:29.000-0700&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;KLM&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;1306.6489&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;False593&nbsp;&nbsp;&nbsp;2013-02-01T23:03:03.000-0700&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;KLM&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;1481.7081&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;False609&nbsp;&nbsp;&nbsp;2013-02-01T23:01:29.000-0700&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;KLM&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;1521.0253&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;False666&nbsp;&nbsp;&nbsp;2013-02-01T22:56:04.000-0700&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;KLM&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;1675.2222&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;True...&nbsp;&nbsp;&nbsp;...&nbsp;&nbsp;&nbsp;...&nbsp;&nbsp;&nbsp;... &nbsp; 结果数据集上多了一列isAnomaly用来标记每一行记录是否是异常点，我们看到已经有一些点被标记为异常点了。 我们看看程序的详细内容：    方法a1定义了一个异常检测的函数    dataframe['responsetime']等价于dataframe.responsetime,该操作取出responsetime这一列的值    d.quantile(threshold)用正态分布假定返回位于95%的点的值，大于该值得点都落在正态分布95%之外    d &gt;&nbsp;d.quantile(threshold)是一个数组操作，返回的新数组是responsetime和threshold的比较结果，[False,False,True,... ... False]    然后通过dataframe的赋值操作增加一个新的列，标记所有的异常点。   数据可视化往往是数据分析的最后一步，我们看看结果如何： import&nbsp;matplotlib.pyplot&nbsp;as&nbsp;pltda&nbsp;=&nbsp;a1(dd)fig&nbsp;=&nbsp;plt.figure()ax1&nbsp;=&nbsp;fig.add_subplot(2,&nbsp;1,&nbsp;1)ax2&nbsp;=&nbsp;fig.add_subplot(2,&nbsp;1,&nbsp;2)ax1.plot(da['responsetime'])ax2.plot(da['isAnomaly']) &nbsp;  这异常点也太多了，用99%在试试：  现在似乎好一点，然而我们知道，对于数据集的正态分布的假定往往是不成立的,假如数据分布在大小两头，那么这样的异常检测就很难奏效了。我们看看其他一些改进的方法。 基于ZSCORE的异常检测 zscore的计算如下  sd是标准差，X是均值。一般建议门限值取为3.5 代码如下： def&nbsp;a2(dataframe,&nbsp;threshold=3.5):&nbsp;&nbsp;&nbsp;&nbsp;d&nbsp;=&nbsp;dataframe['responsetime']&nbsp;&nbsp;&nbsp;&nbsp;zscore&nbsp;=&nbsp;(d&nbsp;-&nbsp;d.mean())/d.std()&nbsp;&nbsp;&nbsp;&nbsp;dataframe['isAnomaly']&nbsp;=&nbsp;zscore.abs()&nbsp;&gt;&nbsp;threshold&nbsp;&nbsp;&nbsp;&nbsp;return&nbsp;dataframe &nbsp; &nbsp; &nbsp; 另外还有一种增强的zscore算法，基于MAD。MAD的定义是  其中X是中位数。  增强的zscore算法如下： def&nbsp;a3(dataframe,&nbsp;threshold=3.5):&nbsp;&nbsp;&nbsp;&nbsp;dd&nbsp;=&nbsp;dataframe['responsetime']&nbsp;&nbsp;&nbsp;&nbsp;MAD&nbsp;=&nbsp;(dd&nbsp;-&nbsp;dd.median()).abs().median()&nbsp;&nbsp;&nbsp;&nbsp;zscore&nbsp;=&nbsp;((dd&nbsp;-&nbsp;dd.median())*&nbsp;0.6475&nbsp;/MAD).abs()&nbsp;&nbsp;&nbsp;&nbsp;dataframe['isAnomaly']&nbsp;=&nbsp;zscore&nbsp;&gt;&nbsp;threshold&nbsp;&nbsp;&nbsp;&nbsp;return&nbsp;dataframe &nbsp; 用zscore算法得到：  调整门限为3得到  如果换一组数据AAL，结果会怎么样呢?  我们发现有一段时间，所有的响应都很慢，我们想要把这些点都标记为异常，可能么？ 基于KMEAN聚集的异常检测 通常基于KMEAN的聚集算法并不适用于异常点检测，以为聚集算法总是试图平衡每一个聚集中的点的数目，所以对于少数的异常点，聚集非常不好用，但是我们这个例子中，异常点都聚在一起，所以应该可以使用。 首先，为了看清聚集，我们使用时间序列的常用分析方法，增加一个维度，该维度是每一个点得前一个点得响应时间。 preresponse&nbsp;=&nbsp;0newcol&nbsp;=&nbsp;[]newcol.append(0)for&nbsp;index,&nbsp;row&nbsp;in&nbsp;dd.iterrows():&nbsp;&nbsp;&nbsp;&nbsp;if&nbsp;preresponse&nbsp;!=&nbsp;0:&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;newcol.append(preresponse)&nbsp;&nbsp;&nbsp;&nbsp;preresponse&nbsp;=&nbsp;row.responsetimedd["t0"]&nbsp;=&nbsp;newcolplt.scatter(dd.t0,dd.responsetime) &nbsp; 我们利用iterrows来循环数据，把前一个点的响应时间增加到当前点，第一个点的该值为0，命名该列为t0。然后用scatter plot把它画出来。  上面是法航KLM的数据，其中最左边的点是一个无效的点，因为前一个点的响应时间不知道所以填了0，分析时应该过滤该店。 对于AAL，我们可以清楚的看到两个聚集：  其中右上方的聚集，也就是点数目比较少得聚集就是我们希望检测到的异常点得集合。 我们看看如何使用KMEAN算法来检测吧： def&nbsp;a4(dataframe,&nbsp;threshold&nbsp;=&nbsp;.9):&nbsp;&nbsp;&nbsp;&nbsp;##&nbsp;add&nbsp;one&nbsp;dimention&nbsp;of&nbsp;previous&nbsp;response&nbsp;&nbsp;&nbsp;&nbsp;preresponse&nbsp;=&nbsp;0&nbsp;&nbsp;&nbsp;&nbsp;newcol&nbsp;=&nbsp;[]&nbsp;&nbsp;&nbsp;&nbsp;newcol.append(0)&nbsp;&nbsp;&nbsp;&nbsp;for&nbsp;index,&nbsp;row&nbsp;in&nbsp;dataframe.iterrows():&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;if&nbsp;preresponse&nbsp;!=&nbsp;0:&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;newcol.append(preresponse)&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;preresponse&nbsp;=&nbsp;row.responsetime&nbsp;&nbsp;&nbsp;&nbsp;dataframe["t0"]&nbsp;=&nbsp;newcol&nbsp;&nbsp;&nbsp;&nbsp;##&nbsp;remove&nbsp;first&nbsp;row&nbsp;as&nbsp;there&nbsp;is&nbsp;no&nbsp;previous&nbsp;event&nbsp;for&nbsp;time&nbsp;&nbsp;&nbsp;&nbsp;dd&nbsp;=&nbsp;dataframe.drop(dataframe.head(1).index)&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;clf&nbsp;=&nbsp;cluster.KMeans(n_clusters=2)&nbsp;&nbsp;&nbsp;&nbsp;X=np.array(dd[['responsetime','t0']])&nbsp;&nbsp;&nbsp;&nbsp;cls&nbsp;=&nbsp;clf.fit_predict(X)&nbsp;&nbsp;&nbsp;&nbsp;freq&nbsp;=&nbsp;itemfreq(cls)&nbsp;&nbsp;&nbsp;&nbsp;(A,B)&nbsp;=&nbsp;(freq[0,1],freq[1,1])&nbsp;&nbsp;&nbsp;&nbsp;t&nbsp;=&nbsp;abs(A-B)/max(A,B)&nbsp;&nbsp;&nbsp;&nbsp;if&nbsp;t&nbsp;&gt;&nbsp;threshold&nbsp;:&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;##&nbsp;"Anomaly&nbsp;Detected!"&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;index&nbsp;=&nbsp;freq[0,0]&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;if&nbsp;A&nbsp;&gt;&nbsp;B&nbsp;:&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;index&nbsp;=&nbsp;freq[1,0]&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;dd['isAnomaly']&nbsp;=&nbsp;(cls&nbsp;==&nbsp;index)&nbsp;&nbsp;&nbsp;&nbsp;else&nbsp;:&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;##&nbsp;"No&nbsp;Anomaly&nbsp;Point"&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;dd['isAnomaly']&nbsp;=&nbsp;False&nbsp;&nbsp;&nbsp;&nbsp;return&nbsp;dd &nbsp; 其核心代码是以下这几行： clf&nbsp;=&nbsp;cluster.KMeans(n_clusters=2)X=np.array(dd[['responsetime','t0']])cls&nbsp;=&nbsp;clf.fit_predict(X) &nbsp; cluster.KMeans返回一个预测模型，我们假定有两个聚集。你可以试着加大聚集的数量，结果没什么影响。 dd[['responsetime','t0']]返回一个2*n的数组，并赋值给X，用于聚集计算。 fit_pridict方法是对X做聚集运算,并计算每一个点对应的聚集编号。 freq&nbsp;=&nbsp;itemfreq(cls) &nbsp; itemfreq返回聚集结果中每一个聚集的发生频率，如果其中一个比另一个显著地多，我们则认为那个少得是异常点聚集。 用该方法可以把所有聚集里的点标记为异常点。  这里我用红色标记结果让大家看的清楚一点，注意因为是line chart，连个竖线间的都是异常点。 总结 除了上述的算法，还有其它一些相关的算法，大家如果对背后的数据知识有兴趣的话，可以参考这篇相关介绍。 单变量的异常检测算法相对比较简单，但是要做到精准检测就更难，因为掌握的信息更少。另外boxplot也经常被用于异常检测，他和基于方差的异常检测是一致的，只不过用图形让大家一目了然的获得结果，大家有兴趣可以了解一下。 &nbsp;</div>
                            <div class='commentList'>
                    <h3>评论列表</h3>
                                    </div>
                    </div>
    
        <div class='blog'>
            <h2><a href="#top" class='top'>回到顶部</a><a name="blog_285605" href="https://my.oschina.net/taogang/blog/285605">在Mac OS X上构建wget来抓取静态网站内容</a></h2>
            <div class='outline'>
                <div class='date'>时间：2014-06-30 17:02:27</div>
                <div class='catalog'>分类：工作日志</div>
                                                                            </div>
            <div class='content'>我们的目标是在Mac OS上获取一个静态服务器的内容，通常用wget是一个很好的选择。wget是一个命令行工具用于从网络服务器来获取内容。但是在Mac OS X（Mountain Lion/ Mavericks / Snow Leopard）上没有提供该工具，但是有curl。wget VS curlcurl基于跨平台的库libcurl支持unix管道返回错误代码来支持错误处理只返回单个url的内容，不支持自动取链接的内容大量协议支持诸如 ：FTP, FTPS, HTTP, HTTPS, SCP, SFTP, TFTP, TELNET, DICT, LDAP, LDAPS, FILE, POP3, IMAP, SMTP, RTMP and RTSP可移植性好支持不同的SSL/TSL库支持HTTP认证（HTTP Authentication）支持双向和多部分提交数据支持压缩MIT协议wget只支持命令行支持递归的抓取数据，也就是说可以抓取返回内容中的url链接的内容。非常古老，开发不活跃。使用HTTP 1.0GNU项目的一部分GPL 协议总体而言curl比wget要进步许多，可是要获取一个网站的镜像，迭代功能必不可少。只好自己动手，在Mac上构建一个wget。构建wget首先确定你已经安装了Xcode和GCC，如果不知道如何安装，可以参考这个链接。然后从gnu下载wget的源码curl&nbsp;-O&nbsp;http://ftp.gnu.org/gnu/wget/wget-1.15.tar.gz下载好后，解压缩tar&nbsp;-xvf&nbsp;wget-1.15.tar.gz解压缩好后，需要运行配置命令，为编译做准备cd&nbsp;wget-1.15./configure&nbsp;--with-ssl=openssl这里我们选用openssl作为ssl的参数选项。大家一定不会忘记最近发生的openssl的heartbleed漏洞吧 ：）配置好了以后，运行makemake这里不出意外会跳出一大堆的警告，不要担心，如果你看到如下的内容，你应该编译成功了...&nbsp;......&nbsp;...gcc&nbsp;&nbsp;-O2&nbsp;-Wall&nbsp;&nbsp;&nbsp;-o&nbsp;wget&nbsp;cmpt.o&nbsp;connect.o&nbsp;convert.o&nbsp;cookies.o&nbsp;ftp.o&nbsp;css_.o&nbsp;css-url.o&nbsp;ftp-basic.o&nbsp;ftp-ls.o&nbsp;hash.o&nbsp;host.o&nbsp;html-parse.o&nbsp;html-url.o&nbsp;http.o&nbsp;init.o&nbsp;log.o&nbsp;main.o&nbsp;netrc.o&nbsp;progress.o&nbsp;ptimer.o&nbsp;recur.o&nbsp;res.o&nbsp;retr.o&nbsp;spider.o&nbsp;url.o&nbsp;warc.o&nbsp;utils.o&nbsp;exits.o&nbsp;build_info.o&nbsp;&nbsp;version.o&nbsp;ftp-opie.o&nbsp;openssl.o&nbsp;http-ntlm.o&nbsp;../lib/libgnu.a&nbsp;-liconv&nbsp;&nbsp;-lssl&nbsp;-lcrypto&nbsp;-lz&nbsp;-ldl&nbsp;-lz&nbsp;-lzMaking&nbsp;all&nbsp;in&nbsp;doc./texi2pod.pl&nbsp;-D&nbsp;VERSION="1.15"&nbsp;./wget.texi&nbsp;wget.pod/usr/bin/pod2man&nbsp;--center="GNU&nbsp;Wget"&nbsp;--release="GNU&nbsp;Wget&nbsp;1.14"&nbsp;wget.pod&nbsp;&gt;&nbsp;wget.1Making&nbsp;all&nbsp;in&nbsp;poMaking&nbsp;all&nbsp;in&nbsp;testsmake[2]:&nbsp;Nothing&nbsp;to&nbsp;be&nbsp;done&nbsp;for&nbsp;`all'.Making&nbsp;all&nbsp;in&nbsp;utilmake[2]:&nbsp;Nothing&nbsp;to&nbsp;be&nbsp;done&nbsp;for&nbsp;`all'.make[2]:&nbsp;Nothing&nbsp;to&nbsp;be&nbsp;done&nbsp;for&nbsp;`all-am'.最后，安装sudo&nbsp;make&nbsp;install安装成功后，试一试wget是否成功安装$&nbsp;which&nbsp;wget/usr/local/bin/wget如果看到上述结果说明wget已经成功构建并部署到/usr/local/bin目录了好了，万事具备，可以开始抓取你想要获得内容的网站了。wget&nbsp;-mk&nbsp;http://website.com其中-m参数表示迭代的抓取，-k参数表示用相对路径取代绝对路径。抓取的内容会被存放在本地的website.com的目录下。举个例子，比如我要抓新浪新闻$&nbsp;wget&nbsp;-mk&nbsp;http://news.sina.com.cn--2014-06-30&nbsp;16:55:26--&nbsp;&nbsp;http://news.sina.com.cn/Resolving&nbsp;news.sina.com.cn...&nbsp;58.63.236.31,&nbsp;58.63.236.46,&nbsp;58.63.236.48,&nbsp;...Connecting&nbsp;to&nbsp;news.sina.com.cn|58.63.236.31|:80...&nbsp;connected.HTTP&nbsp;request&nbsp;sent,&nbsp;awaiting&nbsp;response...&nbsp;200&nbsp;OKLength:&nbsp;636992&nbsp;(622K)&nbsp;[text/html]Saving&nbsp;to:&nbsp;‘news.sina.com.cn/index.html’100%[======================================&gt;]&nbsp;636,992&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;391KB/s&nbsp;&nbsp;&nbsp;in&nbsp;1.6s&nbsp;&nbsp;&nbsp;2014-06-30&nbsp;16:55:29&nbsp;(391&nbsp;KB/s)&nbsp;-&nbsp;‘news.sina.com.cn/index.html’&nbsp;saved&nbsp;[636992/636992]Loading&nbsp;robots.txt;&nbsp;please&nbsp;ignore&nbsp;errors.--2014-06-30&nbsp;16:55:29--&nbsp;&nbsp;http://news.sina.com.cn/robots.txtReusing&nbsp;existing&nbsp;connection&nbsp;to&nbsp;news.sina.com.cn:80.HTTP&nbsp;request&nbsp;sent,&nbsp;awaiting&nbsp;response...&nbsp;200&nbsp;OKLength:&nbsp;70&nbsp;[text/plain]Saving&nbsp;to:&nbsp;‘news.sina.com.cn/robots.txt’100%[======================================&gt;]&nbsp;70&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;--.-K/s&nbsp;&nbsp;&nbsp;in&nbsp;0.03s&nbsp;&nbsp;&nbsp;2014-06-30&nbsp;16:55:29&nbsp;(2.54&nbsp;KB/s)&nbsp;-&nbsp;‘news.sina.com.cn/robots.txt’&nbsp;saved&nbsp;[70/70]--2014-06-30&nbsp;16:55:29--&nbsp;&nbsp;http://news.sina.com.cn/js/792/2012-08-09/41/headnews.jsReusing&nbsp;existing&nbsp;connection&nbsp;to&nbsp;news.sina.com.cn:80.HTTP&nbsp;request&nbsp;sent,&nbsp;awaiting&nbsp;response...&nbsp;200&nbsp;OKLength:&nbsp;31699&nbsp;(31K)&nbsp;[application/x-javascript]Saving&nbsp;to:&nbsp;‘news.sina.com.cn/js/792/2012-08-09/41/headnews.js’100%[======================================&gt;]&nbsp;31,699&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;--.-K/s&nbsp;&nbsp;&nbsp;in&nbsp;0.04s&nbsp;&nbsp;&nbsp;2014-06-30&nbsp;16:55:29&nbsp;(731&nbsp;KB/s)&nbsp;-&nbsp;‘news.sina.com.cn/js/792/2012-08-09/41/headnews.js’&nbsp;saved&nbsp;[31699/31699]--2014-06-30&nbsp;16:55:29--&nbsp;&nbsp;http://news.sina.com.cn/pfpnews/js/libweb.jsReusing&nbsp;existing&nbsp;connection&nbsp;to&nbsp;news.sina.com.cn:80.HTTP&nbsp;request&nbsp;sent,&nbsp;awaiting&nbsp;response...&nbsp;200&nbsp;OKLength:&nbsp;6554&nbsp;(6.4K)&nbsp;[application/x-javascript]Saving&nbsp;to:&nbsp;‘news.sina.com.cn/pfpnews/js/libweb.js’100%[======================================&gt;]&nbsp;6,554&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;--.-K/s&nbsp;&nbsp;&nbsp;in&nbsp;0.03s抓成功后的目录如下注意：这样的方式仅适用于静态网站，对于使用动态代码生成的网站无能为力地址转换是发生在所有内容抓取完成之后，如果你中途退出，则所有的地址链接仍然指向原始地址。当你所要抓取的内容巨大时，需要非常小心。为了防止流量过大，对服务器造成太大的负担，可以使用-w参数设置两个请求中的间隔时间使用Brew另外一个很有效地方法是使用homebrew，homebrew是一个包管理工具。安装非常简单：ruby&nbsp;-e&nbsp;"$(curl&nbsp;-fsSL&nbsp;https://raw.githubusercontent.com/Homebrew/install/master/install)"安好了以后，运行$&nbsp;brew&nbsp;install&nbsp;wget就好了，非常方便</div>
                            <div class='commentList'>
                    <h3>评论列表</h3>
                                    </div>
                    </div>
    
        <div class='blog'>
            <h2><a href="#top" class='top'>回到顶部</a><a name="blog_286955" href="https://my.oschina.net/taogang/blog/286955">在ipython notebook中调用ggplot的三种不同的方法</a></h2>
            <div class='outline'>
                <div class='date'>时间：2014-07-03 15:35:07</div>
                <div class='catalog'>分类：数据可视化</div>
                                                                            </div>
            <div class='content'>在大数据时代，数据可视化是一个非常热门的话题。各个BI的厂商无不在数据可视化领域里投入大量的精力。Tableau凭借其强大的数据可视化的功能成为硅谷炙手可热的上市公司。Tableau的数据可视化的产品，其理论基础其实是《The Grammar of Graphic》，该书提出了对信息可视化的图表的语法抽象体系，数据的探索和分析可以由图像的语法来驱动，而非有固定的图表类型来驱动，使得数据的探索过程变得友好而有趣。 然而对于The Grammar of Graphic的理论的实践，并非Tableau独占，ggplot作为R语言上得一个图形库，其理论基础也是这本书。（注，笔者曾就职的某BI巨头，主要职责也是数据可视化，我们曾经和加拿大团队研发过类似的产品，基于HTML5和D3，可惜由于种种原因未能推向市场） 现在越来越多的人开始使用python来做数据分析，IPython Notebook尤其令人喜爱，它的实时交互把脚本语言的优势发挥到极致。那么怎样才能在IPython Notebook中使用ggplot呢？我这里跟大家分享三种不同的方式供大家选择。 RPy2  第一种方式是使用rpy2, rpy2是对rpy的改写和重新设计，旨在提供Python用户在python中使用R的API。 rpy2提供了对R语言的对象和方法的基本封装，当然也包括可视化的图库这一块。 下面就是一段运行ggplot的R程序使用rpy2在python中运行的例子： from&nbsp;rpy2&nbsp;import&nbsp;robjectsfrom&nbsp;rpy2.robjects&nbsp;import&nbsp;Formula,&nbsp;Environmentfrom&nbsp;rpy2.robjects.vectors&nbsp;import&nbsp;IntVector,&nbsp;FloatVectorfrom&nbsp;rpy2.robjects.lib&nbsp;import&nbsp;gridfrom&nbsp;rpy2.robjects.packages&nbsp;import&nbsp;importr,&nbsp;dataimport&nbsp;rpy2.robjects.lib.ggplot2&nbsp;as&nbsp;ggplot2#&nbsp;The&nbsp;R&nbsp;'print'&nbsp;functionrprint&nbsp;=&nbsp;robjects.globalenv.get(&quot;print&quot;)stats&nbsp;=&nbsp;importr('stats')grdevices&nbsp;=&nbsp;importr('grDevices')base&nbsp;=&nbsp;importr('base')datasets&nbsp;=&nbsp;importr('datasets')mtcars&nbsp;=&nbsp;data(datasets).fetch('mtcars')['mtcars']pp&nbsp;=&nbsp;ggplot2.ggplot(mtcars)&nbsp;+&nbsp;\&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;ggplot2.aes_string(x='wt',&nbsp;y='mpg',&nbsp;col='factor(cyl)')&nbsp;+&nbsp;\&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;ggplot2.geom_point()&nbsp;+&nbsp;\&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;ggplot2.geom_smooth(ggplot2.aes_string(group&nbsp;=&nbsp;'cyl'),&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;method&nbsp;=&nbsp;'lm')pp.plot()  以上程序在IPython Notebook中运行会有缺陷，会弹出一个新的窗口显示图，而且该python进程会阻塞在那里。我们希望图表能内嵌在IPython Notebook的页面中，为了解决该问题，我们引入如下代码： %matplotlib&nbsp;inline  import&nbsp;uuidfrom&nbsp;rpy2.robjects.packages&nbsp;import&nbsp;importr&nbsp;from&nbsp;IPython.core.display&nbsp;import&nbsp;Imagegrdevices&nbsp;=&nbsp;importr('grDevices')def&nbsp;ggplot_notebook(gg,&nbsp;width&nbsp;=&nbsp;800,&nbsp;height&nbsp;=&nbsp;600):&nbsp;&nbsp;&nbsp;&nbsp;fn&nbsp;=&nbsp;'{uuid}.png'.format(uuid&nbsp;=&nbsp;uuid.uuid4())&nbsp;&nbsp;&nbsp;&nbsp;grdevices.png(fn,&nbsp;width&nbsp;=&nbsp;width,&nbsp;height&nbsp;=&nbsp;height)&nbsp;&nbsp;&nbsp;&nbsp;gg.plot()&nbsp;&nbsp;&nbsp;&nbsp;grdevices.dev_off()&nbsp;&nbsp;&nbsp;&nbsp;return&nbsp;Image(filename=fn)  运行上述代码后，我们把ggplot的调用pp.plot()改为调用ggplot_notebook(pp, height=300)就能成功嵌入显示ggplot的结果。   RMagic 另一种方式是使用rmagic，rmagicy实际上依赖于rpy2。它的使用方式更像是直接在使用R %load_ext&nbsp;rmagiclibrary(ggplot2)dat&nbsp;&lt;-&nbsp;data.frame(x&nbsp;=&nbsp;rnorm(10),&nbsp;y&nbsp;=&nbsp;rnorm(10),&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;lab&nbsp;=&nbsp;sample(c('A',&nbsp;'B'),&nbsp;10,&nbsp;replace&nbsp;=&nbsp;TRUE))x&nbsp;&lt;-&nbsp;ggplot(dat,&nbsp;aes(x&nbsp;=&nbsp;x,&nbsp;y&nbsp;=&nbsp;y,&nbsp;color&nbsp;=&nbsp;lab))&nbsp;+&nbsp;geom_point()print(x)  运行结果如下   ggplot for python ggplot是一个python的库，基本上是对R语言ggplot的功能移植到Python上。  运行安装脚本 pip&nbsp;install&nbsp;ggplot  安装成功后，可以试一下这个例子 %matplotlib&nbsp;inlineimport&nbsp;pandas&nbsp;as&nbsp;pdfrom&nbsp;ggplot&nbsp;import&nbsp;*meat_lng&nbsp;=&nbsp;pd.melt(meat[['date',&nbsp;'beef',&nbsp;'pork',&nbsp;'broilers']],&nbsp;id_vars='date')ggplot(aes(x='date',&nbsp;y='value',&nbsp;colour='variable'),&nbsp;data=meat_lng)&nbsp;+&nbsp;\&nbsp;&nbsp;&nbsp;&nbsp;geom_point()&nbsp;+&nbsp;\&nbsp;&nbsp;&nbsp;&nbsp;stat_smooth(color='red')  结果如下：     总结  本文提供了三种不同的方式在Python（IPython Notebook）中调用ggplot。 rpy2和Rmagic都是一种对R的桥接，所以都需要安装R。不同之处在于rpy2提供Python接口而Rmagic更接近R。 ggplot Python库是ggplot的Python移植，所以无需安装R，部署起来更为简单，但功能上也许和R的ggplot还有差距。 大家可以根据自己的需要做出选择。</div>
                            <div class='commentList'>
                    <h3>评论列表</h3>
                                    </div>
                    </div>
    
        <div class='blog'>
            <h2><a href="#top" class='top'>回到顶部</a><a name="blog_299955" href="https://my.oschina.net/taogang/blog/299955">利用Splunk做应用程序的性能分析</a></h2>
            <div class='outline'>
                <div class='date'>时间：2014-08-11 11:27:57</div>
                <div class='catalog'>分类：日常记录</div>
                                                                            </div>
            <div class='content'>在我们的开发过程中，经常会需要对我们开发的程序做性能分析，有很多性能分析的工具，很多语言都提供了不同的profiling工具，这些工具很有用，提供了程序运行的原始记录数据，通过对这些数据的分析，可以得到程序运行的性能状况，找到问题所在。然而，这样的工具手机的数据比较原始，往往还需要一些更进一步的分析，才能定位问题。   Splunk是一个可以运行在不同平台上的机器数据的实时运维平台，所谓机器数据，就是指机器产生的数据，其中一个常见的场景就是日志。对于广大程序员来说，分析日志是一个非常常见，而且繁琐的工作，而且很多时候，必须通过日志来对程序进行调试，例如多线程的情况。记得以前为了几百兆或者几G的日志进行分析，不得不写了logViewer来分析。现在有了Splunk，真的极大的简化了对日志分析的工作。（注Splunk免费版支持每天500M的日志数据，超过这个额度需要收费）   通过日志进行性能测试是非常常见的，传统的也是在要分析的代码处，注入性能日志，然后在程序运行后，对写入的性能数据进行分析。使用Splunk，方法是一样的，但是有以下明显的改进      Splunk提供大量友好的分析命令和图表，无需另行开发分析日志的程序      Splunk可以实时的对应用程序作分析，可以在程序的运行过程中，一边运行，一边分析     我下面举一个我碰到的例子。   我要分析的程序是一个从AWS CloudWatch收集数据的Python程序。收集数据使用的是AWS提供的Restful API (Boto),为了更高效的收集数据，程序使用多个线程来调用Restful API 的Query接口。我希望通过性能日志了解每一个请求大概的耗时，以决定使用多少个线程数和对应的采集间隔。   首先，需要写日志：  logger.log(logging.DEBUG, &quot;PerfLog=QueryStart&quot; )## Query Code Goes Heredo_query_aws()## Query Completelogger.log(logging.DEBUG, &quot;PerfLog=QueryEnd, Query Result)   注意使用Name=Value的形式可以帮助Splunk在搜索时，提取要分析的字段。   然后运行程序，程序运行以后会生成日志文件，把该日志文件导入到Splunk，开始分析。      点击Add Data按钮，然后跟随Splunk的指导，选择A file or directory of files.&nbsp;导入你的日志文件，导入过程中，Splunk会要求给你的日志文件命名一个sourcetype，我用的是“cloud_watch_debug”   导入好以后就可以开始搜索了。   在搜索框中输入  sourcetype=&quot;cloud_watch_debug”   Splunk会实时的返回所有的日志文件，并按时间解析为一个个的事件。      Splunk的SPL（Splunk Search Language）是一个类似SQL和UNIX Command的综合体，可以对数据进行搜索，分析，统计，生成图表，支持管道，使用起来非常方便，建议大家通过官方文档来了解。   我么今天要做的是性能分析，那么我就是要统计一下，发了多少个query，每一个query用了多少时间。   每一条日志的内容大致如下  2014-08-11 10:52:40,587 DEBUG pid=3742 tid=QueryWorkerThread-1 file=aws_cloudwatch.py:_main_work_loop:469 | PerfLog = QueryStart   Splunk能够提取出大量的信息和字段（field），包括事件，pid，tid，file等等，还有我们在日志中加入的字段PerfLog。   想要知道每一个查询所花费的时间，可以通过Splunk提供的transaction命令。  sourcetype=cloud_watch_debug | transaction tid startswith=&quot;QueryStart&quot; endswith=&quot;QueryEnd&quot;      tid表示每一个transaction需要有相同的tid，也就是说同一个线程      startwith和endwith表示transaction的其实和结束标志     该命令返回所有的query的transaction       然后我们就可以统计每一个transaction所用的时间  sourcetype=cloud_watch_debug | transaction tid startswith=&quot;QueryStart&quot; endswith=&quot;QueryEnd&quot; | stats sum(duration),count, avg(duration),max(duration),min(duration)      stats命令用于对数据进行统计      duration是Splunk对transaction生成的事件跨度      sum,count,avg,max,min是统计命令     运行结果如下：      程序一共发送了111075个cloudwatch的请求，最慢的需要2.5秒，最快的0.06秒，平均大概0.11秒。   我还想知道query的耗时随时间的变化，我可以生成一个timechart  sourcetype=cloud_watch_debug | transaction tid startswith=&quot;QueryStart&quot; endswith=&quot;QueryEnd&quot; | timechart avg(duration)   结果如下（最近1小时）：      通过该分析在过去的一个小时里10：30和11：00之后的十分钟时间段，耗时略有上升，大概峰值0.2秒。      总结：   Splunk的日志分析功能非常强大，而且500M的免费版基本能够满足大部分程序员对程序日志的分析要求。有效地使用Splunk来进行日志分析，可以做到事半功倍，小伙伴们快来试用吧！</div>
                            <div class='commentList'>
                    <h3>评论列表</h3>
                                    </div>
                    </div>
    
        <div class='blog'>
            <h2><a href="#top" class='top'>回到顶部</a><a name="blog_350255" href="https://my.oschina.net/taogang/blog/350255">用HTML5构建一个流程图绘制工具</a></h2>
            <div class='outline'>
                <div class='date'>时间：2014-11-29 11:16:10</div>
                <div class='catalog'>分类：数据可视化</div>
                                                                            </div>
            <div class='content'>在我们的开发工程中经常会使用到各种图，所谓的图就是由节点和节点之间的连接所形成的系统，数学上专门有一个分支叫图论（Graph Theroy）。利用图我们可以做很多工具，比如思维导图，流程图，状态机，组织架构图，等等。今天我要做的是用开源的HTML5工具来快速构造一个做图的工具。 工具选择 工预善其事，必先利其器。第一件事是选择一件合适的工具，开源时代，程序员还是很幸福的，选择很多。   flowchart.js &nbsp;http://adrai.github.io/flowchart.js/&nbsp;, 基于SVG创建Flow Chart  go.js&nbsp;http://www.gojs.net/latest/index.html&nbsp; go.js 提供一整套的JS工具&nbsp;，支持各种交互式图表的创建。有免费版和收费版  joint.js&nbsp;http://www.jointjs.com/&nbsp;joint.js 是另一个创建图标库的工具，也提供免费版和商业版  jsPlumb&nbsp;http://www.jsplumb.org/&nbsp; jsPlumb是一套开源的流程图创建工具&nbsp;，小巧精悍，使用简单  d3 http://d3js.org&nbsp;在html5领域，d3可谓是最好的可视化基础库，提供方面的DOM操作，非常强大。  最终，我选择了jsPlumb，因为它完全开源，使用很简单，用D3的话可能会多花很多功夫。joint.js也不错。大家可以根据自己的需要选择。 构建静态应用 下面我们一步一步的来使用jsPlumb来创建我们的流程图工具。 第一步是等待DOM和jsPlumb初始化完毕，类似document.ready()和jquery.ready(), 要使用jsPlumb, 需要把代码放在这个函数里： jsPlumb.ready(function()&nbsp;{&nbsp;&nbsp;&nbsp;&nbsp;//&nbsp;...&nbsp;your&nbsp;code&nbsp;goes&nbsp;here&nbsp;...}  创建一个jsPlumb的实例，并初始化jsPlumb的配置参数： //Initialize&nbsp;JsPlumbvar&nbsp;color&nbsp;=&nbsp;"#E8C870";var&nbsp;instance&nbsp;=&nbsp;jsPlumb.getInstance({&nbsp;&nbsp;&nbsp;&nbsp;//&nbsp;notice&nbsp;the&nbsp;'curviness'&nbsp;argument&nbsp;to&nbsp;this&nbsp;Bezier&nbsp;curve.&nbsp;&nbsp;the&nbsp;curves&nbsp;on&nbsp;this&nbsp;page&nbsp;are&nbsp;far&nbsp;smoother&nbsp;&nbsp;&nbsp;&nbsp;//&nbsp;than&nbsp;the&nbsp;curves&nbsp;on&nbsp;the&nbsp;first&nbsp;demo,&nbsp;which&nbsp;use&nbsp;the&nbsp;default&nbsp;curviness&nbsp;value.&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Connector&nbsp;:&nbsp;[&nbsp;"Bezier",&nbsp;{&nbsp;curviness:50&nbsp;}&nbsp;],&nbsp;&nbsp;&nbsp;&nbsp;DragOptions&nbsp;:&nbsp;{&nbsp;cursor:&nbsp;"pointer",&nbsp;zIndex:2000&nbsp;},&nbsp;&nbsp;&nbsp;&nbsp;PaintStyle&nbsp;:&nbsp;{&nbsp;strokeStyle:color,&nbsp;lineWidth:2&nbsp;},&nbsp;&nbsp;&nbsp;&nbsp;EndpointStyle&nbsp;:&nbsp;{&nbsp;radius:5,&nbsp;fillStyle:color&nbsp;},&nbsp;&nbsp;&nbsp;&nbsp;HoverPaintStyle&nbsp;:&nbsp;{strokeStyle:"#7073EB"&nbsp;},&nbsp;&nbsp;&nbsp;&nbsp;EndpointHoverStyle&nbsp;:&nbsp;{fillStyle:"#7073EB"&nbsp;},&nbsp;&nbsp;&nbsp;&nbsp;Container:"container-id"&nbsp;});  这里给给出了一些配置包括，连接线（这里配置了一个贝塞尔曲线），线的风格，连接点得风格。Container需要配置一个对应的DIV容器的id。（这里也可以使用setContainer的方法） 下面我们要创建一个节点（node），每一个节点可以用一个DIV来实现。我这里提供了一个函数来创建节点。 function&nbsp;addNode(parentId,&nbsp;nodeId,&nbsp;nodeLable,&nbsp;position)&nbsp;{&nbsp;&nbsp;var&nbsp;panel&nbsp;=&nbsp;d3.select("#"&nbsp;+&nbsp;parentId);&nbsp;&nbsp;panel.append('div').style('width','120px').style('height','50px')&nbsp;&nbsp;&nbsp;&nbsp;.style('position','absolute')&nbsp;&nbsp;&nbsp;&nbsp;.style('top',position.y).style('left',position.x)&nbsp;&nbsp;&nbsp;&nbsp;.style('border','2px&nbsp;#9DFFCA&nbsp;solid').attr('align','center')&nbsp;&nbsp;&nbsp;&nbsp;.attr('id',nodeId).classed('node',true)&nbsp;&nbsp;&nbsp;&nbsp;.text(nodeLable);&nbsp;&nbsp;return&nbsp;jsPlumb.getSelector('#'&nbsp;+&nbsp;nodeId)[0];}  这里做的事情就是创建了一个DIV元素，并放在对应的容器的制定位置上，注意为了支持拖拽的功能，必须使用position:absolute 。 我使用D3来操作DOM，大家可能会更习惯JQuery，这纯属个人喜好的问题。 最后返回创建节点的实例引用，这是的selector使用了jsPlumb.getSelector()方法，它和JQuery的selector是一样的，这样用的好处是你可以使用不同的DOM操作库，例如Vanilla 下面我使用一个函数来创建端点/锚点（anchor），锚点就是节点上的连接点，用于连接不同的节点。 function&nbsp;addPorts(instance,&nbsp;node,&nbsp;ports,&nbsp;type)&nbsp;{&nbsp;&nbsp;//Assume&nbsp;horizental&nbsp;layout&nbsp;&nbsp;var&nbsp;number_of_ports&nbsp;=&nbsp;ports.length;&nbsp;&nbsp;var&nbsp;i&nbsp;=&nbsp;0;&nbsp;&nbsp;var&nbsp;height&nbsp;=&nbsp;$(node).height();&nbsp;&nbsp;//Note,&nbsp;jquery&nbsp;does&nbsp;not&nbsp;include&nbsp;border&nbsp;for&nbsp;height&nbsp;&nbsp;var&nbsp;y_offset&nbsp;=&nbsp;1&nbsp;/&nbsp;(&nbsp;number_of_ports&nbsp;+&nbsp;1);&nbsp;&nbsp;var&nbsp;y&nbsp;=&nbsp;0;&nbsp;&nbsp;for&nbsp;(&nbsp;;&nbsp;i&nbsp;&lt;&nbsp;number_of_ports;&nbsp;i++&nbsp;)&nbsp;{&nbsp;&nbsp;&nbsp;&nbsp;var&nbsp;anchor&nbsp;=&nbsp;[0,0,0,0];&nbsp;&nbsp;&nbsp;&nbsp;var&nbsp;paintStyle&nbsp;=&nbsp;{&nbsp;radius:5,&nbsp;fillStyle:'#FF8891'&nbsp;};&nbsp;&nbsp;&nbsp;&nbsp;var&nbsp;isSource&nbsp;=&nbsp;false,&nbsp;isTarget&nbsp;=&nbsp;false;&nbsp;&nbsp;&nbsp;&nbsp;if&nbsp;(&nbsp;type&nbsp;===&nbsp;'output'&nbsp;)&nbsp;{&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;anchor[0]&nbsp;=&nbsp;1;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;paintStyle.fillStyle&nbsp;=&nbsp;'#D4FFD6';&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;isSource&nbsp;=&nbsp;true;&nbsp;&nbsp;&nbsp;&nbsp;}&nbsp;else&nbsp;{&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;isTarget&nbsp;=true;&nbsp;&nbsp;&nbsp;&nbsp;}&nbsp;&nbsp;&nbsp;&nbsp;anchor[1]&nbsp;=&nbsp;y&nbsp;+&nbsp;y_offset;&nbsp;&nbsp;&nbsp;&nbsp;y&nbsp;=&nbsp;anchor[1];&nbsp;&nbsp;&nbsp;&nbsp;instance.addEndpoint(node,&nbsp;{&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;uuid:node.getAttribute("id")&nbsp;+&nbsp;"-"&nbsp;+&nbsp;ports[i],&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;paintStyle:&nbsp;paintStyle,&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;anchor:anchor,&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;maxConnections:-1,&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;isSource:isSource,&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;isTarget:isTarget&nbsp;&nbsp;&nbsp;&nbsp;});&nbsp;&nbsp;}}  instance是jsPlumb的实例 node是我们用addNode方法创建的Node实例 ports，是一个string的数组，指定端点的个数和名字 type，可能是output或者input，指定端点的种类，一个节点的输出端口可以连接另一个节点的输入端口。 这里anchor是一个四维数组，0维和1维分别是锚点在节点x轴和y轴的偏移百分比。我这里希望把端口画在节点的左右两侧，并按照端口的数量均匀分布。 最后使用instance.addEndpoint来创建端点。注意这里只要指定isSource和isTarget就可以用drag&amp;drop的方式来连接端点，非常方便。 下面一步我们提供一个函数来连接端点： function&nbsp;connectPorts(instance,&nbsp;node1,&nbsp;port1,&nbsp;node2&nbsp;,&nbsp;port2)&nbsp;{&nbsp;&nbsp;//&nbsp;declare&nbsp;some&nbsp;common&nbsp;values:&nbsp;&nbsp;var&nbsp;color&nbsp;=&nbsp;"gray";&nbsp;&nbsp;var&nbsp;arrowCommon&nbsp;=&nbsp;{&nbsp;foldback:0.8,&nbsp;fillStyle:color,&nbsp;width:5&nbsp;},&nbsp;&nbsp;//&nbsp;use&nbsp;three-arg&nbsp;spec&nbsp;to&nbsp;create&nbsp;two&nbsp;different&nbsp;arrows&nbsp;with&nbsp;the&nbsp;common&nbsp;values:&nbsp;&nbsp;overlays&nbsp;=&nbsp;[&nbsp;&nbsp;&nbsp;&nbsp;[&nbsp;"Arrow",&nbsp;{&nbsp;location:0.8&nbsp;},&nbsp;arrowCommon&nbsp;],&nbsp;&nbsp;&nbsp;&nbsp;[&nbsp;"Arrow",&nbsp;{&nbsp;location:0.2,&nbsp;direction:-1&nbsp;},&nbsp;arrowCommon&nbsp;]&nbsp;&nbsp;];&nbsp;&nbsp;var&nbsp;uuid_source&nbsp;=&nbsp;node1.getAttribute("id")&nbsp;+&nbsp;"-"&nbsp;+&nbsp;port1;&nbsp;&nbsp;var&nbsp;uuid_target&nbsp;=&nbsp;node2.getAttribute("id")&nbsp;+&nbsp;"-"&nbsp;+&nbsp;port2;&nbsp;&nbsp;instance.connect({uuids:[uuid_source,&nbsp;uuid_target]});}  node1和node2是源节点和目标节点的引用，port1和port2是源端口和目标端口的名字。 使用instance.connect方法来创建连接。 overlays用来添加连接线的箭头效果或者其他风格，我这里没有使用，因为觉得都不是很好看。大家如果要用，只要把overlays加入到instance.connect的方法参数就可以了。 调用以上方法来创建节点，端点和连接线。 var&nbsp;node1&nbsp;=&nbsp;addNode('container-id','node1',&nbsp;'node1',&nbsp;{x:'80px',y:'20px'});var&nbsp;node2&nbsp;=&nbsp;addNode('container-id','node2',&nbsp;'node2',&nbsp;{x:'280px',y:'20px'});addPorts(instance,&nbsp;node1,&nbsp;['out1','out2'],'output');addPorts(instance,&nbsp;node2,&nbsp;['in','in1','in2'],'input');connectPorts(instance,&nbsp;node1,&nbsp;'out2',&nbsp;node2,&nbsp;'in');  这里我们创建了两个节点，第一个节点有两个输出端口，第二个节点有三个输入端口，然后把第一个节点的out2端口连接到第二个端点的in端口。效果如下：  最后我们给节点增加drag&amp;drop的功能，这样我们就可以拖动这些节点来改变图的布局了。 instance.draggable($('.node'));  这里似乎依赖于JQuery-UI,我还不是很清楚。 交互式创建节点 我们已经初步具有了创建图的功能，可是节点的创建必须通过程序，我们希望用交互的方式来创建节点。 通常我们希望有一个tree view的控件，让后通过拖拽来创建对应类型的节点。这里我使用了这个开源的tree view，基于bootstrap&nbsp;https://github.com/jonmiles/bootstrap-treeview&nbsp; 我们先创建一个tree view： function&nbsp;getTreeData()&nbsp;{&nbsp;&nbsp;var&nbsp;tree&nbsp;=&nbsp;[&nbsp;&nbsp;&nbsp;&nbsp;{&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;text:&nbsp;"Nodes",&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;nodes:&nbsp;[&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;{&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;text:&nbsp;"Node1",&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;},&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;{&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;text:&nbsp;"Node2"&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;}&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;]&nbsp;&nbsp;&nbsp;&nbsp;}&nbsp;&nbsp;];&nbsp;&nbsp;&nbsp;return&nbsp;tree;}//Initialize&nbsp;Control&nbsp;Tree&nbsp;View$('#control-panel').treeview({data:&nbsp;getTreeData()});  树上有两个节点:  然后我实现从树上拖拽对应的节点，到流程图上的逻辑。 //Handle&nbsp;drag&nbsp;and&nbsp;drop$('.list-group-item').attr('draggable','true').on('dragstart',&nbsp;function(ev){&nbsp;&nbsp;//ev.dataTransfer.setData("text",&nbsp;ev.target.id);&nbsp;&nbsp;ev.originalEvent.dataTransfer.setData('text',ev.target.textContent);&nbsp;&nbsp;console.log('drag&nbsp;start');});$('#container-id').on('drop',&nbsp;function(ev){&nbsp;&nbsp;//avoid&nbsp;event&nbsp;conlict&nbsp;for&nbsp;jsPlumb&nbsp;&nbsp;if&nbsp;(ev.target.className.indexOf('_jsPlumb')&nbsp;&gt;=&nbsp;0&nbsp;)&nbsp;{&nbsp;&nbsp;&nbsp;&nbsp;return;&nbsp;&nbsp;}&nbsp;&nbsp;ev.preventDefault();&nbsp;&nbsp;var&nbsp;mx&nbsp;=&nbsp;''&nbsp;+&nbsp;ev.originalEvent.offsetX&nbsp;+&nbsp;'px';&nbsp;&nbsp;var&nbsp;my&nbsp;=&nbsp;''&nbsp;+&nbsp;ev.originalEvent.offsetY&nbsp;+&nbsp;'px';&nbsp;&nbsp;console.log('on&nbsp;drop&nbsp;:&nbsp;'&nbsp;+&nbsp;ev.originalEvent.dataTransfer.getData('text'));&nbsp;&nbsp;var&nbsp;uid&nbsp;=&nbsp;new&nbsp;Date().getTime();&nbsp;&nbsp;var&nbsp;node&nbsp;=&nbsp;addNode('flow-panel','node'&nbsp;+&nbsp;uid,&nbsp;'node',&nbsp;{x:mx,y:my});&nbsp;&nbsp;addPorts(instance,&nbsp;node,&nbsp;['out'],'output');&nbsp;&nbsp;addPorts(instance,&nbsp;node,&nbsp;['in1','in2'],'input');&nbsp;&nbsp;instance.draggable($(node));}).on('dragover',&nbsp;function(ev){&nbsp;&nbsp;ev.preventDefault();&nbsp;&nbsp;console.log('on&nbsp;drag&nbsp;over');});  这里要注意的是要避免和jsPlumb拖拽端点的逻辑冲突，当检测到target是jsPlumb对象是需要直接从drop方法中退出以执行对应的jsPlumb的drop逻辑。 好了，一个绘制流程图的软件工具初步完工。  我把代码放在oschina的代码托管服务上了， 大家有兴趣可以下来试试&nbsp;http://git.oschina.net/gangtao/FlowChart-Builder&nbsp;</div>
                    </div>
    
        <div class='blog'>
            <h2><a href="#top" class='top'>回到顶部</a><a name="blog_383241" href="https://my.oschina.net/taogang/blog/383241">使用sphinx快速生成Python API 文档</a></h2>
            <div class='outline'>
                <div class='date'>时间：2015-03-06 09:49:46</div>
                <div class='catalog'>分类：日常记录</div>
                                                                            </div>
            <div class='content'>不管是开源还是闭源，文档都是很重要的。当然理论上说，最好的文档就是代码本身，但是要让所有人都能读懂你的代码这太难了。所以我们要写文档。大部分情况，我们不希望维护一份代码再加上一份文档，这样做很容易造成文档和代码的不一致，程序员最讨厌更新文档了。所以最佳实践就是在程序员代码中加注释，然后通过构建脚本自通生成文档。 对应于Pyhon，有很多可供选择的工具：   sphinx&nbsp;中文版介绍&nbsp;Sphinx使用&nbsp;reStructuredText作为标记语言（类似Markdown），可扩展，功能强大。要注意的是何有一个开源的搜索也叫Sphinx，斯芬克斯果然太受欢迎，开源的世界起个名字不容易呀。  pdoc&nbsp;是一个简单易用的命令行工具，可以生成Python的API文档  Doxygen&nbsp;是老牌的文档生成工具，可以针对各种语言生成文档，我们以前在C++的项目中曾经使用过  其他还有诸如&nbsp;pydoc&nbsp;， pydoctor 等等  下面我就介绍一下如果使用Sphinx为你的python项目快速的构建API 文档。 首先要安装Sphinx，不同的操作系统有不同的安装方式，Sphinx的源代码在这里&nbsp;， 你也可以自己构建。我推荐使用pip install。（注，如果你安装了Anaconda，Sphinx已经包含在内了） 然后，假定你的python的源代码是在 src 目录下，我们在同一级并行建立一个文档目录 doc （你当然可以根据自己的项目需要，确定目录命名和结构）。 在doc目录下运行&nbsp; sphinx-quickstart sphinx会提示你的项目的一些设置，生成项目的配置文件，这里给出一些推荐的配置： &gt;&nbsp;Root&nbsp;path&nbsp;for&nbsp;the&nbsp;documentation&nbsp;[.]:&lt;ENTER&gt;&gt;&nbsp;Separate&nbsp;source&nbsp;and&nbsp;build&nbsp;directories&nbsp;(y/N)&nbsp;[n]:y&gt;&nbsp;Name&nbsp;prefix&nbsp;for&nbsp;templates&nbsp;and&nbsp;static&nbsp;dir&nbsp;[_]:&lt;ENTER&gt;&gt;&nbsp;Project&nbsp;name:an_example_pypi_project&gt;&nbsp;Author&nbsp;name(s):Andrew&nbsp;Carter&gt;&nbsp;Project&nbsp;version:0.0.1&gt;&nbsp;Project&nbsp;release&nbsp;[0.0.1]:&lt;ENTER&gt;&gt;&nbsp;Source&nbsp;file&nbsp;suffix&nbsp;[.rst]:&lt;ENTER&gt;&gt;&nbsp;Name&nbsp;of&nbsp;your&nbsp;master&nbsp;document&nbsp;(without&nbsp;suffix)&nbsp;[index]:&lt;ENTER&gt;&gt;&nbsp;autodoc:&nbsp;automatically&nbsp;insert&nbsp;docstrings&nbsp;from&nbsp;modules&nbsp;(y/N)&nbsp;[n]:y&gt;&nbsp;doctest:&nbsp;automatically&nbsp;test&nbsp;code&nbsp;snippets&nbsp;in&nbsp;doctest&nbsp;blocks&nbsp;(y/N)&nbsp;[n]:n&gt;&nbsp;intersphinx:&nbsp;link&nbsp;between&nbsp;Sphinx&nbsp;documentation&nbsp;of&nbsp;different&nbsp;projects&nbsp;(y/N)&nbsp;[n]:y&gt;&nbsp;todo:&nbsp;write&nbsp;“todo”&nbsp;entries&nbsp;that&nbsp;can&nbsp;be&nbsp;shown&nbsp;or&nbsp;hidden&nbsp;on&nbsp;build&nbsp;(y/N)&nbsp;[n]:n&gt;&nbsp;coverage:&nbsp;checks&nbsp;for&nbsp;documentation&nbsp;coverage&nbsp;(y/N)&nbsp;[n]:n&gt;&nbsp;pngmath:&nbsp;include&nbsp;math,&nbsp;rendered&nbsp;as&nbsp;PNG&nbsp;images&nbsp;(y/N)&nbsp;[n]:n&gt;&nbsp;jsmath:&nbsp;include&nbsp;math,&nbsp;rendered&nbsp;in&nbsp;the&nbsp;browser&nbsp;by&nbsp;JSMath&nbsp;(y/N)&nbsp;[n]:n&gt;&nbsp;ifconfig:&nbsp;conditional&nbsp;inclusion&nbsp;of&nbsp;content&nbsp;based&nbsp;on&nbsp;config&nbsp;values&nbsp;(y/N)&nbsp;[n]:y&gt;&nbsp;Create&nbsp;Makefile?&nbsp;(Y/n)&nbsp;[y]:n&gt;&nbsp;Create&nbsp;Windows&nbsp;command&nbsp;file?&nbsp;(Y/n)&nbsp;[y]:n 运行完毕，sphinx会生成项目的配置文档conf.py还有源文件（后缀为rst） 下一步要为捏python源文件生成sphinx的源文件，用来生成API文档，需要运行 sphinx-apidoc&nbsp;[options]&nbsp;-o&nbsp;outputdir&nbsp;packagedir&nbsp;[pathnames] 其中outputdir是doc目录，packagedir是src目录，也就是你的python代码包所在的目录 运行好后，会对每一个Python包生成一个rst文件，你可以编辑该文件来修改生成文档的细节，一般情况下不用改。 好了，准备工作做好了以后，就可以生成API文档了。在运行文档生成脚本之前，要确保你的Python源代码所在的包在系统路径中是可以找到的，需要修改conf.py。因为在生成文档是需要运行你的python代码，要保证code运行不出错。 sys.path.insert(0,&nbsp;os.path.abspath('../src')) 在doc目录下运行脚本 sphinx-build&nbsp;-b&nbsp;html&nbsp;.&nbsp;./ouput 在output目录会生成HTML格式的API文档。（也可以选其他文档格式） Sphinx还有一个automsummay的扩展，可能能简化以上的过程，等我试一试在更新结果。</div>
                            <div class='commentList'>
                    <h3>评论列表</h3>
                                    </div>
                    </div>
    
        <div class='blog'>
            <h2><a href="#top" class='top'>回到顶部</a><a name="blog_386077" href="https://my.oschina.net/taogang/blog/386077">Python 并行分布式框架之 Celery</a></h2>
            <div class='outline'>
                <div class='date'>时间：2015-03-12 11:07:02</div>
                <div class='catalog'>分类：编程语言</div>
                                                                            </div>
            <div class='content'>Celery&nbsp;（芹菜）是基于Python开发的分布式任务队列。它支持使用任务队列的方式在分布的机器／进程／线程上执行任务调度。架构设计Celery的架构由三部分组成，消息中间件（message broker），任务执行单元（worker）和任务执行结果存储（task result store）组成。消息中间件Celery本身不提供消息服务，但是可以方便的和第三方提供的消息中间件集成。包括，RabbitMQ,&nbsp;Redis,&nbsp;MongoDB&nbsp;(experimental), Amazon SQS (experimental),CouchDB&nbsp;(experimental),&nbsp;SQLAlchemy&nbsp;(experimental),Django ORM (experimental),&nbsp;IronMQ任务执行单元Worker是Celery提供的任务执行的单元，worker并发的运行在分布式的系统节点中。任务结果存储Task result store用来存储Worker执行的任务的结果，Celery支持以不同方式存储任务的结果，包括AMQP, Redis，memcached, MongoDB，SQLAlchemy, Django ORM，Apache Cassandra, IronCache另外， Celery还支持不同的并发和序列化的手段并发Prefork,&nbsp;Eventlet,&nbsp;gevent, threads/single threaded序列化pickle,&nbsp;json,&nbsp;yaml,&nbsp;msgpack.&nbsp;zlib,&nbsp;bzip2&nbsp;compression， Cryptographic message signing 等等安装和运行Celery的安装过程略为复杂，下面的安装过程是基于我的AWS EC2的Linux版本的安装过程，不同的系统安装过程可能会有差异。大家可以参考官方文档。首先我选择RabbitMQ作为消息中间件，所以要先安装RabbitMQ。作为安装准备，先更新YUM。sudo&nbsp;yum&nbsp;-y&nbsp;updateRabbitMQ是基于erlang的，所以先安装erlang#&nbsp;Add&nbsp;and&nbsp;enable&nbsp;relevant&nbsp;application&nbsp;repositories:#&nbsp;Note:&nbsp;We&nbsp;are&nbsp;also&nbsp;enabling&nbsp;third&nbsp;party&nbsp;remi&nbsp;package&nbsp;repositories.wget&nbsp;http://dl.fedoraproject.org/pub/epel/6/x86_64/epel-release-6-8.noarch.rpmwget&nbsp;http://rpms.famillecollet.com/enterprise/remi-release-6.rpmsudo&nbsp;rpm&nbsp;-Uvh&nbsp;remi-release-6*.rpm&nbsp;epel-release-6*.rpm#&nbsp;Finally,&nbsp;download&nbsp;and&nbsp;install&nbsp;Erlang:yum&nbsp;install&nbsp;-y&nbsp;erlang然后安装RabbitMQ#&nbsp;Download&nbsp;the&nbsp;latest&nbsp;RabbitMQ&nbsp;package&nbsp;using&nbsp;wget:wget&nbsp;&nbsp;#&nbsp;Add&nbsp;the&nbsp;necessary&nbsp;keys&nbsp;for&nbsp;verification:rpm&nbsp;--import&nbsp;&nbsp;#&nbsp;Install&nbsp;the&nbsp;.RPM&nbsp;package&nbsp;using&nbsp;YUM:yum&nbsp;install&nbsp;rabbitmq-server-3.2.2-1.noarch.rpm启动RabbitMQ服务rabbitmq-server&nbsp;startRabbitMQ服务已经准备好了，然后安装Celery， 假定你使用pip来管理你的python安装包pip&nbsp;install&nbsp;Celery为了测试Celery是否工作，我们运行一个最简单的任务，编写tasks.pyfrom&nbsp;celery&nbsp;import&nbsp;Celeryapp&nbsp;=&nbsp;Celery('tasks',&nbsp;backend='amqp',&nbsp;broker='amqp://guest@localhost//')app.conf.CELERY_RESULT_BACKEND&nbsp;=&nbsp;'db+sqlite:///results.sqlite'@app.taskdef&nbsp;add(x,&nbsp;y):&nbsp;&nbsp;&nbsp;&nbsp;return&nbsp;x&nbsp;+&nbsp;y在当前目录运行一个worker，用来执行这个加法的taskcelery&nbsp;-A&nbsp;tasks&nbsp;worker&nbsp;--loglevel=info其中－A参数表示的是Celery App的名字。注意这里我使用的是SQLAlchemy作为结果存储。对应的python包要事先安装好。worker日志中我们会看到这样的信息-&nbsp;**&nbsp;----------&nbsp;[config]-&nbsp;**&nbsp;----------&nbsp;.&gt;&nbsp;app:&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;tasks:0x1e68d50-&nbsp;**&nbsp;----------&nbsp;.&gt;&nbsp;transport:&nbsp;&nbsp;&nbsp;amqp://guest:**@localhost:5672//-&nbsp;**&nbsp;----------&nbsp;.&gt;&nbsp;results:&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;db+sqlite:///results.sqlite-&nbsp;***&nbsp;---&nbsp;*&nbsp;---&nbsp;.&gt;&nbsp;concurrency:&nbsp;8&nbsp;(prefork)其中，我们可以看到worker缺省使用prefork来执行并发，并设置并发数为8下面的任务执行的客户端代码：from&nbsp;tasks&nbsp;import&nbsp;addimport&nbsp;timeresult&nbsp;=&nbsp;add.delay(4,4)while&nbsp;not&nbsp;result.ready():&nbsp;&nbsp;print&nbsp;"not&nbsp;ready&nbsp;yet"&nbsp;&nbsp;time.sleep(5)print&nbsp;result.get()用python执行这段客户端代码，在客户端，结果如下not&nbsp;ready&nbsp;&nbsp;&nbsp;8Work日志显示[2015-03-12&nbsp;02:54:07,973:&nbsp;INFO/MainProcess]&nbsp;Received&nbsp;task:&nbsp;tasks.add[34c4210f-1bc5-420f-a421-1500361b914f][2015-03-12&nbsp;02:54:08,006:&nbsp;INFO/MainProcess]&nbsp;Task&nbsp;tasks.add[34c4210f-1bc5-420f-a421-1500361b914f]&nbsp;succeeded&nbsp;in&nbsp;0.0309705100954s:&nbsp;8这里我们可以发现，每一个task有一个唯一的ID，task异步执行在worker上。这里要注意的是，如果你运行官方文档中的例子，你是无法在客户端得到结果的，这也是我为什么要使用SQLAlchemy来存储任务执行结果的原因。官方的例子使用AMPQ，有可能Worker在打印日志的时候取出了task的运行结果显示在worker日志中，然而AMPQ作为一个消息队列，当消息被取走后，队列中就没有了，于是客户端总是无法得到任务的执行结果。不知道为什么官方文档对这样的错误视而不见。如果大家想要对Celery做更进一步的了解，请参考官方文档</div>
                            <div class='commentList'>
                    <h3>评论列表</h3>
                                    </div>
                    </div>
    
        <div class='blog'>
            <h2><a href="#top" class='top'>回到顶部</a><a name="blog_386512" href="https://my.oschina.net/taogang/blog/386512">Python 并行分布式框架之 PP</a></h2>
            <div class='outline'>
                <div class='date'>时间：2015-03-13 10:34:19</div>
                <div class='catalog'>分类：架构设计</div>
                                                                            </div>
            <div class='content'>PP （Parallel Python）是基于Python的一个轻量级的，提供在SMP（多处理器或者多核系统）或者集群环境中并行执行Python代码的机制。最简单和最常见的并行方式是使用多线程，然而如果应用程序使用Python提供的线程库， 它实际上并不能并行的运行Python的字节码（Byte－Code）。这是因为Pyton解释器使用GIL（全局解释器锁），这样的机制是的在同一时间，即使是多核的机器，也只能运行一个字节码指令。PP试图克服这样的限制，提供一种更简单的方式来编写并行应用。PP采用了多进程和进程间通信来处理并发，并隐藏所有的实现细节，使得其容易使用。功能介绍并行运行Python代码（废话）易于理解的基于任务（Job）的并行技术自动检测优化配置动态处理器分配负载均衡容错自动发现和动态分配计算资源基于SHA的网络连接认证跨平台（Windows，Linux， Unix， Mac OS）和架构（X86，X86－64）支持开源 （BSD）安装wget&nbsp;&nbsp;tar&nbsp;-xvf&nbsp;pp-1.6.4.tar.gzcd&nbsp;pp-1.6.4sudo&nbsp;python&nbsp;setup.py&nbsp;install架构和设计ServerPP server 包含并管理多个worker并行的执行客户端发送的任务Client客户端负责发送任务（Python Function）到服务器Cluster多个PP Server可以构成一个PP Cluster，在CLuster模式客户端提交任务到Cluster，cluster 找到合适的Server来运行任务。使用方式SMP在SMP的模式下使用PP非常简单import&nbsp;pp＃&nbsp;create&nbsp;a&nbsp;job&nbsp;sererjob_server&nbsp;=&nbsp;pp.Server()#&nbsp;submit&nbsp;some&nbsp;jobs&nbsp;with&nbsp;python&nbsp;functionsf1&nbsp;=&nbsp;job_server.submit(func1,&nbsp;args1,&nbsp;depfuncs1,&nbsp;modules1)f2&nbsp;=&nbsp;job_server.submit(func1,&nbsp;args2,&nbsp;depfuncs1,&nbsp;modules1)f3&nbsp;=&nbsp;job_server.submit(func2,&nbsp;args3,&nbsp;depfuncs2,&nbsp;modules2)#&nbsp;Get&nbsp;result&nbsp;from&nbsp;each&nbsp;jobsr1&nbsp;=&nbsp;f1()r2&nbsp;=&nbsp;f2()r3&nbsp;=&nbsp;f3()Cluster在Cluster模式下，需要在不同的节点运行ppservernode-1&gt;&nbsp;./ppserver.pynode-2&gt;&nbsp;./ppserver.pynode-3&gt;&nbsp;./ppserver.py客户端代码和SMP模式类似import&nbsp;pp＃&nbsp;create&nbsp;clusterppservers=("node-1",&nbsp;"node-2",&nbsp;"node-3")＃&nbsp;create&nbsp;a&nbsp;job&nbsp;sererpp.Server(ppservers=ppservers)&nbsp;#&nbsp;submit&nbsp;some&nbsp;jobs&nbsp;with&nbsp;python&nbsp;functionsf1&nbsp;=&nbsp;job_server.submit(func1,&nbsp;args1,&nbsp;depfuncs1,&nbsp;modules1)f2&nbsp;=&nbsp;job_server.submit(func1,&nbsp;args2,&nbsp;depfuncs1,&nbsp;modules1)f3&nbsp;=&nbsp;job_server.submit(func2,&nbsp;args3,&nbsp;depfuncs2,&nbsp;modules2)#&nbsp;Get&nbsp;result&nbsp;from&nbsp;each&nbsp;jobsr1&nbsp;=&nbsp;f1()r2&nbsp;=&nbsp;f2()r3&nbsp;=&nbsp;f3()总结PP是一个轻量级的并行框架，代码不多，安装使用起来也比较简单，并行方式是多进程，但是缺乏对任务的封装，也缺少调度的功能。适合于构造简单的并行分布式系统。</div>
                            <div class='commentList'>
                    <h3>评论列表</h3>
                                    </div>
                    </div>
    
        <div class='blog'>
            <h2><a href="#top" class='top'>回到顶部</a><a name="blog_389293" href="https://my.oschina.net/taogang/blog/389293">使用Python进行并发编程</a></h2>
            <div class='outline'>
                <div class='date'>时间：2015-03-20 09:45:02</div>
                <div class='catalog'>分类：编程语言</div>
                                                                            </div>
            <div class='content'>让计算机程序并发的运行是一个经常被讨论的话题，今天我想讨论一下Python下的各种并发方式。并发方式线程（Thread）多线程几乎是每一个程序猿在使用每一种语言时都会首先想到用于解决并发的工具（JS程序员请回避），使用多线程可以有效的利用CPU资源（Python例外）。然而多线程所带来的程序的复杂度也不可避免，尤其是对竞争资源的同步问题。然而在python中由于使用了全局解释锁（GIL）的原因，代码并不能同时在多核上并发的运行，也就是说，Python的多线程不能并发，很多人会发现使用多线程来改进自己的Python代码后，程序的运行效率却下降了，这是多么蛋疼的一件事呀！如果想了解更多细节，推荐阅读这篇文章。实际上使用多线程的编程模型是很困难的，程序员很容易犯错，这并不是程序员的错误，因为并行思维是反人类的，我们大多数人的思维是串行（精神分裂不讨论），而且冯诺依曼设计的计算机架构也是以顺序执行为基础的。所以如果你总是不能把你的多线程程序搞定，恭喜你，你是个思维正常的程序猿：）Python提供两组线程的接口，一组是thread模块，提供基础的，低等级（Low Level）接口，使用Function作为线程的运行体。还有一组是threading模块，提供更容易使用的基于对象的接口（类似于Java），可以继承Thread对象来实现线程，还提供了其它一些线程相关的对象，例如Timer，Lock使用thread模块的例子import&nbsp;threaddef&nbsp;worker():&nbsp;&nbsp;&nbsp;&nbsp;"""thread&nbsp;worker&nbsp;function"""&nbsp;&nbsp;&nbsp;&nbsp;print&nbsp;'Worker'thread.start_new_thread(worker)使用threading模块的例子import&nbsp;threadingdef&nbsp;worker():&nbsp;&nbsp;&nbsp;&nbsp;"""thread&nbsp;worker&nbsp;function"""&nbsp;&nbsp;&nbsp;&nbsp;print&nbsp;'Worker't&nbsp;=&nbsp;threading.Thread(target=worker)t.start()&nbsp;或者Java Styleimport&nbsp;threadingclass&nbsp;worker(threading.Thread):&nbsp;&nbsp;&nbsp;&nbsp;def&nbsp;__init__(self):&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;pass&nbsp;&nbsp;&nbsp;&nbsp;def&nbsp;run():&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;"""thread&nbsp;worker&nbsp;function"""&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;print&nbsp;'Worker'&nbsp;&nbsp;&nbsp;&nbsp;t&nbsp;=&nbsp;worker()t.start()进程 （Process）由于前文提到的全局解释锁的问题，Python下比较好的并行方式是使用多进程，这样可以非常有效的使用CPU资源，并实现真正意义上的并发。当然，进程的开销比线程要大，也就是说如果你要创建数量惊人的并发进程的话，需要考虑一下你的机器是不是有一颗强大的心。Python的mutliprocess模块和threading具有类似的接口。from&nbsp;multiprocessing&nbsp;import&nbsp;Processdef&nbsp;worker():&nbsp;&nbsp;&nbsp;&nbsp;"""thread&nbsp;worker&nbsp;function"""&nbsp;&nbsp;&nbsp;&nbsp;print&nbsp;'Worker'p&nbsp;=&nbsp;Process(target=worker)p.start()p.join()由于线程共享相同的地址空间和内存，所以线程之间的通信是非常容易的，然而进程之间的通信就要复杂一些了。常见的进程间通信有，管道，消息队列，Socket接口（TCP/IP）等等。Python的mutliprocess模块提供了封装好的管道和队列，可以方便的在进程间传递消息。Python进程间的同步使用锁，这一点喝线程是一样的。另外，Python还提供了进程池Pool对象，可以方便的管理和控制线程。远程分布式主机 （Distributed Node）随着大数据时代的到临，摩尔定理在单机上似乎已经失去了效果，数据的计算和处理需要分布式的计算机网络来运行，程序并行的运行在多个主机节点上，已经是现在的软件架构所必需考虑的问题。远程主机间的进程间通信有几种常见的方式TCP／IPTCP／IP是所有远程通信的基础，然而API比较低级别，使用起来比较繁琐，所以一般不会考虑远程方法调用 Remote Function CallRPC是早期的远程进程间通信的手段。Python下有一个开源的实现RPyC远程对象&nbsp;Remote Object远程对象是更高级别的封装，程序可以想操作本地对象一样去操作一个远程对象在本地的代理。远程对象最广为使用的规范CORBA，CORBA最大的好处是可以在不同语言和平台中进行通信。当让不用的语言和平台还有一些各自的远程对象实现，例如Java的RMI，MS的DCOMPython的开源实现，有许多对远程对象的支持DopyFnorb&nbsp;（CORBA）ICEomniORB （CORBA）PyroYAMI消息队列 Message Queue比起RPC或者远程对象，消息是一种更为灵活的通信手段，常见的支持Python接口的消息机制有RabbitMQZeroMQKafkaAWS SQS ＋ BOTO在远程主机上执行并发和本地的多进程并没有非常大的差异，都需要解决进程间通信的问题。当然对远程进程的管理和协调比起本地要复杂。Python下有许多开源的框架来支持分布式的并发，提供有效的管理手段包括：Celery&nbsp;Celery是一个非常成熟的Python分布式框架，可以在分布式的系统中，异步的执行任务，并提供有效的管理和调度功能。参考这里SCOOPSCOOP （Scalable COncurrent Operations in Python）提供简单易用的分布式调用接口，使用Future接口来进行并发。Dispy相比起Celery和SCOOP，Dispy提供更为轻量级的分布式并行服务PP&nbsp;PP （Parallel Python）是另外一个轻量级的Python并行服务，&nbsp;参考这里AsyncoroAsyncoro是另一个利用Generator实现分布式并发的Python框架，当然还有许多其它的系统，我没有一一列出另外，许多的分布式系统多提供了对Python接口的支持，例如Spark伪线程 （Pseudo－Thread）还有一种并发手段并不常见，我们可以称之为伪线程，就是看上去像是线程，使用的接口类似线程接口，但是实际使用非线程的方式，对应的线程开销也不存的。greenlet&nbsp;greenlet提供轻量级的coroutines来支持进程内的并发。greenlet是Stackless的一个副产品，使用tasklet来支持一中被称之为微线程（mirco－thread）的技术，这里是一个使用greenlet的伪线程的例子from&nbsp;greenlet&nbsp;import&nbsp;greenletdef&nbsp;test1():&nbsp;&nbsp;&nbsp;&nbsp;print&nbsp;12&nbsp;&nbsp;&nbsp;&nbsp;gr2.switch()&nbsp;&nbsp;&nbsp;&nbsp;print&nbsp;34&nbsp;&nbsp;&nbsp;&nbsp;def&nbsp;test2():&nbsp;&nbsp;&nbsp;&nbsp;print&nbsp;56&nbsp;&nbsp;&nbsp;&nbsp;gr1.switch()&nbsp;&nbsp;&nbsp;&nbsp;print&nbsp;78&nbsp;&nbsp;&nbsp;&nbsp;gr1&nbsp;=&nbsp;greenlet(test1)gr2&nbsp;=&nbsp;greenlet(test2)gr1.switch()&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;运行以上程序得到如下结果：125634伪线程gr1 switch会打印12，然后调用gr2 switch得到56，然后switch回到gr1，打印34，然后伪线程gr1结束，程序退出，所以78永远不会被打印。通过这个例子我们可以看出，使用伪线程，我们可以有效的控制程序的执行流程，但是伪线程并不存在真正意义上的并发。eventlet，gevent和concurence都是基于greenlet提供并发的。eventlet&nbsp;http://eventlet.net/eventlet是一个提供网络调用并发的Python库，使用者可以以非阻塞的方式来调用阻塞的IO操作。import&nbsp;eventletfrom&nbsp;eventlet.green&nbsp;import&nbsp;urllib2urls&nbsp;=&nbsp;['http://www.google.com',&nbsp;'http://www.example.com',&nbsp;'http://www.python.org']def&nbsp;fetch(url):&nbsp;&nbsp;&nbsp;&nbsp;return&nbsp;urllib2.urlopen(url).read()pool&nbsp;=&nbsp;eventlet.GreenPool()for&nbsp;body&nbsp;in&nbsp;pool.imap(fetch,&nbsp;urls):&nbsp;&nbsp;&nbsp;&nbsp;print("got&nbsp;body",&nbsp;len(body))执行结果如下('got&nbsp;body',&nbsp;17629)('got&nbsp;body',&nbsp;1270)('got&nbsp;body',&nbsp;46949)eventlet为了支持generator的操作对urllib2做了修改，接口和urllib2是一致的。这里的GreenPool和Python的Pool接口一致。geventgevent和eventlet类似，关于它们的差异大家可以参考这篇文章import&nbsp;geventfrom&nbsp;gevent&nbsp;import&nbsp;socketurls&nbsp;=&nbsp;['www.google.com',&nbsp;'www.example.com',&nbsp;'www.python.org']jobs&nbsp;=&nbsp;[gevent.spawn(socket.gethostbyname,&nbsp;url)&nbsp;for&nbsp;url&nbsp;in&nbsp;urls]gevent.joinall(jobs,&nbsp;timeout=2)print&nbsp;[job.value&nbsp;for&nbsp;job&nbsp;in&nbsp;jobs]执行结果如下：['206.169.145.226',&nbsp;'93.184.216.34',&nbsp;'23.235.39.223']concurence&nbsp;https://github.com/concurrence/concurrenceconcurence是另外一个利用greenlet提供网络并发的开源库，我没有用过，大家可以自己尝试一下。实战运用通常需要用到并发的场合有两种，一种是计算密集型，也就是说你的程序需要大量的CPU资源;另一种是IO密集型，程序可能有大量的读写操作，包括读写文件，收发网络请求等等。计算密集型对应计算密集型的应用，我们选用著名的蒙特卡洛算法来计算PI值。基本原理如下蒙特卡洛算法利用统计学原理来模拟计算圆周率，在一个正方形中，一个随机的点落在1/4圆的区域（红色点）的概率与其面积成正比。也就该概率 p ＝ Pi ＊ R＊R ／4 &nbsp;： R＊ R ， 其中R是正方形的边长，圆的半径。也就是说该概率是圆周率的1/4, 利用这个结论，只要我们模拟出点落在四分之一圆上的概率就可以知道圆周率了，为了得到这个概率，我们可以通过大量的实验，也就是生成大量的点，看看这个点在哪个区域，然后统计出结果。基本算法如下：from&nbsp;math&nbsp;import&nbsp;hypotfrom&nbsp;random&nbsp;import&nbsp;randomdef&nbsp;test(tries):&nbsp;&nbsp;&nbsp;&nbsp;return&nbsp;sum(hypot(random(),&nbsp;random())&nbsp;&lt;&nbsp;1&nbsp;for&nbsp;_&nbsp;in&nbsp;range(tries))这里test方法做了n（tries）次试验，返回落在四分之一圆中的点的个数。判断方法是检查该点到圆心的距离，如果小于R则是在圆上。通过大量的并发，我们可以快速的运行多次试验，试验的次数越多，结果越接近真实的圆周率。这里给出不同并发方法的程序代码非并发我们先在单线程，但进程运行，看看性能如何from&nbsp;math&nbsp;import&nbsp;hypotfrom&nbsp;random&nbsp;import&nbsp;randomimport&nbsp;eventletimport&nbsp;timedef&nbsp;test(tries):&nbsp;&nbsp;&nbsp;&nbsp;return&nbsp;sum(hypot(random(),&nbsp;random())&nbsp;&lt;&nbsp;1&nbsp;for&nbsp;_&nbsp;in&nbsp;range(tries))def&nbsp;calcPi(nbFutures,&nbsp;tries):&nbsp;&nbsp;&nbsp;&nbsp;ts&nbsp;=&nbsp;time.time()&nbsp;&nbsp;&nbsp;&nbsp;result&nbsp;=&nbsp;map(test,&nbsp;[tries]&nbsp;*&nbsp;nbFutures)&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;ret&nbsp;=&nbsp;4.&nbsp;*&nbsp;sum(result)&nbsp;/&nbsp;float(nbFutures&nbsp;*&nbsp;tries)&nbsp;&nbsp;&nbsp;&nbsp;span&nbsp;=&nbsp;time.time()&nbsp;-&nbsp;ts&nbsp;&nbsp;&nbsp;&nbsp;print&nbsp;"time&nbsp;spend&nbsp;",&nbsp;span&nbsp;&nbsp;&nbsp;&nbsp;return&nbsp;retprint&nbsp;calcPi(3000,4000)多线程 thread为了使用线程池，我们用multiprocessing的dummy包，它是对多线程的一个封装。注意这里代码虽然一个字的没有提到线程，但它千真万确是多线程。通过测试我们开（jing）心（ya）的发现，果然不出所料，当线程池为1是，它的运行结果和没有并发时一样，当我们把线程池数字设置为5时，耗时几乎是没有并发的2倍，我的测试数据从5秒到9秒。所以对于计算密集型的任务，还是放弃多线程吧。from&nbsp;multiprocessing.dummy&nbsp;import&nbsp;Poolfrom&nbsp;math&nbsp;import&nbsp;hypotfrom&nbsp;random&nbsp;import&nbsp;randomimport&nbsp;timedef&nbsp;test(tries):&nbsp;&nbsp;&nbsp;&nbsp;return&nbsp;sum(hypot(random(),&nbsp;random())&nbsp;&lt;&nbsp;1&nbsp;for&nbsp;_&nbsp;in&nbsp;range(tries))def&nbsp;calcPi(nbFutures,&nbsp;tries):&nbsp;&nbsp;&nbsp;&nbsp;ts&nbsp;=&nbsp;time.time()&nbsp;&nbsp;&nbsp;&nbsp;p&nbsp;=&nbsp;Pool(1)&nbsp;&nbsp;&nbsp;&nbsp;result&nbsp;=&nbsp;p.map(test,&nbsp;[tries]&nbsp;*&nbsp;nbFutures)&nbsp;&nbsp;&nbsp;&nbsp;ret&nbsp;=&nbsp;4.&nbsp;*&nbsp;sum(result)&nbsp;/&nbsp;float(nbFutures&nbsp;*&nbsp;tries)&nbsp;&nbsp;&nbsp;&nbsp;span&nbsp;=&nbsp;time.time()&nbsp;-&nbsp;ts&nbsp;&nbsp;&nbsp;&nbsp;print&nbsp;"time&nbsp;spend&nbsp;",&nbsp;span&nbsp;&nbsp;&nbsp;&nbsp;return&nbsp;retif&nbsp;__name__&nbsp;==&nbsp;'__main__':&nbsp;&nbsp;&nbsp;&nbsp;p&nbsp;=&nbsp;Pool()&nbsp;&nbsp;&nbsp;&nbsp;print("pi&nbsp;=&nbsp;{}".format(calcPi(3000,&nbsp;4000)))多进程 multiprocess理论上对于计算密集型的任务，使用多进程并发比较合适，在以下的例子中，进程池的规模设置为5，修改进程池的大小可以看到对结果的影响，当进程池设置为1时，和多线程的结果所需的时间类似，因为这时候并不存在并发；当设置为2时，响应时间有了明显的改进，是之前没有并发的一半；然而继续扩大进程池对性能影响并不大，甚至有所下降，也许我的Apple Air的CPU只有两个核？当心，如果你设置一个非常大的进程池，你会遇到 Resource temporarily unavailable的错误，系统并不能支持创建太多的进程，毕竟资源是有限的。from&nbsp;multiprocessing&nbsp;import&nbsp;Poolfrom&nbsp;math&nbsp;import&nbsp;hypotfrom&nbsp;random&nbsp;import&nbsp;randomimport&nbsp;timedef&nbsp;test(tries):&nbsp;&nbsp;&nbsp;&nbsp;return&nbsp;sum(hypot(random(),&nbsp;random())&nbsp;&lt;&nbsp;1&nbsp;for&nbsp;_&nbsp;in&nbsp;range(tries))def&nbsp;calcPi(nbFutures,&nbsp;tries):&nbsp;&nbsp;&nbsp;&nbsp;ts&nbsp;=&nbsp;time.time()&nbsp;&nbsp;&nbsp;&nbsp;p&nbsp;=&nbsp;Pool(5)&nbsp;&nbsp;&nbsp;&nbsp;result&nbsp;=&nbsp;p.map(test,&nbsp;[tries]&nbsp;*&nbsp;nbFutures)&nbsp;&nbsp;&nbsp;&nbsp;ret&nbsp;=&nbsp;4.&nbsp;*&nbsp;sum(result)&nbsp;/&nbsp;float(nbFutures&nbsp;*&nbsp;tries)&nbsp;&nbsp;&nbsp;&nbsp;span&nbsp;=&nbsp;time.time()&nbsp;-&nbsp;ts&nbsp;&nbsp;&nbsp;&nbsp;print&nbsp;"time&nbsp;spend&nbsp;",&nbsp;span&nbsp;&nbsp;&nbsp;&nbsp;return&nbsp;retif&nbsp;__name__&nbsp;==&nbsp;'__main__':&nbsp;&nbsp;&nbsp;&nbsp;print("pi&nbsp;=&nbsp;{}".format(calcPi(3000,&nbsp;4000)))gevent （伪线程）不论是gevent还是eventlet，因为不存在实际的并发，响应时间和没有并发区别不大，这个和测试结果一致。import&nbsp;geventfrom&nbsp;math&nbsp;import&nbsp;hypotfrom&nbsp;random&nbsp;import&nbsp;randomimport&nbsp;timedef&nbsp;test(tries):&nbsp;&nbsp;&nbsp;&nbsp;return&nbsp;sum(hypot(random(),&nbsp;random())&nbsp;&lt;&nbsp;1&nbsp;for&nbsp;_&nbsp;in&nbsp;range(tries))def&nbsp;calcPi(nbFutures,&nbsp;tries):&nbsp;&nbsp;&nbsp;&nbsp;ts&nbsp;=&nbsp;time.time()&nbsp;&nbsp;&nbsp;&nbsp;jobs&nbsp;=&nbsp;[gevent.spawn(test,&nbsp;t)&nbsp;for&nbsp;t&nbsp;in&nbsp;[tries]&nbsp;*&nbsp;nbFutures]&nbsp;&nbsp;&nbsp;&nbsp;gevent.joinall(jobs,&nbsp;timeout=2)&nbsp;&nbsp;&nbsp;&nbsp;ret&nbsp;=&nbsp;4.&nbsp;*&nbsp;sum([job.value&nbsp;for&nbsp;job&nbsp;in&nbsp;jobs])&nbsp;/&nbsp;float(nbFutures&nbsp;*&nbsp;tries)&nbsp;&nbsp;&nbsp;&nbsp;span&nbsp;=&nbsp;time.time()&nbsp;-&nbsp;ts&nbsp;&nbsp;&nbsp;&nbsp;print&nbsp;"time&nbsp;spend&nbsp;",&nbsp;span&nbsp;&nbsp;&nbsp;&nbsp;return&nbsp;retprint&nbsp;calcPi(3000,4000)eventlet （伪线程）from&nbsp;math&nbsp;import&nbsp;hypotfrom&nbsp;random&nbsp;import&nbsp;randomimport&nbsp;eventletimport&nbsp;timedef&nbsp;test(tries):&nbsp;&nbsp;&nbsp;&nbsp;return&nbsp;sum(hypot(random(),&nbsp;random())&nbsp;&lt;&nbsp;1&nbsp;for&nbsp;_&nbsp;in&nbsp;range(tries))def&nbsp;calcPi(nbFutures,&nbsp;tries):&nbsp;&nbsp;&nbsp;&nbsp;ts&nbsp;=&nbsp;time.time()&nbsp;&nbsp;&nbsp;&nbsp;pool&nbsp;=&nbsp;eventlet.GreenPool()&nbsp;&nbsp;&nbsp;&nbsp;result&nbsp;=&nbsp;pool.imap(test,&nbsp;[tries]&nbsp;*&nbsp;nbFutures)&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;ret&nbsp;=&nbsp;4.&nbsp;*&nbsp;sum(result)&nbsp;/&nbsp;float(nbFutures&nbsp;*&nbsp;tries)&nbsp;&nbsp;&nbsp;&nbsp;span&nbsp;=&nbsp;time.time()&nbsp;-&nbsp;ts&nbsp;&nbsp;&nbsp;&nbsp;print&nbsp;"time&nbsp;spend&nbsp;",&nbsp;span&nbsp;&nbsp;&nbsp;&nbsp;return&nbsp;retprint&nbsp;calcPi(3000,4000)SCOOPSCOOP中的Future接口符合PEP-3148的定义，也就是在Python3中提供的Future接口。在缺省的SCOOP配置环境下（单机，4个Worker），并发的性能有提高，但是不如两个进程池配置的多进程。from&nbsp;math&nbsp;import&nbsp;hypotfrom&nbsp;random&nbsp;import&nbsp;randomfrom&nbsp;scoop&nbsp;import&nbsp;futuresimport&nbsp;timedef&nbsp;test(tries):&nbsp;&nbsp;&nbsp;&nbsp;return&nbsp;sum(hypot(random(),&nbsp;random())&nbsp;&lt;&nbsp;1&nbsp;for&nbsp;_&nbsp;in&nbsp;range(tries))def&nbsp;calcPi(nbFutures,&nbsp;tries):&nbsp;&nbsp;&nbsp;&nbsp;ts&nbsp;=&nbsp;time.time()&nbsp;&nbsp;&nbsp;&nbsp;expr&nbsp;=&nbsp;futures.map(test,&nbsp;[tries]&nbsp;*&nbsp;nbFutures)&nbsp;&nbsp;&nbsp;&nbsp;ret&nbsp;=&nbsp;4.&nbsp;*&nbsp;sum(expr)&nbsp;/&nbsp;float(nbFutures&nbsp;*&nbsp;tries)&nbsp;&nbsp;&nbsp;&nbsp;span&nbsp;=&nbsp;time.time()&nbsp;-&nbsp;ts&nbsp;&nbsp;&nbsp;&nbsp;print&nbsp;"time&nbsp;spend&nbsp;",&nbsp;span&nbsp;&nbsp;&nbsp;&nbsp;return&nbsp;retif&nbsp;__name__&nbsp;==&nbsp;"__main__":&nbsp;&nbsp;&nbsp;&nbsp;print("pi&nbsp;=&nbsp;{}".format(calcPi(3000,&nbsp;4000)))Celery任务代码from&nbsp;celery&nbsp;import&nbsp;Celeryfrom&nbsp;math&nbsp;import&nbsp;hypotfrom&nbsp;random&nbsp;import&nbsp;random&nbsp;app&nbsp;=&nbsp;Celery('tasks',&nbsp;backend='amqp',&nbsp;broker='amqp://guest@localhost//')app.conf.CELERY_RESULT_BACKEND&nbsp;=&nbsp;'db+sqlite:///results.sqlite'&nbsp;@app.taskdef&nbsp;test(tries):&nbsp;&nbsp;&nbsp;&nbsp;return&nbsp;sum(hypot(random(),&nbsp;random())&nbsp;&lt;&nbsp;1&nbsp;for&nbsp;_&nbsp;in&nbsp;range(tries))客户端代码from&nbsp;celery&nbsp;import&nbsp;groupfrom&nbsp;tasks&nbsp;import&nbsp;testimport&nbsp;timedef&nbsp;calcPi(nbFutures,&nbsp;tries):&nbsp;&nbsp;&nbsp;&nbsp;ts&nbsp;=&nbsp;time.time()&nbsp;&nbsp;&nbsp;&nbsp;result&nbsp;=&nbsp;group(test.s(tries)&nbsp;for&nbsp;i&nbsp;in&nbsp;xrange(nbFutures))().get()&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;ret&nbsp;=&nbsp;4.&nbsp;*&nbsp;sum(result)&nbsp;/&nbsp;float(nbFutures&nbsp;*&nbsp;tries)&nbsp;&nbsp;&nbsp;&nbsp;span&nbsp;=&nbsp;time.time()&nbsp;-&nbsp;ts&nbsp;&nbsp;&nbsp;&nbsp;print&nbsp;"time&nbsp;spend&nbsp;",&nbsp;span&nbsp;&nbsp;&nbsp;&nbsp;return&nbsp;retprint&nbsp;calcPi(3000,&nbsp;4000)使用Celery做并发的测试结果出乎意料（环境是单机，4frefork的并发，消息broker是rabbitMQ），是所有测试用例里最糟糕的，响应时间是没有并发的5～6倍。这也许是因为控制协调的开销太大。对于这样的计算任务，Celery也许不是一个好的选择。asyncoroAsyncoro的测试结果和非并发保持一致。import&nbsp;asyncorofrom&nbsp;math&nbsp;import&nbsp;hypotfrom&nbsp;random&nbsp;import&nbsp;randomimport&nbsp;timedef&nbsp;test(tries):&nbsp;&nbsp;&nbsp;&nbsp;yield&nbsp;sum(hypot(random(),&nbsp;random())&nbsp;&lt;&nbsp;1&nbsp;for&nbsp;_&nbsp;in&nbsp;range(tries))def&nbsp;calcPi(nbFutures,&nbsp;tries):&nbsp;&nbsp;&nbsp;&nbsp;ts&nbsp;=&nbsp;time.time()&nbsp;&nbsp;&nbsp;&nbsp;coros&nbsp;=&nbsp;[&nbsp;asyncoro.Coro(test,t)&nbsp;for&nbsp;t&nbsp;in&nbsp;[tries]&nbsp;*&nbsp;nbFutures]&nbsp;&nbsp;&nbsp;&nbsp;ret&nbsp;=&nbsp;4.&nbsp;*&nbsp;sum([job.value()&nbsp;for&nbsp;job&nbsp;in&nbsp;coros])&nbsp;/&nbsp;float(nbFutures&nbsp;*&nbsp;tries)&nbsp;&nbsp;&nbsp;&nbsp;span&nbsp;=&nbsp;time.time()&nbsp;-&nbsp;ts&nbsp;&nbsp;&nbsp;&nbsp;print&nbsp;"time&nbsp;spend&nbsp;",&nbsp;span&nbsp;&nbsp;&nbsp;&nbsp;return&nbsp;retprint&nbsp;calcPi(3000,4000)IO密集型IO密集型的任务是另一种常见的用例，例如网络WEB服务器就是一个例子，每秒钟能处理多少个请求时WEB服务器的重要指标。我们就以网页读取作为最简单的例子from&nbsp;math&nbsp;import&nbsp;hypotimport&nbsp;timeimport&nbsp;urllib2urls&nbsp;=&nbsp;['http://www.google.com',&nbsp;'http://www.example.com',&nbsp;'http://www.python.org']def&nbsp;test(url):&nbsp;&nbsp;&nbsp;&nbsp;return&nbsp;urllib2.urlopen(url).read()def&nbsp;testIO(nbFutures):&nbsp;&nbsp;&nbsp;&nbsp;ts&nbsp;=&nbsp;time.time()&nbsp;&nbsp;&nbsp;&nbsp;map(test,&nbsp;urls&nbsp;*&nbsp;nbFutures)&nbsp;&nbsp;&nbsp;&nbsp;span&nbsp;=&nbsp;time.time()&nbsp;-&nbsp;ts&nbsp;&nbsp;&nbsp;&nbsp;print&nbsp;"time&nbsp;spend&nbsp;",&nbsp;spantestIO(10)在不同并发库下的代码，由于比较类似，我就不一一列出。大家可以参考计算密集型中代码做参考。通过测试我们可以发现，对于IO密集型的任务，使用多线程，或者是多进程都可以有效的提高程序的效率，而使用伪线程性能提升非常显著，eventlet比没有并发的情况下，响应时间从9秒提高到0.03秒。同时eventlet／gevent提供了非阻塞的异步调用模式，非常方便。这里推荐使用线程或者伪线程，因为在响应时间类似的情况下，线程和伪线程消耗的资源更少。总结Python提供了不同的并发方式，对应于不同的场景，我们需要选择不同的方式进行并发。选择合适的方式，不但要对该方法的原理有所了解，还应该做一些测试和试验，数据才是你做选择的最好参考。</div>
                            <div class='commentList'>
                    <h3>评论列表</h3>
                                    </div>
                    </div>
    
        <div class='blog'>
            <h2><a href="#top" class='top'>回到顶部</a><a name="blog_403939" href="https://my.oschina.net/taogang/blog/403939">用Python抓取亚马逊云（AWS）的日志（CloudTrail）数据</a></h2>
            <div class='outline'>
                <div class='date'>时间：2015-04-20 16:35:39</div>
                <div class='catalog'>分类：工作日志</div>
                                                                            </div>
            <div class='content'>如今是云的时代，许多公司都把自己的IT架构部署在基础架构云（IaaS）上。著名的IaaS提供商有亚马逊，微软（Azure），IBM等，国内也有诸如阿里云等。这里亚马逊毫无疑问是该市场的领军者。AWS提供了非常多的服务，领先了竞争对手一大截。并且AWS提供非常丰富的API，其API基于Rest，所以很容易被不同的语言的平台来调用。在如今的大数据时代，利用数据在做决策是大数据的核心价值，AWS提供了许多服务来获取其运行数据cloudtrail和cloudwatch是经常被用到的两个。CloudTrail是对AWS的所有API调用的日志，CloudWatch是监控AWS服务的性能数据。（新出的Config服务可用于监控AWS的资源变化）今天我们来看看如何使用Python（Boto AWS的开源Python SDK）来自动配置ClouTrail的服务并获取日志内容。我们先来看看CloudTrail的概念和相关的配置。S3 Bucket在打开CloudTrail的服务时，需要指定一个相关的S3的Bucket，S3是亚马逊提供的存储服务，你可以把它当作一个基于云的文件系统。CloudTrail的API调用日志，会以压缩文件的形式，存储在你指定的Bucket里。SNSSNS是亚马逊提供的通知服务，该服务使用的是订阅／发布（Subsrcibe／Publish）的模式。在创建CloudTrail的时候，可以关联一个SNS的Topic（可选），这样做的好处是当有API调用时，可以第一时间得到通知。可以使用不同的客户端来订阅SNS的通知，例如Email，Mobile的Notification Service，SQS等SQSSQS是亚马逊提供的队列服务，在本文中，我们使用SQS订阅SNS的的内容，这样我们的Python程序就可以从SQS的队列中获取相应的通知。配置CloudTrail首先我们需要创建SNS，并指定相应的策略。代码如下：import&nbsp;boto.snsimport&nbsp;jsonkey_id='yourawskeyid'secret_key='yourawssecretkey'region_name="eu-central-1"trail_topic_name="topicABC"sns_policy_sid="snspolicy0001"sns_conn&nbsp;=&nbsp;boto.sns.connect_to_region(region_name,&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;aws_access_key_id=key_id,&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;aws_secret_access_key=secret_key)sns_topic&nbsp;=&nbsp;sns_conn.create_topic(trail_topic_name)#&nbsp;Get&nbsp;ARN&nbsp;of&nbsp;SNS&nbsp;topicsns_arn&nbsp;=&nbsp;sns_topic['CreateTopicResponse']['CreateTopicResult']['TopicArn']#&nbsp;Add&nbsp;related&nbsp;policyattrs&nbsp;=&nbsp;sns_conn.get_topic_attributes(sns_arn)policy&nbsp;=&nbsp;attrs['GetTopicAttributesResponse']['GetTopicAttributesResult']['Attributes']['Policy']policy_obj&nbsp;=&nbsp;json.loads(policy)statements&nbsp;=&nbsp;policy_obj['Statement']default_statement&nbsp;=&nbsp;statements[0]new_statement&nbsp;=&nbsp;default_statement.copy()new_statement["Sid"]&nbsp;=&nbsp;sns_policy_sidnew_statement["Action"]&nbsp;=&nbsp;"SNS:Publish"new_statement["Principal"]&nbsp;=&nbsp;{&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;"AWS":&nbsp;[&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;"arn:aws:iam::903692715234:root",&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;"arn:aws:iam::035351147821:root",&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;"arn:aws:iam::859597730677:root",&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;"arn:aws:iam::814480443879:root",&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;"arn:aws:iam::216624486486:root",&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;"arn:aws:iam::086441151436:root",&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;"arn:aws:iam::388731089494:root",&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;"arn:aws:iam::284668455005:root",&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;"arn:aws:iam::113285607260:root"&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;]&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;}new_statement.pop("Condition",&nbsp;None)statements.append(new_statement)new_policy&nbsp;=&nbsp;json.dumps(policy_obj)sns_conn.set_topic_attributes(sns_arn,"Policy",new_policy)CloudTrail是和区域（Region）相关的，不同的Region有不同的CloudTrail服务，所以，在创建对应的SNS时，需要保证使用同一个Region。这里要注意的是我们创建了新的policy来使得CloudTrail拥有向我们创建的SNS发布消息（Action＝“SNS：Publish”）的权限。我们的做法是从缺省的策略中拷贝了一份，修改了相应的Action和Sid（随便取一个不重复的名字），Principal部分是一个缺省的account的列表，这里是硬编码，AWS有可能会修改该列表的值，但在当前环境下，该值是固定的。最后移除Condition的值。把新创建的Policy片段添加到原来的Policy中就好了。然后我们需要创建一个SQS的队列，并订阅我们创建的SNS的Topic。这一步相对比较简单。import&nbsp;boto.sqssqs_queue_name="sqs_queue"sqs_conn&nbsp;=&nbsp;boto.sqs.connect_to_region(region_name,&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;aws_access_key_id=key_id,&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;aws_secret_access_key=secret_key)sqs_queue&nbsp;=&nbsp;sqs_conn.create_queue(sqs_queue_name)sns_conn.subscribe_sqs_queue(sns_arn,&nbsp;sqs_queue)然后，我们需要创建一个S3的Bucket用来存储CloudTrail产生的日志文件。同样的，需要指定响应的策略以保证CloudTrail能够有权限写入对应的日志文件。import&nbsp;botobucket_name="bucket000"policy_sid="testpolicy000"s3_conn&nbsp;=&nbsp;boto.connect_s3(aws_access_key_id=key_id,aws_secret_access_key=secret_key)bucket&nbsp;=&nbsp;s3_conn.create_bucket(bucket_name)bucket_policy&nbsp;=&nbsp;'''{"Version":&nbsp;"2012-10-17","Statement":&nbsp;[{"Sid":&nbsp;"%Sid%GetPolicy","Effect":&nbsp;"Allow","Principal":&nbsp;{"AWS":&nbsp;["arn:aws:iam::903692715234:root","arn:aws:iam::035351147821:root","arn:aws:iam::859597730677:root","arn:aws:iam::814480443879:root","arn:aws:iam::216624486486:root","arn:aws:iam::086441151436:root","arn:aws:iam::388731089494:root","arn:aws:iam::284668455005:root","arn:aws:iam::113285607260:root"]},"Action":&nbsp;"s3:GetBucketAcl","Resource":&nbsp;"arn:aws:s3:::%bucket_name%"},{"Sid":&nbsp;"%Sid%PutPolicy","Effect":&nbsp;"Allow","Principal":&nbsp;{"AWS":&nbsp;["arn:aws:iam::903692715234:root","arn:aws:iam::035351147821:root","arn:aws:iam::859597730677:root","arn:aws:iam::814480443879:root","arn:aws:iam::216624486486:root","arn:aws:iam::086441151436:root","arn:aws:iam::388731089494:root","arn:aws:iam::284668455005:root","arn:aws:iam::113285607260:root"]},"Action":&nbsp;"s3:PutObject","Resource":&nbsp;"arn:aws:s3:::%bucket_name%/*","Condition":&nbsp;{"StringEquals":&nbsp;{"s3:x-amz-acl":&nbsp;"bucket-owner-full-control"}}}]}'''bucket_policy&nbsp;=&nbsp;bucket_policy.replace("%bucket_name%",bucket_name)bucket_policy&nbsp;=&nbsp;bucket_policy.replace("%Sid%",policy_sid)bucket.set_policy(bucket_policy)这里我们使用一个缺省的Policy文件，替换掉响应的字段就好了。最后，我们创建CloudTrail的服务：import&nbsp;boto.cloudtrailtrail_name="Trailabc"log_prefix="log"cloudtrail_conn=boto.cloudtrail.connect_to_region(region_name,&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;aws_access_key_id=key_id,&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;aws_secret_access_key=secret_key)##cloudtrail_conn.describe_trails()cloudtrail_conn.create_trail(trail_name,bucket_name,&nbsp;s3_key_prefix=log_prefix,sns_topic_name=trail_topic_name)cloudtrail_conn.start_logging(trail_name)好了，现在CloudTrail已经配置好了，并且关联的SNS也被我们创建的SQS队列订阅，下面我们就可以抓取日志了获取日志数据每当有一个API调用，CloudTrail都会把响应的日志文件写入到S3我们创建的Bucket中，同时在我们在创建的SNS的topic中发布一条消息，因为我们使用SQS的队列订阅了该消息，所以我们可以通过读取SQS消息的方式来获得日志数据。首先连接到SQS的队列，并从中读取消息import&nbsp;boto.sqssqs_queue_name="sqs_queue"sqs_conn&nbsp;=&nbsp;boto.sqs.connect_to_region(region_name,&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;aws_access_key_id=key_id,&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;aws_secret_access_key=secret_key)&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;sqs_queue&nbsp;=&nbsp;sqs_conn.get_queue（sqs_queue_name）notifications&nbsp;=&nbsp;sqs_queue.get_messages()然后我们从消息中获得响应的日志文件在S3中的地址，并利用该地址从S3中获得对应的日志文件for&nbsp;notification&nbsp;in&nbsp;notifications:&nbsp;&nbsp;&nbsp;&nbsp;envelope&nbsp;=&nbsp;json.loads(notification.get_body())&nbsp;&nbsp;&nbsp;&nbsp;message&nbsp;=&nbsp;json.loads(envelope['Message'])&nbsp;&nbsp;&nbsp;&nbsp;bucket_name&nbsp;=&nbsp;message['s3Bucket']&nbsp;&nbsp;&nbsp;&nbsp;s3_bucket&nbsp;=&nbsp;s3_conn.get_bucket(bucket_name)&nbsp;&nbsp;&nbsp;&nbsp;for&nbsp;key&nbsp;in&nbsp;message['s3ObjectKey']:&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;s3_file&nbsp;=&nbsp;s3_bucket.get_key(key)&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;with&nbsp;io.BytesIO(s3_file.read())&nbsp;as&nbsp;bfile:&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;with&nbsp;gzip.GzipFile(fileobj=bfile)&nbsp;as&nbsp;gz:&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;logjson&nbsp;=&nbsp;json.loads(gz.read())logjson就是对应的日记内容的JSON格式。这里有一个例子{&nbsp;&nbsp;&nbsp;&nbsp;"Records":&nbsp;[{&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;"eventVersion":&nbsp;"1.0",&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;"userIdentity":&nbsp;{&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;"type":&nbsp;"IAMUser",&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;"principalId":&nbsp;"EX_PRINCIPAL_ID",&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;"arn":&nbsp;"arn:aws:iam::123456789012:user/Alice",&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;"accessKeyId":&nbsp;"EXAMPLE_KEY_ID",&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;"accountId":&nbsp;"123456789012",&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;"userName":&nbsp;"Alice"&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;},&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;"eventTime":&nbsp;"2014-03-06T21:22:54Z",&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;"eventSource":&nbsp;"ec2.amazonaws.com",&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;"eventName":&nbsp;"StartInstances",&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;"awsRegion":&nbsp;"us-west-2",&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;"sourceIPAddress":&nbsp;"205.251.233.176",&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;"userAgent":&nbsp;"ec2-api-tools&nbsp;1.6.12.2",&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;"requestParameters":&nbsp;{&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;"instancesSet":&nbsp;{&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;"items":&nbsp;[{&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;"instanceId":&nbsp;"i-ebeaf9e2"&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;}]&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;}&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;},&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;"responseElements":&nbsp;{&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;"instancesSet":&nbsp;{&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;"items":&nbsp;[{&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;"instanceId":&nbsp;"i-ebeaf9e2",&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;"currentState":&nbsp;{&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;"code":&nbsp;0,&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;"name":&nbsp;"pending"&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;},&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;"previousState":&nbsp;{&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;"code":&nbsp;80,&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;"name":&nbsp;"stopped"&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;}&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;}]&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;}&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;}&nbsp;&nbsp;&nbsp;&nbsp;},&nbsp;&nbsp;&nbsp;&nbsp;...&nbsp;additional&nbsp;entries&nbsp;...]}你可以使用以上代码来监控所有的cloudtrail的日志，拿到的JSON格式的日志可以放在你的数据库（Mongo不错）中，然后利用你的BI工具做分析。注意你也可以不创建SNS和SQS，直接扫描bucket的内容，这样做的好处是配置更简单，缺点是实时性比较差，扫面Bucket需要额外的计算，并且需要在本地保存文件扫描的状态，code会更加复杂。利用CloudTrail的日志，你可以做很多事情，比如看看有没有非法的登陆，各个服务的使用频率，总之，当你有了足够多的数据，你就可以从中发现足够的价值。</div>
                    </div>
    
        <div class='blog'>
            <h2><a href="#top" class='top'>回到顶部</a><a name="blog_410864" href="https://my.oschina.net/taogang/blog/410864">使用Python进行分布式系统协调 （ZooKeeper，Consul， etcd ）</a></h2>
            <div class='outline'>
                <div class='date'>时间：2015-05-05 15:06:56</div>
                <div class='catalog'>分类：工作日志</div>
                                                                            </div>
            <div class='content'>笔者之前的博文提到过，随着大数据时代的到来，分布式是解决大数据问题的一个主要手段，随着越来越多的分布式的服务，如何在分布式的系统中对这些服务做协调变成了一个很棘手的问题。今天我们就来看看如何使用Python，利用开源对分布式服务做协调。在对分布式的应用做协调的时候，主要会碰到以下的应用场景：业务发现（service discovery）找到分布式系统中存在那些可用的服务和节点名字服务 （name service）通过给定的名字知道到对应的资源配置管理 （configuration management）如何在分布式的节点中共享配置文件，保证一致性。故障发现和故障转移 （failure detection and failover）当某一个节点出故障的时候，如何检测到并通知其它节点， 或者把想用的服务转移到其它的可用节点领导选举（leader election）如何在众多的节点中选举一个领导者，来协调所有的节点分布式的锁 （distributed exclusive lock）如何通过锁在分布式的服务中进行同步消息和通知服务 （message queue and notification）如何在分布式的服务中传递消息，以通知的形式对事件作出主动的响应有许多的开源软件试图解决以上的全部或者部分问题，例如ZooKeeper，consul，doozerd等等，我们现在就看看它们是如何做的。ZooKeeperZooKeeper是使用最广泛，也是最有名的解决分布式服务的协调问题的开源软件了，它最早和Hadoop一起开发，后来成为了Apache的顶级项目，很多开源的项目都在使用ZooKeeper，例如大名鼎鼎的Kafka。Zookeeper本身是一个分布式的应用，通过对共享的数据的管理来实现对分布式应用的协调。ZooKeeper使用一个树形目录作为数据模型，这个目录和文件目录类似，目录上的每一个节点被称作ZNodes。ZooKeeper提供基本的API来操纵和控制Znodes，包括对节点的创建，删除，设置和获取数据，获得子节点等。除了这些基本的操作，ZooKeeper还提供了一些配方（Recipe），其实就是一些常见的用例，例如锁，两阶段提交，领导选举等等。ZooKeeper本身是用Java开发的，所以对Java的支持是最自然的。它同时还提供了C语言的绑定。Kazoo是一个非常成熟的Zookeeper Python客户端，我们这就看看如果使用Python来调用ZooKeeper。（注意，运行以下的例子，需要在本地启动ZooKeeper的服务）基本操作以下的例子现实了对Znode的基本操作，首先要创建一个客户端的连接，并启动客户端。然后我们可以利用该客户端对Znode做增删改，取内容的操作。最后推出客户端。from&nbsp;kazoo.client&nbsp;import&nbsp;KazooClientimport&nbsp;logginglogging.basicConfig()zk&nbsp;=&nbsp;KazooClient(hosts='127.0.0.1:2181')zk.start()#&nbsp;Ensure&nbsp;a&nbsp;path,&nbsp;create&nbsp;if&nbsp;necessaryzk.ensure_path("/test/zk1")#&nbsp;Create&nbsp;a&nbsp;node&nbsp;with&nbsp;datazk.create("/test/zk1/node",&nbsp;b"a&nbsp;test&nbsp;value")#&nbsp;Determine&nbsp;if&nbsp;a&nbsp;node&nbsp;existsif&nbsp;zk.exists("/test/zk1"):&nbsp;&nbsp;&nbsp;&nbsp;print&nbsp;"the&nbsp;node&nbsp;exist"#&nbsp;Print&nbsp;the&nbsp;version&nbsp;of&nbsp;a&nbsp;node&nbsp;and&nbsp;its&nbsp;datadata,&nbsp;stat&nbsp;=&nbsp;zk.get("/test/zk1")print("Version:&nbsp;%s,&nbsp;data:&nbsp;%s"&nbsp;%&nbsp;(stat.version,&nbsp;data.decode("utf-8")))#&nbsp;List&nbsp;the&nbsp;childrenchildren&nbsp;=&nbsp;zk.get_children("/test/zk1")print("There&nbsp;are&nbsp;%s&nbsp;children&nbsp;with&nbsp;names&nbsp;%s"&nbsp;%&nbsp;(len(children),&nbsp;children))zk.stop()通过对ZNode的操作，我们可以完成一些分布式服务协调的基本需求，包括名字服务，配置服务，分组等等。故障检测(Failure Detection)在分布式系统中，一个最基本的需求就是当某一个服务出问题的时候，能够通知其它的节点或者某个管理节点。ZooKeeper提供ephemeral Node的概念，当创建该Node的服务退出或者异常中止的时候，该Node会被删除，所以我们就可以利用这种行为来监控服务运行状态。以下是worker的代码from&nbsp;kazoo.client&nbsp;import&nbsp;KazooClientimport&nbsp;timeimport&nbsp;logginglogging.basicConfig()zk&nbsp;=&nbsp;KazooClient(hosts='127.0.0.1:2181')zk.start()#&nbsp;Ensure&nbsp;a&nbsp;path,&nbsp;create&nbsp;if&nbsp;necessaryzk.ensure_path("/test/failure_detection")#&nbsp;Create&nbsp;a&nbsp;node&nbsp;with&nbsp;datazk.create("/test/failure_detection/worker",&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;value=b"a&nbsp;test&nbsp;value",&nbsp;ephemeral=True)while&nbsp;True:&nbsp;&nbsp;&nbsp;&nbsp;print&nbsp;"I&nbsp;am&nbsp;alive!"&nbsp;&nbsp;&nbsp;&nbsp;time.sleep(3)zk.stop()以下的monitor 代码，监控worker服务是否运行。from&nbsp;kazoo.client&nbsp;import&nbsp;KazooClientimport&nbsp;timeimport&nbsp;logginglogging.basicConfig()zk&nbsp;=&nbsp;KazooClient(hosts='127.0.0.1:2181')zk.start()#&nbsp;Determine&nbsp;if&nbsp;a&nbsp;node&nbsp;existswhile&nbsp;True:&nbsp;&nbsp;&nbsp;&nbsp;if&nbsp;zk.exists("/test/failure_detection/worker"):&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;print&nbsp;"the&nbsp;worker&nbsp;is&nbsp;alive!"&nbsp;&nbsp;&nbsp;&nbsp;else:&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;print&nbsp;"the&nbsp;worker&nbsp;is&nbsp;dead!"&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;break&nbsp;&nbsp;&nbsp;&nbsp;time.sleep(3)zk.stop()领导选举Kazoo直接提供了领导选举的API，使用起来非常方便。from&nbsp;kazoo.client&nbsp;import&nbsp;KazooClientimport&nbsp;timeimport&nbsp;uuidimport&nbsp;logginglogging.basicConfig()my_id&nbsp;=&nbsp;uuid.uuid4()def&nbsp;leader_func():&nbsp;&nbsp;&nbsp;&nbsp;print&nbsp;"I&nbsp;am&nbsp;the&nbsp;leader&nbsp;{}".format(str(my_id))&nbsp;&nbsp;&nbsp;&nbsp;while&nbsp;True:&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;print&nbsp;"{}&nbsp;is&nbsp;working!&nbsp;".format(str(my_id))&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;time.sleep(3)zk&nbsp;=&nbsp;KazooClient(hosts='127.0.0.1:2181')zk.start()election&nbsp;=&nbsp;zk.Election("/electionpath")#&nbsp;blocks&nbsp;until&nbsp;the&nbsp;election&nbsp;is&nbsp;won,&nbsp;then&nbsp;calls#&nbsp;leader_func()election.run(leader_func)zk.stop()你可以同时运行多个worker，其中一个会获得Leader，当你杀死当前的leader后，会有一个新的leader被选出。分布式锁锁的概念大家都熟悉，当我们希望某一件事在同一时间只有一个服务在做，或者某一个资源在同一时间只有一个服务能访问，这个时候，我们就需要用到锁。from&nbsp;kazoo.client&nbsp;import&nbsp;KazooClientimport&nbsp;timeimport&nbsp;uuidimport&nbsp;logginglogging.basicConfig()my_id&nbsp;=&nbsp;uuid.uuid4()def&nbsp;work():&nbsp;&nbsp;&nbsp;&nbsp;print&nbsp;"{}&nbsp;is&nbsp;working!&nbsp;".format(str(my_id))zk&nbsp;=&nbsp;KazooClient(hosts='127.0.0.1:2181')zk.start()lock&nbsp;=&nbsp;zk.Lock("/lockpath",&nbsp;str(my_id))print&nbsp;"I&nbsp;am&nbsp;{}".format(str(my_id))while&nbsp;True:&nbsp;&nbsp;&nbsp;&nbsp;with&nbsp;lock:&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;work()&nbsp;&nbsp;&nbsp;&nbsp;time.sleep(3)&nbsp;&nbsp;&nbsp;&nbsp;zk.stop()当你运行多个worker的时候，不同的worker会试图获取同一个锁，然而只有一个worker会工作，其它的worker必须等待获得锁后才能执行。监视ZooKeeper提供了监视（Watch）的功能，当节点的数据被修改的时候，监控的function会被调用。我们可以利用这一点进行配置文件的同步，发消息，或其他需要通知的功能。from&nbsp;kazoo.client&nbsp;import&nbsp;KazooClientimport&nbsp;timeimport&nbsp;logginglogging.basicConfig()zk&nbsp;=&nbsp;KazooClient(hosts='127.0.0.1:2181')zk.start()@zk.DataWatch('/path/to/watch')def&nbsp;my_func(data,&nbsp;stat):&nbsp;&nbsp;&nbsp;&nbsp;if&nbsp;data:&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;print&nbsp;"Data&nbsp;is&nbsp;%s"&nbsp;%&nbsp;data&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;print&nbsp;"Version&nbsp;is&nbsp;%s"&nbsp;%&nbsp;stat.version&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;else&nbsp;:&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;print&nbsp;"data&nbsp;is&nbsp;not&nbsp;available"while&nbsp;True:&nbsp;&nbsp;&nbsp;&nbsp;time.sleep(10)zk.stop()除了我们上面列举的内容外，Kazoo还提供了许多其他的功能，例如：计数，租约，队列等等，大家有兴趣可以参考它的文档ConsulConsul是用Go开发的分布式服务协调管理的工具，它提供了服务发现，健康检查，Key／Value存储等功能，并且支持跨数据中心的功能。Consul提供ZooKeeper类似的功能，它的基于HTTP的API可以方便的和各种语言进行绑定。自然Python也在列。与Zookeeper有所差异的是Consul通过基于Client／Server架构的Agent部署来支持跨Data Center的功能。Consul在Cluster伤的每一个节点都运行一个Agent，这个Agent可以使Server或者Client模式。Client负责到Server的高效通信，相对为无状态的。 Server负责包括选举领导节点，维护cluster的状态，对所有的查询做响应，跨数据中心的通信等等。KV基本操作类似于Zookeeper，Consul支持对KV的增删查改的操作。import&nbsp;consulc&nbsp;=&nbsp;consul.Consul()#&nbsp;set&nbsp;data&nbsp;for&nbsp;key&nbsp;fooc.kv.put('foo',&nbsp;'bar')#&nbsp;poll&nbsp;a&nbsp;key&nbsp;for&nbsp;updatesindex&nbsp;=&nbsp;Nonewhile&nbsp;True:&nbsp;&nbsp;&nbsp;&nbsp;index,&nbsp;data&nbsp;=&nbsp;c.kv.get('foo',&nbsp;index=index)&nbsp;&nbsp;&nbsp;&nbsp;print&nbsp;data['Value']&nbsp;&nbsp;&nbsp;&nbsp;c.kv.delete('foo')这里和ZooKeeper对Znode的操作几乎是一样的。服务发现（Service Discovery）和健康检查（Health Check）Consul的另一个主要的功能是用于对分布式的服务做管理，用户可以注册一个服务，同时还提供对服务做健康检测的功能。首先，用户需要定义一个服务。{&nbsp;&nbsp;"service":&nbsp;{&nbsp;&nbsp;&nbsp;&nbsp;"name":&nbsp;"redis",&nbsp;&nbsp;&nbsp;&nbsp;"tags":&nbsp;["master"],&nbsp;&nbsp;&nbsp;&nbsp;"address":&nbsp;"127.0.0.1",&nbsp;&nbsp;&nbsp;&nbsp;"port":&nbsp;8000,&nbsp;&nbsp;&nbsp;&nbsp;"checks":&nbsp;[&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;{&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;"script":&nbsp;"/usr/local/bin/check_redis.py",&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;"interval":&nbsp;"10s"&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;}&nbsp;&nbsp;&nbsp;&nbsp;]&nbsp;&nbsp;}}其中，服务的名字是必须的，其它的字段可以自选，包括了服务的地址，端口，相应的健康检查的脚本。当用户注册了一个服务后，就可以通过Consul来查询该服务，获得该服务的状态。Consul支持三种Check的模式：调用一个外部脚本（Script），在该模式下，consul定时会调用一个外部脚本，通过脚本的返回内容获得对应服务的健康状态。调用HTTP，在该模式下，consul定时会调用一个HTTP请求，返回2XX，则为健康；429 （Too many request）是警告。其它均为不健康主动上报，在该模式下，服务需要主动调用一个consul提供的HTTP PUT请求，上报健康状态。Python API提供对应的接口，大家可以参考&nbsp;http://python-consul.readthedocs.org/en/latest/Consul.Agent.ServiceConsul.Agent.CheckConsul的Health Check和Zookeeper的Failure Detection略有不同，ZooKeeper可以利用ephemeral Node来检测服务的状态，Consul的Health Check，通过调用脚本，HTTP或者主动上报的方式检查服务的状态，更为灵活，可以获得等多的信息，但是也需要做更多的工作。故障检测(Failure Detection)Consul提供Session的概念，利用Session可以检查服务是否存活。对每一个服务我们都可以创建一个session对象，注意这里我们设置了ttl，consul会以ttl的数值为间隔时间，持续的对session的存活做检查。对应的在服务中，我们需要持续的renew session，保证session是合法的。import&nbsp;consulimport&nbsp;timec&nbsp;=&nbsp;consul.Consul()s&nbsp;=&nbsp;c.session.create(name="worker",behavior='delete',ttl=10)print&nbsp;"session&nbsp;id&nbsp;is&nbsp;{}".format(s)while&nbsp;True:&nbsp;&nbsp;&nbsp;&nbsp;c.session.renew(s)&nbsp;&nbsp;&nbsp;&nbsp;print&nbsp;"I&nbsp;am&nbsp;alive&nbsp;..."&nbsp;&nbsp;&nbsp;&nbsp;time.sleep(3)Moniter代码用于监控worker相关联的session的状态，但发现worker session已经不存在了，就做出响应的处理。import&nbsp;consulimport&nbsp;timedef&nbsp;is_session_exist(name,&nbsp;sessions):&nbsp;&nbsp;&nbsp;&nbsp;for&nbsp;s&nbsp;in&nbsp;sessions:&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;if&nbsp;s['Name']&nbsp;==&nbsp;name:&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;return&nbsp;True&nbsp;&nbsp;&nbsp;&nbsp;return&nbsp;Falsec&nbsp;=&nbsp;consul.Consul()while&nbsp;True:&nbsp;&nbsp;&nbsp;&nbsp;index,&nbsp;sessions&nbsp;=&nbsp;c.session.list()&nbsp;&nbsp;&nbsp;&nbsp;if&nbsp;is_session_exist('worker',&nbsp;sessions):&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;print&nbsp;"worker&nbsp;is&nbsp;alive&nbsp;..."&nbsp;&nbsp;&nbsp;&nbsp;else:&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;print&nbsp;'worker&nbsp;is&nbsp;dead!'&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;break&nbsp;&nbsp;&nbsp;&nbsp;time.sleep(3)这里注意，因为是基于ttl（最小10秒）的检测，从业务中断到被检测到，至少有10秒的时延，对应需要实时响应的情景，并不适用。Zookeeper使用ephemeral Node的方式时延相对短一点，但也非实时。领导选举和分布式的锁无论是Consul本身还是Python客户端，都不直接提供Leader Election的功能，但是这篇文档介绍了如何利用Consul的KV存储来实现Leader Election,利用Consul的KV功能，可以很方便的实现领导选举和锁的功能。当对某一个Key做put操作的时候，可以创建一个session对象，设置一个acquire标志为该 session，这样就获得了一个锁，获得所得客户则是被选举的leader。代码如下：import&nbsp;consulimport&nbsp;timec&nbsp;=&nbsp;consul.Consul()def&nbsp;request_lead(namespace,&nbsp;session_id):&nbsp;&nbsp;&nbsp;&nbsp;lock&nbsp;=&nbsp;c.kv.put(leader_namespace,"leader&nbsp;check",&nbsp;acquire=session_id)&nbsp;&nbsp;&nbsp;&nbsp;return&nbsp;lockdef&nbsp;release_lead(session_id):&nbsp;&nbsp;&nbsp;&nbsp;c.session.destroy(session_id)def&nbsp;whois_lead(namespace):&nbsp;&nbsp;&nbsp;&nbsp;index,value&nbsp;=&nbsp;c.kv.get(namespace)&nbsp;&nbsp;&nbsp;&nbsp;session&nbsp;=&nbsp;value.get('Session')&nbsp;&nbsp;&nbsp;&nbsp;if&nbsp;session&nbsp;is&nbsp;None:&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;print&nbsp;'No&nbsp;one&nbsp;is&nbsp;leading,&nbsp;maybe&nbsp;in&nbsp;electing'&nbsp;&nbsp;&nbsp;&nbsp;else:&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;index,&nbsp;value&nbsp;=&nbsp;c.session.info(session)&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;print&nbsp;'{}&nbsp;is&nbsp;leading'.format(value['ID'])def&nbsp;work_non_block():&nbsp;&nbsp;&nbsp;&nbsp;print&nbsp;"working"def&nbsp;work_block():&nbsp;&nbsp;&nbsp;&nbsp;while&nbsp;True:&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;print&nbsp;"working"&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;time.sleep(3)leader_namespace&nbsp;=&nbsp;'leader/test'##&nbsp;initialize&nbsp;leader&nbsp;key/value&nbsp;nodeleader_index,&nbsp;leader_node&nbsp;=&nbsp;c.kv.get(leader_namespace)if&nbsp;leader_node&nbsp;is&nbsp;None:&nbsp;&nbsp;&nbsp;&nbsp;c.kv.put(leader_namespace,"a&nbsp;leader&nbsp;test")while&nbsp;True:&nbsp;&nbsp;&nbsp;&nbsp;whois_lead(leader_namespace)&nbsp;&nbsp;&nbsp;&nbsp;session_id&nbsp;=&nbsp;c.session.create(ttl=10)&nbsp;&nbsp;&nbsp;&nbsp;if&nbsp;request_lead(leader_namespace,session_id):&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;print&nbsp;"I&nbsp;am&nbsp;now&nbsp;the&nbsp;leader"&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;work_block()&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;release_lead(session_id)&nbsp;&nbsp;&nbsp;&nbsp;else:&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;print&nbsp;"wait&nbsp;leader&nbsp;elected!"&nbsp;&nbsp;&nbsp;&nbsp;time.sleep(3)利用同样的机制，可以方便的实现锁，信号量等分布式的同步操作。监视Consul的Agent提供了Watch的功能，然而Python客户端并没有相应的接口。etcdetcd是另一个用GO开发的分布式协调应用，它提供一个分布式的Key／Value存储来进行共享的配置管理和服务发现。同样的etcd使用基于HTTP的API，可以灵活的进行不同语言的绑定，我们用的是这个客户端https://github.com/jplana/python-etcd 基本操作import&nbsp;etcdclient&nbsp;=&nbsp;etcd.Client()&nbsp;client.write('/nodes/n1',&nbsp;1)print&nbsp;client.read('/nodes/n1').valueetcd对节点的操作和ZooKeeper类似，不过etcd不支持ZooKeeper的ephemeral Node的概念，要监控服务的状态似乎比较麻烦。分布式锁etcd支持分布式锁，以下是一个例子。import&nbsp;syssys.path.append("../../")import&nbsp;etcdimport&nbsp;uuidimport&nbsp;timemy_id&nbsp;=&nbsp;uuid.uuid4()def&nbsp;work():&nbsp;&nbsp;&nbsp;&nbsp;print&nbsp;"I&nbsp;get&nbsp;the&nbsp;lock&nbsp;{}".format(str(my_id))client&nbsp;=&nbsp;etcd.Client()&nbsp;lock&nbsp;=&nbsp;etcd.Lock(client,&nbsp;'/customerlock',&nbsp;ttl=60)with&nbsp;lock&nbsp;as&nbsp;my_lock:&nbsp;&nbsp;&nbsp;&nbsp;work()&nbsp;&nbsp;&nbsp;&nbsp;lock.is_locked()&nbsp;&nbsp;#&nbsp;True&nbsp;&nbsp;&nbsp;&nbsp;lock.renew(60)lock.is_locked()&nbsp;&nbsp;#&nbsp;False老版本的etcd支持leader election，但是在最新版该功能被deprecated了，参见https://coreos.com/etcd/docs/0.4.7/etcd-modules/其它我们针对分布式协调的功能讨论了三个不同的开源应用，其实还有许多其它的选择，我这里就不一一介绍，大家有兴趣可以访问以下的链接：eureka&nbsp;https://github.com/Netflix/eurekaNetflix开发的定位服务，应用于fail over和load balance的功能curator&nbsp;http://curator.apache.org/基于ZooKeeper的更高层次的封装doozerd&nbsp;https://github.com/ha/doozerd基于GO的高可靠，分布式的数据存储，过去两年已经不活跃openreplica&nbsp;http://openreplica.org/基于Python开发的，面向对象的接口的分布式应用协调的工具serf&nbsp;http://www.serfdom.io/serf提供轻量级的cluster成员管理，故障检测（failure detection）和协调。开发基于GO语言。Consul使用了serf提供的功能noah&nbsp;https://github.com/lusis/Noah基于ruby的ZooKeeper实现，过去三年不活跃copy cat&nbsp;https://github.com/kuujo/copycat基于日志的分布式协调的框架，使用Java开发总结ZooKeeper无疑是分布式协调应用的最佳选择，功能全，社区活跃，用户群体很大，对所有典型的用例都有很好的封装，支持不同语言的绑定。缺点是，整个应用比较重，依赖于Java，不支持跨数据中心。Consul作为使用Go语言开发的分布式协调，对业务发现的管理提供很好的支持，他的HTTP API也能很好的和不同的语言绑定，并支持跨数据中心的应用。缺点是相对较新，适合喜欢尝试新事物的用户。etcd是一个更轻量级的分布式协调的应用，提供了基本的功能，更适合一些轻量级的应用来使用。参考如果大家对于分布式系统的协调想要进行更多的了解，可以阅读一下的链接：http://stackoverflow.com/questions/6047917/zookeeper-alternatives-cluster-coordination-servicehttp://txt.fliglio.com/2014/05/encapsulated-services-with-consul-and-confd/http://txt.fliglio.com/2013/12/service-discovery-with-docker-docker-links-and-beyond/http://www.serfdom.io/intro/vs-zookeeper.htmlhttp://devo.ps/blog/zookeeper-vs-doozer-vs-etcd/https://www.digitalocean.com/community/articles/how-to-set-up-a-serf-cluster-on-several-ubuntu-vpshttp://www.slideshare.net/JyrkiPulliainen/taming-pythons-with-zoo-keeper-ep2013?qid=e1267f58-090d-4147-9909-ec673525e76b&amp;v=qf1&amp;b=&amp;from_search=8http://muratbuffalo.blogspot.com/2014/09/paper-summary-tango-distributed-data.htmlhttps://developer.yahoo.com/blogs/hadoop/apache-zookeeper-making-417.htmlhttp://www.knewton.com/tech/blog/2014/12/eureka-shouldnt-use-zookeeper-service-discovery/http://codahale.com/you-cant-sacrifice-partition-tolerance/</div>
                    </div>
    
        <div class='blog'>
            <h2><a href="#top" class='top'>回到顶部</a><a name="blog_485586" href="https://my.oschina.net/taogang/blog/485586">谈谈程序员面试之刷题</a></h2>
            <div class='outline'>
                <div class='date'>时间：2015-07-30 13:41:06</div>
                <div class='catalog'>分类：日常记录</div>
                                                                            </div>
            <div class='content'>前一段时间有一个非常有趣的故事（http://www.pingwest.com/sorry-cant-hire-you/&nbsp; ），Max Howell （Homebrew的作者） 在 Google 面试时遇到了让人悲伤的情境，google拒绝了Max， 给出了答复：“我们90%的工程师都用你写的软件，但抱歉我们不能聘用你，因为你没法在白板上翻转二叉树”。这确实是一个令人悲伤的故事，作为一个程序员，类似的算法也许在你的开发中一辈子也不会用到，但是为了面试，我们必须要熟悉这些算法，下面给出一些网站，可能能够帮助避免以上的情景。刷题网站一个办法就是刷题，国内外都有一些网站来帮助程序员刷题，大部分是算法题。牛客网 &nbsp;http://www.nowcoder.com/&nbsp;。牛客网是一个比较全面的程序员刷题网站，集合了大量的各大公司的面试题，提供在线做题的功能， 并有很多课程。脑客&nbsp;http://www.knockgate.com/&nbsp;另一个专注于程序员刷题的网站，没有什么真题，但是有很多培训和视频九章算法 http://www.jiuzhang.com/&nbsp;提供大量的算法培训和讲座内容，同时能够查询所有leetCode和LintCode的答案，也有很多的面试题，很全面的刷题网站。LeetCode&nbsp;https://leetcode.com/&nbsp;这个就不用介绍了，大名鼎鼎的LeetCode，支持在线做题，语言上支持C++，Java，Python，C#，Ruby，C和JavaScriptLintCode&nbsp;http://www.lintcode.com/en/&nbsp;另一个在线刷题培训，类似LeetCode，提供Lint的功能CodeEval&nbsp;https://www.codeeval.com/&nbsp;CodeEval在刷题的同时，提供了更丰富的社交功能，可以看到自己解题的速度和性能的排名，并且可以借助解答各大公司的题目来直接解锁该公司的面试。程序员招聘网站许多企业现在会采用一些辅助招聘程序员的网站，大家如果知道你要应聘的企业采用了哪一家，也可以主动去这些地方刷题。嘿嘿，有作弊的嫌疑！Codility&nbsp;https://codility.com/&nbsp;Codility可以帮助公司出一套在线的题目，来减轻程序员招聘是出题的负担。功能比较强大，缺点是题目有点少，也不能自主出题，必须从题库中选题。浙江大学PAT&nbsp;http://www.patest.cn/&nbsp;浙江大学的PAT是国内比较有名的程序员能力考试，大家也可以上去刷题。他们同时提供企业服务，可以帮助企业招聘出题。另外像TestDome和PotKnox和Codility类似，大家也可以去看看。TestDome&nbsp;http://www.testdome.com/&nbsp;PotKnox&nbsp;http://www.potknox.com/&nbsp;好了，最后祝所有的程序猿，码农都能找到自己称心如意的工作！</div>
                    </div>
    
        <div class='blog'>
            <h2><a href="#top" class='top'>回到顶部</a><a name="blog_497932" href="https://my.oschina.net/taogang/blog/497932">美丽的曼陀罗曲线</a></h2>
            <div class='outline'>
                <div class='date'>时间：2015-08-27 14:17:03</div>
                <div class='catalog'>分类：编程语言</div>
                                                                            </div>
            <div class='content'>最近看到一篇微信朋友圈上的文章，说两个行星运行轨迹的中心连线可以画出一个美丽的曼陀罗曲线，于是就写了一段代码生成这样的曲线，结果真是令人惊叹的美丽。代码参见 ：http://runjs.cn/detail/lbgqwfiu 或者http://codepen.io/gangtao/pen/oXKqYB 这里是一部分生成曲线的图v1=15，v2=50， 七瓣花v1=20，v2=50， 三叶草v1=30，v2=50， 心连心v1=35，v2=50，小三叶草v1=40,v2=50, 心其实画的过程更美，大家自己去试试吧。这里只列了一部分，更多的留给大家去尝试。</div>
                            <div class='commentList'>
                    <h3>评论列表</h3>
                                    </div>
                    </div>
    
        <div class='blog'>
            <h2><a href="#top" class='top'>回到顶部</a><a name="blog_524385" href="https://my.oschina.net/taogang/blog/524385">大数据系统数据采集产品的架构分析</a></h2>
            <div class='outline'>
                <div class='date'>时间：2015-10-31 20:32:51</div>
                <div class='catalog'>分类：架构设计</div>
                                                                            </div>
            <div class='content'>任何完整的大数据平台，一般包括以下的几个过程：数据采集数据存储数据处理数据展现（可视化，报表和监控）其中，数据采集是所有数据系统必不可少的，随着大数据越来越被重视，数据采集的挑战也变的尤为突出。这其中包括：数据源多种多样数据量大，变化快如何保证数据采集的可靠性的性能如何避免重复数据如何保证数据的质量我们今天就来看看当前可用的一些数据采集的产品，重点关注一些它们是如何做到高可靠，高性能和高扩展。Apache FlumeFlume 是Apache旗下，开源，高可靠，高扩展，容易管理，支持客户扩展的数据采集系统。 Flume使用JRuby来构建，所以依赖Java运行环境。Flume最初是由Cloudera的工程师设计用于合并日志数据的系统，后来逐渐发展用于处理流数据事件。Flume设计成一个分布式的管道架构，可以看作在数据源和目的地之间有一个Agent的网络，支持数据路由。每一个agent都由Source，Channel和Sink组成。SourceSource负责接收输入数据，并将数据写入管道。Flume的Source支持HTTP，JMS，RPC，NetCat，Exec，Spooling Directory。其中Spooling支持监视一个目录或者文件，解析其中新生成的事件。ChannelChannel 存储，缓存从source到Sink的中间数据。可使用不同的配置来做Channel，例如内存，文件，JDBC等。使用内存性能高但不持久，有可能丢数据。使用文件更可靠，但性能不如内存。SinkSink负责从管道中读出数据并发给下一个Agent或者最终的目的地。Sink支持的不同目的地种类包括：HDFS，HBASE，Solr，ElasticSearch，File，Logger或者其它的Flume AgentFlume在source和sink端都使用了transaction机制保证在数据传输中没有数据丢失。Source上的数据可以复制到不同的通道上。每一个Channel也可以连接不同数量的Sink。这样连接不同配置的Agent就可以组成一个复杂的数据收集网络。通过对agent的配置，可以组成一个路由复杂的数据传输网络。配置如上图所示的agent结构，Flume支持设置sink的Failover和Load Balance，这样就可以保证即使有一个agent失效的情况下，整个系统仍能正常收集数据。Flume中传输的内容定义为事件（Event），事件由Headers（包含元数据，Meta Data）和Payload组成。Flume提供SDK，可以支持用户定制开发：Flume客户端负责在事件产生的源头把事件发送给Flume的Agent。客户端通常和产生数据源的应用在同一个进程空间。常见的Flume客户端有Avro，log4J，syslog和HTTP Post。另外ExecSource支持指定一个本地进程的输出作为Flume的输入。当然很有可能，以上的这些客户端都不能满足需求，用户可以定制的客户端，和已有的FLume的Source进行通信，或者定制实现一种新的Source类型。同时，用户可以使用Flume的SDK定制Source和Sink。似乎不支持定制的Channel。FluentdFluentd （Github 地址）是另一个开源的数据收集框架。Fluentd使用C/Ruby开发，使用JSON文件来统一日志数据。它的可插拔架构，支持各种不同种类和格式的数据源和数据输出。最后它也同时提供了高可靠和很好的扩展性。Treasure Data, Inc对该产品提供支持和维护。Fluentd的部署和Flume非常相似：Fluentd的架构设计和Flume如出一辙：Fluentd的Input／Buffer／Output非常类似于Flume的Source／Channel／Sink。InputInput负责接收数据或者主动抓取数据。支持syslog，http，file tail等。BufferBuffer负责数据获取的性能和可靠性，也有文件或内存等不同类型的Buffer可以配置。OutputOutput负责输出数据到目的地例如文件，AWS S3或者其它的Fluentd。Fluentd的配置非常方便，如下图：Fluentd的技术栈如下图：FLuentd和其插件都是由Ruby开发，MessgaePack提供了JSON的序列化和异步的并行通信RPC机制。Cool.io是基于libev的事件驱动框架。FLuentd的扩展性非常好，客户可以自己定制（Ruby）Input／Buffer／Output。Fluentd从各方面看都很像Flume，区别是使用Ruby开发，Footprint会小一些，但是也带来了跨平台的问题，并不能支持Windows平台。另外采用JSON统一数据／日志格式是它的另一个特点。相对去Flumed，配置也相对简单一些。LogstashLogstash是著名的开源数据栈ELK（ElasticSearch，Logstash，Kibana）中的那个L。Logstash用JRuby开发，所有运行时依赖JVM。Logstash的部署架构如下图，当然这只是一种部署的选项。一个典型的Logstash的配置如下，包括了Input，filter的Output的设置。input&nbsp;{&nbsp;&nbsp;file&nbsp;{&nbsp;&nbsp;&nbsp;&nbsp;type&nbsp;=&gt;&nbsp;"apache-access"&nbsp;&nbsp;&nbsp;&nbsp;path&nbsp;=&gt;&nbsp;"/var/log/apache2/other_vhosts_access.log"&nbsp;&nbsp;}&nbsp;&nbsp;file&nbsp;{&nbsp;&nbsp;&nbsp;&nbsp;type&nbsp;=&gt;&nbsp;"apache-error"&nbsp;&nbsp;&nbsp;&nbsp;path&nbsp;=&gt;&nbsp;"/var/log/apache2/error.log"&nbsp;&nbsp;}}filter&nbsp;{&nbsp;&nbsp;grok&nbsp;{&nbsp;&nbsp;&nbsp;&nbsp;match&nbsp;=&gt;&nbsp;{&nbsp;"message"&nbsp;=&gt;&nbsp;"%{COMBINEDAPACHELOG}"&nbsp;}&nbsp;&nbsp;}&nbsp;&nbsp;date&nbsp;{&nbsp;&nbsp;match&nbsp;=&gt;&nbsp;[&nbsp;"timestamp"&nbsp;,&nbsp;"dd/MMM/yyyy:HH:mm:ss&nbsp;Z"&nbsp;]&nbsp;&nbsp;}}output&nbsp;{&nbsp;&nbsp;stdout&nbsp;{&nbsp;}&nbsp;&nbsp;redis&nbsp;{&nbsp;&nbsp;&nbsp;&nbsp;host&nbsp;=&gt;&nbsp;"192.168.1.200"&nbsp;&nbsp;&nbsp;&nbsp;data_type&nbsp;=&gt;&nbsp;"list"&nbsp;&nbsp;&nbsp;&nbsp;key&nbsp;=&gt;&nbsp;"logstash"&nbsp;&nbsp;}}几乎在大部分的情况下ELK作为一个栈是被同时使用的。所有当你的数据系统使用ElasticSearch的情况下，logstash是首选。ChukwaApache Chukwa&nbsp;（github）是apache旗下另一个开源的数据收集平台，它远没有其他几个有名。Chukwa基于Hadoop的HDFS和Map Reduce来构建（显而易见，它用Java来实现），提供扩展性和可靠性。Chukwa同时提供对数据的展示，分析和监视。很奇怪的是它的上一次github的更新事7年前。可见该项目应该已经不活跃了。Chukwa的部署架构如下。Chukwa的主要单元有：Agent，Collector，DataSink，ArchiveBuilder，Demux等等，看上去相当复杂。由于该项目已经不活跃，我们就不细看了。ScribeScribe是Facebook开发的数据（日志）收集系统。已经多年不维护，同样的，就不多说了。Splunk Forwarder以上的所有系统都是开源的，在商业化的大数据平台产品中，Splunk提供完整的数据采金，数据存储，数据分析和处理，以及数据展现的能力。Splunk是一个分布式的机器数据平台，主要有三个角色：Search Head负责数据的搜索和处理，提供搜索时的信息抽取。Indexer负责数据的存储和索引Forwarder，负责数据的收集，清洗，变形，并发送给IndexerSplunk内置了对Syslog，TCP/UDP，Spooling的支持，同时，用户可以通过开发Script Input和Modular Input的方式来获取特定的数据。在Splunk提供的软件仓库里有很多成熟的数据采集应用，例如AWS，数据库（DBConnect）等等，可以方便的从云或者是数据库中获取数据进入Splunk的数据平台做分析。这里要注意的是，Search Head和Indexer都支持Cluster的配置，也就是高可用，高扩展的，但是Splunk现在还没有针对Farwarder的Cluster的功能。也就是说如果有一台Farwarder的机器出了故障，数据收集也会随之中断，并不能把正在运行的数据采集任务Failover到其它的Farwarder上。总结我们简单讨论了几种流行的数据收集平台，它们大都提供高可靠和高扩展的数据收集。大多平台都抽象出了输入，输出和中间的缓冲的架构。利用分布是的网络连接，大多数平台都能实现一定程度的扩展性和高可靠性。其中Flume，Fluentd是两个被使用较多的产品。如果你用ElasticSearch，Logstash也许是首选，因为ELK栈提供了很好的集成。Chukwa和Scribe由于项目的不活跃，不推荐使用。Splunk作为一个优秀的商业产品，它的数据采集还存在一定的限制，相信Splunk很快会开发出更好的数据收集的解决方案。</div>
                            <div class='commentList'>
                    <h3>评论列表</h3>
                                    </div>
                    </div>
    
        <div class='blog'>
            <h2><a href="#top" class='top'>回到顶部</a><a name="blog_630382" href="https://my.oschina.net/taogang/blog/630382">在云上的机器学习 </a></h2>
            <div class='outline'>
                <div class='date'>时间：2016-03-03 15:07:55</div>
                <div class='catalog'>分类：机器学习</div>
                                                                            </div>
            <div class='content'>随着大数据日新月异的飞速发展，机器学习也变的越来越性感。云和大数据是天生的一对，那么云上的机器学习又是什么样呢？我们今天就来看看几个基于云的机器学习平台：亚马逊，微软和bigml亚马逊机器学习我们先来看看云的领军人物亚马逊的机器学习平台 Amazon Machine Learning首先，要是用亚马逊的机器学习，你需要有一个AWS的账号（废话）。在Analytics服务区域你会找到他的位置：Machine Learning是AWS的一个相对比较新的功能，所以当前只有EU（爱尔兰）和美西（弗吉尼亚）两个domain支持。选择Launch启动机器学习，首先需要一个数据源。创建一个数据源分几步： 输入数据 input data输入的数据源可以来自：用户可以使用S3上的一个csv文件或者来自Amazon Redshift数据库这里，我在我的S3上放了一个Iris的dataset，Verify后，数据就准备好了。⚠ Iris Dataset 可能是最著名的数据集 （参见wikipedia的介绍），该数据集包含了三种鸢尾属植物（什么鬼？）的50个样本，该数据包涵植物的花瓣的萼片的长度和宽度的统计。 数据模式 schema在schema这一步，选择CSV包含name column， （Does the first line in your CSV contain the column names? &nbsp; -&gt; Yes） 结果如下： 目标 Target在Target这一步，选择一个要预测的目标。如果选择了Species，AWS会选择创建一个分类模型 （multinomial logistic regression）如果选择了长度或者宽度属性，AWS会选择创建一个回归模型 （线性回归） Rowid这一步用户可以选择一个rowid reviewreview所有的数据源选项后，数据源就创建成功了数据源创建成功后，可以对模型做一些设置。这里可以选择缺省的设置。设置好模型后，AWS会对该模型进行评估（evaluation），需要等一段时间看到结果。我们不需要等待评估的结果，直接来进行预测。为了省事，我们直接使用原始的Iris数据集进行预测（本文的目的只是为了感受在云上的机器学习，并非解决实际问题）选择Batch Prediction选择我们刚刚创建的模型：选择Iris数据集来进行预测选择一个S3bucket的路径作为预测结果的存放，注意，确保机器学习的服务有对该目录的写权限。注意，模型的创建是不收费的，但是预测是收费，这里AWS提示每1000个预测收费0.1美元。review后没有问题，就开始预测了。在Dashboard里面可以查看预测的状态，如果是completed表示预测完成。我们去S3里面看看预测的结果如何。在S3里面我们看到我们之前指定的Bucket里面多了一个batch-prediction的目录，该目录的result目录下就是预测的结果。我们下载下来看看：预测的结果是一个文本格式的CSV文件，包含了对每一种结果的预测概率。例如对于第一行的数据，实际是一个setosa，而模型预测是setosa的概率是99.99%。 好像还不错！好了，现在我们经历了AWS 机器学习 创建数据源 －》 建立模型 －》 对指定89数据集做预测的过程，貌似还不错。微软Azure机器学习平台如果说AWS是云行业的number one的话，那么微软的Azure的第二名的位子应该也是稳定的很。当然，微软可不甘心屈居第二，至少在机器学习上，Azure让我们眼前一亮。（ Microsoft Azure Machine Learning ）我们试试在Azure上完成同样的测试吧。首先，你并不需要有一个Azure的账号，可以使用微软账号或者匿名试用，真是太贴心了。登陆后的界面是这个样子，还好了，不算好看，也不算难看。Azure ML的主要功能菜单有： Experiments （实验） Web Service （WEB服务） NOTEBOOK （IPython Notebook， 这个功能很赞） Datasets （数据集） Trained Models （已训练的模型） SETTING （设置）我们来试验一下同样的对Iris数据集的分类预测，是如何在Azure平台上进行的。首先还是创建数据源，Azure提供四种数据源，上传本地文件，来自另一个模块（MODULE），来自一个实验（EXPERIMENT）或者一个Notebook。我们还是上传本地的Iris的CSV文件，上传好后，我们选择一个实验。选择实验（Experiment）菜单，该功能页面有两个Tab页，我的实验和样例，这里微软很贴心的准备了许多的例子供你参考。我们选择创建一个空的实验你会看到以下的界面：这里Azure使用了基于流程图的编程（Flow Based Programming）概念来创建一个实验。可以把机器学习的过程，直观的用流程图的表示和创建，非常的方便。为了做一个基于Iris数据集的预测模型，我们需要创建如下的一个流程。注意，在连接数据集到Train Model之后，需要在右侧的UI中选择一个要作为预测目标的列，launch column selector。在这个例子中，我们还是选择Species作为预测目标。模型建立好后可以保存下来供以后使用。下面我们开始做预测，我找了半天才找到做预测的模块。名字叫Score Model加入预测功能后的流程路如下：点击下方的运行按钮就可以开始预测的计算了。预测完成后，右键点击Score Model模块，可以直接以可视化的形式查看结果数据集 （点击Visualize），这个功能非常方便。预测的可视化结果如下图：预测的分类结果是Scored Labels。同时用户还可以方便的基于该预测结果的数据集生成不同的图表：Azure的Machine Learning 平台提供了大量的模块功能，用户可以用Flow的形式把这些模块连接成一个计算网络，实现复杂的机器学习的功能。除了机器学习，这里列出其中的一些其它的功能： R／Python 可以直接调用Python和R的模块 图像处理 Open CV 文本处理&nbsp; 统计功能 数据变形 Data Transform Web Service APIAzure的机器学习平台提供非常丰富的机器学习的算法，并且支持R和Python的调用，非常适合数据科学家和程序员使用。而且，免费账户的功能也非常的powerful。堪称业内良心。BigMLBigML 是提供基于云的机器学习平台， 目标是让机器学习更容易使用。同样的我们试用一下如何在BigML上对Iris数据集做预测。登陆后，先进入Dashboard创建一个数据源，Sources，选择上传CSV文件，上传好了之后，点击数据源，显示如下：点击Configure Dataset，将数据源转换成Dataset下一步创建模型，BigML提供了很方便的模型创建功能：我们选择创建一个模型BigML会自动选择Species作为预测目标。点击创建模型，注意创建之前，我们需要deselect rowid，因为rowid与预测无关。创建好的模型是一个Tree结构分类模型，可以看到每一条预测路径的信息：下面我们就可以用这个模型来做预测了：点击Predict菜单项，看到如下的预测界面。这里很奇怪的是，我明明有四个变量，Tree Model里也可以看到，为什么预测的时候，只能使用其中三个？好了，不关注这些细节，用鼠标滑动改变每一个变量的值，就可以直观的看到预测的结果。这个界面我给100分！使用下来，BigML确实比较容易使用，用户体验也不错，值得拥有。其它除了这几家之外，还有其他的一些厂商也提供了在云上的机器学习，包括Databricks，Google，百度，IBMDatabricksDatabricks这个名字很多人可能不太清楚，然而如果我说Spark，相信大家都是如雷贯耳，2015年大数据领域最火的技术就是Spark了，没有之一。Databricks是Spark开发者创办的公司，提供一个基于Spark的数据云平台，因为使用需要信用卡绑定，笔者还没有对该产品进行评估。Google Prediction APIGoogl的Prediction API可能是最早的在提供的云上的机器学习功能了。它提供了一些场景下的预测功能，包括情感分析，垃圾邮件检测，推荐系统等等。不过和其他云平台比较起来，总觉得google的云用着不那么爽。试用账户还要绑定信用卡，我也没有对该产品进行评估。百度预测百度基本上是Google有什么都要来抄一个的，所以如果存在一个Baidu Prediction API，我相信大家不会感觉奇怪。果然，百度有一个百度预测。不过百度预测并不是一个通用的机器学习的平台，只能做特定领域的，特定数据集的预测。其中百度的预测开放平台可以对时间序列的数据进行预测，例如股票（发财了）。阿里云数加 机器学习阿里在去年10月高调发布他的数加平台，掌声不断。阿里云的机器学习正处于公测阶段。https://data.aliyun.com/product/learn?spm=a2c0j.7906235.devenv.2.7rWRRt 大家有兴趣也可以去试用。阅读了他的快速开始文档后，我的内心久久的不能平静，不抄就不会做产品了么。和Azure ML studio 相似度 76.35%。阿里机器学习的项目名称也叫实验，太专业了，完全保留了Azure的命名风格！腾讯云腾讯作为国内体量最大的互联网公司，在云这一块自然不能落后。腾讯云也有自己的机器学习 TML。这里是他的帮助文档同样采用了流程图的方式，开来也许我们错怪了阿里，用流程图来建模是机器学习的标准，微软只是运气好，帅先实现了该标准而已。IBM BluemixIBM近年来在云上的投入非常大，新的Bluemix也提供了非常多的功能呢，其中Watson模块提供了一部分机器学习的功能，但这些功能更多是实现一个更接近用户的功能，比较多的是自然语言处理相关的功能。除了以上这些厂商，还有其他的几个玩家，我就不一一介绍了，有兴趣的可以自己去看： http://www.algorithms.io/&nbsp; http://www.ersatzlabs.com/&nbsp; http://www.nutonian.com/&nbsp; 比较和总结这里我们对几个主要的通用机器学习平台最一个比较和总结：         AWS Machine Learning   Azure Machine Learning   BigML       数据源格式   CSV， Redshift   CSV   CSV，Gziped CSV       数据源存储   S3   upload from local   S3，Google Storage， Google Drive， Azure， Drop Box       数据准备和变形   使用Data Recipes进行数据变形   使用特定的Transformation模块或者Python／R编程来进行数据变形   只支持简单的数据变形，例如增加字段，过滤等。       特征提取   没有   支持   没有       机器学习算法   提供基本的分类，回归算法，用户可控部分较少   有非常丰富的算法涵盖分类，回归，聚类，异常检测。   提供基本的分类，聚类，异常检测的功能，用户可选择的部分较少       其它功能      图像处理，统计，文本分析          编程扩展   不支持   Python／R   不支持       可视化功能   基本没有   可以灵活的对计算过程中的数据进行可视化的展现和分析   以图（大部分是树行图）的形式展现模型       API／SDK   Rest API和不同语言的绑定   Web Service   Rest API以及各种主要语言的绑定（Python，Java，Nodejs，etc）       价格   基于预测的次数收费，0.1$／1000次预测   免费用户有10G的是试用空间，真心赞。 标准版每月10$   免费用户有16M空间   除了这几个通用的机器学习的平台，另外几个云上的机器学习也各有特点。Google Prediction API，百度预测，IBM Watson几个产品也都提供了特定场景下的的机器学习或者预测的功能，如果有这一类的需求，也可以使用。对于Databricks以为没有试用，了解不多，希望以后能有机会试用。如果你是一个程序员或者数据科学家，我觉得Azure的Machine Learning Studio是你不二的选择。他提供了大量的基础算法，并且支持R/Pyhon的扩展。如果你对机器学习了解不多，希望快速的对已有的数据进行预测，我推荐AWS或者BigML。如果你是有特定场景的预测要求，可以考虑Google Prediction API，百度预测或者IBM的Watson。相关链接http://www.infoworld.com/article/3039052/cloud-computing/how-ibm-google-microsoft-and-amazon-do-machine-learning-in-the-cloud.html&nbsp;http://www.zdnet.com/article/cloud-machine-learning-wars-heat-up/&nbsp;</div>
                    </div>
    
        <div class='blog'>
            <h2><a href="#top" class='top'>回到顶部</a><a name="blog_630632" href="https://my.oschina.net/taogang/blog/630632">使用开源软件快速搭建数据分析平台</a></h2>
            <div class='outline'>
                <div class='date'>时间：2016-03-03 22:38:38</div>
                <div class='catalog'>分类：架构设计</div>
                                                                            </div>
            <div class='content'>最近，国内涌现出了不少数据分析平台产品，例如魔镜和数据观。 这些产品的目标应该都是self service的BI，利用可视化提供数据探索的功能，并且加入机器学习和预测的功能。它们对标的产品应该是Tableau或者SAP Lumira。因为笔者曾经为Lumira开发数据可视化的功能，对这一块很感兴趣，于是就试用了一下这些产品，感觉这些产品似乎还有很大的差距，于是就想自己用开源软件搭一个简单的数据分析平台试试看。 代码在这里&nbsp;https://github.com/gangtao/dataplay2&nbsp; 废话少说，上架构图：  列一下主要用到的开源软件： 服务器端：    flask&nbsp;http://flask.pocoo.org/&nbsp; 轻量级的Python Web框架    pandas&nbsp;http://pandas.pydata.org/&nbsp; Python的数据结构和数据分析工具包，提供数据处理的Wrangling的功能    sklearn&nbsp;http://scikit-learn.org/&nbsp; 非常流行的Python机器学习包，依赖于numpy，scipy和matplotlib   客户端:    jquery 这个就不用介绍了    reactjs&nbsp;http://facebook.github.io/react/&nbsp; facebook开发的js UI框架，基于组件（component）而非mvc    d3js&nbsp;https://d3js.org/&nbsp; 数据驱动的DOM操纵库，可以创建丰富的数据可视化呈现。    echarts&nbsp;http://www.oschina.net/p/echarts&nbsp; 百度开发的数据可视化库，基于canvas技术，功能丰富。实为中国开源项目的翘楚。    bootstrap&nbsp;http://getbootstrap.com/&nbsp; twitter开发的前端框架，非常流行。    jquery datatables &nbsp;http://www.datatables.net/&nbsp; 非常实用的基于jquery的表格控件    bootstrap fielinput&nbsp;https://github.com/kartik-v/bootstrap-fileinput&nbsp; HTML5文件上传控件    papaparse&nbsp;https://github.com/mholt/PapaParse&nbsp; CSV文件的JS解析    requirejs&nbsp;http://www.requirejs.org/&nbsp; JS 依赖管理    select2 &nbsp;https://select2.github.io/&nbsp; 基于jquery的select控件   开发构建工具    nodejs&nbsp;https://nodejs.org/en/&nbsp; 这个应该也不用介绍    babel&nbsp;https://babeljs.io/&nbsp; javascript的编译器，支持把ES6的代码转换成浏览器可执行的代码，这里主要是为了支持reactjs使用的jsx的编译。   好了，罗列了这么多的开源软件后，我们看看dataplay2的功能，然后看看这些开源软件起到的作用和我为什么要选择它们的原因。 在介入正题之前，我们先聊聊dataplay2这个名字，dataplay很容易理解，我希望创建一个简单易用的数据平台，使用起来像玩一样的愉快。但为什么是2呢？因为这个软件很二么？当然不是。其实我之前写过一个dataplay的，当时的架构略有不同，为了使用R里的ggplot来支持语法驱动的可视化方案，我后台使用了R/Python的桥接方案，前台的可视化操作会生成ggplot的命令，好处是可以有一个统一的数据模型和语法来驱动数据的可视化分析，便于用户进行数据的探索。然而这样的架构太复杂了，服务器端既有R又有Python，我自己都看不下去了，后来就放弃了。新的dataplay2使用echart的图表库来做可视化，优缺点我们后面再聊。 好了，运行dataplay2非常简单，下载github上的code后，建议安装anaconda，所有的Python依赖就都准备好了，进入dataplay2/package目录，运行： python&nbsp;main.py 这里补充说明一下，因为react的jsf需要编译，需要运行如下的命令用babel进行jsf的编译才能运行，具体命令如下： ##&nbsp;install&nbsp;node&nbsp;first##&nbsp;cd&nbsp;package/staticnpm&nbsp;install&nbsp;-g&nbsp;babel-clinpm&nbsp;install&nbsp;babel-preset-es2015&nbsp;--savenpm&nbsp;install&nbsp;babel-preset-react&nbsp;--savebabel&nbsp;--presets&nbsp;es2015,react&nbsp;--watch&nbsp;js/&nbsp;--out-dir&nbsp;lib/ 另外还需要使用bower安装客户端的所有依赖 ##&nbsp;install&nbsp;bower&nbsp;first##&nbsp;cd&nbsp;package/staticbower&nbsp;install 大家也可以参考package/static/package.json了解需要的依赖。有时间需要集成一个更简单的build脚本来做这些事情。生成的JS文件在lib目录下。修改js目录下的原始文件，babel会触发编译，生成新的js文件在lib目录下。 然后在浏览器中键入 localhost:5000启动客户端。  首先我们进入数据菜单  在这个页面，用户可以浏览已有的数据，或者上传一个CSV文件，增加一个数据集。  简单介绍一下这一部分的实现。 数据上传用到了file input控件，数据表用了datatable控件。为了方便CSV文件直接存贮在本地文件系统中。后台用pandas对csv文件进行处理。前台用Rest API读取csv文件，然后用papaparse解析后，展现在数据表中。这样做纯粹是为了方便，因为整个POC是我在假期花了3/4天做的，所以怎么方便怎么来。更好的做法是在后台用Python对CSV文件作解析。 注意这里我们对上传的CSV文件有严格的要求，必须有首行的header，末尾不能有空行。 有了数据后，就可以开始做分析了。首先我们看看可视化的分析。点击菜单Analysis／Visualization  例如我们选定Iris数据源做一个Scatter Plot  可视化这一块的主要工作是从CSV的表结构数据，根据数据绑定，变形到echart的数据结构。因为echart并没有一个统一的数据模型，所以每一个类型的图表都需要有对应的数据变形的逻辑 。（代码&nbsp;package/static/js/visualization ） 现在主要的做了Pie，Bar，Line，Treemap，Scatter， Area这几种chart。 现在用下来感觉echart优缺点都很明显，他提供的辅助功能很好，可以方便的增加辅助线，note，存贮为图形等。但是由于缺乏统一的数据模型扩展起来比较麻烦，我希望有时间试用一下plotly，当然highchart是非常成熟的图表库，无需证明。 其实我希望能找到一个ggplot的D3的实现，例如这个http://benjh33.github.io/ggd3/&nbsp;，可惜该项目似乎不活跃了。 除了基于可视化的分析功能，还有机器学习的功能。 分类 分类的算法可以使用KNN，Bayes和SVM。  如果选择两个Feature做预测，我用D3画出了该预测的模型。大于两个时，就没有办法画出来了。 然后用户可以选择基于该模型来做预测。  聚类和回归的功能和分类基本一致。 聚类 聚类算法现在实现了Kmeans  线性回归  逻辑回归  &nbsp; 基本功能就这些了，这里列出一些我想要实现的功能：    数据源 现在的数据源只有CSV文件，可以考虑更多的数据源支持，例如数据库／数据仓库，REST调用，流等等。    数据模型 现在的数据模型比较简单，就是pandas的dataframe或者一个简单的cvs的表结构。可以考虑引入数据库。另外还需要增加对层级数据（hierachical）的支持    数据变形 数据变形是数据分析的必要准备工作。业内有很多专注于数据准备的产品，例如paxata,trifacta 这个版本的dataplay没有任何的数据变形和准备的功能，其实pandas有非常丰富的data wrangling的功能，我希望能在这之上包装一个data wrangling的DSL，可以让用户快速的进行数据准备。    可视化库 Baidu的echart是非常优秀的可视化库，可是用于数据探索时，还不够好。希望能有一套类似ggplot的前端可视化库来使用。另外地图功能和层级化的图表也是数据分析常见的功能。 还需要加入图表的选项    仪表盘功能 这个版本的dataplay没有仪表盘功能，这个功能是数据分析软件的标配，必须有。pyxley似乎是个不错的选择，也和dataplay的架构一致（python，reactjs），有时间可以尝试一下    机器学习和预测 dataplay现在实现了最简单的一些机器学习的算法，我觉得方向应该是面向用户，变得更简单，用户只给出简单的选项，例如要预测的目标属性，和用于预测的属性，然后自动的选择算法。另外需要更方便的对算法进行扩展。   好了，最后谈谈简单的感受    reactjs真不错，一直不喜欢MVC，reactjs的组件化用起来更舒服，而且开发效率确实高，整个项目我用假期3/4天完成，react功不可没。    dataplay现在的功能还比较弱，但是基本的架构已经搭好了，大家喜欢的话可以拿去扩展。我不一定会有时间继续对它的功能增强，但是欢迎大家和我一起讨论。   更新： 因为很多同学反映不能正常运行，我制作了一个Dockerfile，大家可以参考&nbsp;https://github.com/gangtao/dataplay2/tree/master/docker 来构建。希望可以解决大家不能运行的问题。&nbsp; Image 已经发布到 docker hub 了 ：https://hub.docker.com/r/naughtytao/dataplay/&nbsp;</div>
                            <div class='commentList'>
                    <h3>评论列表</h3>
                                    </div>
                    </div>
    
        <div class='blog'>
            <h2><a href="#top" class='top'>回到顶部</a><a name="blog_652287" href="https://my.oschina.net/taogang/blog/652287">Spark Python 快速体验</a></h2>
            <div class='outline'>
                <div class='date'>时间：2016-04-04 14:23:18</div>
                <div class='catalog'>分类：编程语言</div>
                                                                            </div>
            <div class='content'>Spark是2015年最受热捧大数据开源平台，我们花一点时间来快速体验一下Spark。Spark 技术栈如上图所示，Spark的技术栈包括了这些模块：核心模块 ：Spark Core集群管理&nbsp;Standalone SchedulerYARNMesosSpark SQLSpark 流 StreamingSpark 机器学习 MLLibGraphX 图处理模块安装和启动SparkSpark Python Shell&gt;&nbsp;bin/pysparkSpark Ipython Shell&gt;&nbsp;IPYTHON=1&nbsp;./bin/pyspark&gt;&nbsp;IPYTHON_OPTS="notebook"&nbsp;./bin/pysparkSpark 架构初始化 Spark Context在使用Spark的功能之前首先要初始化Spark的context，Context包含了Spark的连接和配置信息。Spark Context，Driver和Worker节点之间的关系如下图：from&nbsp;pyspark&nbsp;import&nbsp;SparkConf,&nbsp;SparkContextconf&nbsp;=&nbsp;SparkConf().setMaster("local").setAppName("My&nbsp;App")sc&nbsp;=&nbsp;SparkContext(conf&nbsp;=&nbsp;conf)创建 RDDRDD是Spark的基本数据模型，所有的操作都是基于RDD。RDD是inmutable（不可改变）的。lines&nbsp;=&nbsp;sc.textFile("README.md")pythonLines&nbsp;=&nbsp;lines.filter(lambda&nbsp;line:&nbsp;"Python"&nbsp;in&nbsp;line)&nbsp;pythonLines.first()pythonLines.count()RDD 操作:下面是一些对RDD的变形操作RDD Transformation on {1,2,3,4}两个RDD之间的操作， Transformation on {1,2,3} and {3,4,5}RDD actions on {1,2,3,3}Transformation on pair RDD {(1,2),(3,4),(3,6)}Transform on two pair RDDs {(1,2),(3,4),(3,6)}, {(3,9)}Spark 流 streamSpark流基于RDD，可以理解对小的时间片段上的RDD操作。SparkSQLSpark SQL可以用于操作和查询结构化和半结构化的数据。包括Hive，JSON， CSV等。#&nbsp;Import&nbsp;Spark&nbsp;SQLfrom&nbsp;pyspark.sql&nbsp;import&nbsp;HiveContext,&nbsp;Row#&nbsp;Or&nbsp;if&nbsp;you&nbsp;can't&nbsp;include&nbsp;the&nbsp;hive&nbsp;requirementsfrom&nbsp;pyspark.sql&nbsp;import&nbsp;SQLContext,&nbsp;Row&nbsp;input&nbsp;=&nbsp;hiveCtx.jsonFile(inputFile)#&nbsp;Register&nbsp;the&nbsp;input&nbsp;schema&nbsp;RDDinput.registerTempTable("tweets")#&nbsp;Select&nbsp;tweets&nbsp;based&nbsp;on&nbsp;the&nbsp;retweetCounttopTweets&nbsp;=&nbsp;hiveCtx.sql("SELECT&nbsp;text,&nbsp;retweetCount&nbsp;FROM&nbsp;tweets&nbsp;ORDER&nbsp;BY&nbsp;retweetCount&nbsp;LIMIT&nbsp;10")Spark SQL支持JDBCSparkML&nbsp;机器学习的基本流程如下：获得数据从数据中提取特征对数据进行有监督的或者无监督的学习，训练机器学习的模型对模型进行评估，找出最佳模型由于Spark的架构特点，Spark支持的机器学习算法是哪些可以并行的算法。from&nbsp;pyspark.mllib.regression&nbsp;import&nbsp;LabeledPointfrom&nbsp;pyspark.mllib.feature&nbsp;import&nbsp;HashingTFfrom&nbsp;pyspark.mllib.classification&nbsp;import&nbsp;LogisticRegressionWithSGDspam&nbsp;=&nbsp;sc.textFile("spam.txt")normal&nbsp;=&nbsp;sc.textFile("normal.txt")#&nbsp;Create&nbsp;a&nbsp;HashingTF&nbsp;instance&nbsp;to&nbsp;map&nbsp;email&nbsp;text&nbsp;to&nbsp;vectors&nbsp;of&nbsp;10,000&nbsp;features.tf&nbsp;=&nbsp;HashingTF(numFeatures&nbsp;=&nbsp;10000)#&nbsp;Each&nbsp;email&nbsp;is&nbsp;split&nbsp;into&nbsp;words,&nbsp;and&nbsp;each&nbsp;word&nbsp;is&nbsp;mapped&nbsp;to&nbsp;one&nbsp;feature.spamFeatures&nbsp;=&nbsp;spam.map(lambda&nbsp;email:&nbsp;tf.transform(email.split("&nbsp;")))normalFeatures&nbsp;=&nbsp;normal.map(lambda&nbsp;email:&nbsp;tf.transform(email.split("&nbsp;")))#&nbsp;Create&nbsp;LabeledPoint&nbsp;datasets&nbsp;for&nbsp;positive&nbsp;(spam)&nbsp;and&nbsp;negative&nbsp;(normal)&nbsp;examples.positiveExamples&nbsp;=&nbsp;spamFeatures.map(lambda&nbsp;features:&nbsp;LabeledPoint(1,&nbsp;features))negativeExamples&nbsp;=&nbsp;normalFeatures.map(lambda&nbsp;features:&nbsp;LabeledPoint(0,&nbsp;features))trainingData&nbsp;=&nbsp;positiveExamples.union(negativeExamples)trainingData.cache()&nbsp;#&nbsp;Cache&nbsp;since&nbsp;Logistic&nbsp;Regression&nbsp;is&nbsp;an&nbsp;iterative&nbsp;algorithm.#&nbsp;Run&nbsp;Logistic&nbsp;Regression&nbsp;using&nbsp;the&nbsp;SGD&nbsp;algorithm.model&nbsp;=&nbsp;LogisticRegressionWithSGD.train(trainingData)#&nbsp;Test&nbsp;on&nbsp;a&nbsp;positive&nbsp;example&nbsp;(spam)&nbsp;and&nbsp;a&nbsp;negative&nbsp;one&nbsp;(normal).&nbsp;We&nbsp;first&nbsp;apply#&nbsp;the&nbsp;same&nbsp;HashingTF&nbsp;feature&nbsp;transformation&nbsp;to&nbsp;get&nbsp;vectors,&nbsp;then&nbsp;apply&nbsp;the&nbsp;model.posTest&nbsp;=&nbsp;tf.transform("O&nbsp;M&nbsp;G&nbsp;GET&nbsp;cheap&nbsp;stuff&nbsp;by&nbsp;sending&nbsp;money&nbsp;to&nbsp;...".split("&nbsp;"))negTest&nbsp;=&nbsp;tf.transform("Hi&nbsp;Dad,&nbsp;I&nbsp;started&nbsp;studying&nbsp;Spark&nbsp;the&nbsp;other&nbsp;...".split("&nbsp;"))print&nbsp;"Prediction&nbsp;for&nbsp;positive&nbsp;test&nbsp;example:&nbsp;%g"&nbsp;%&nbsp;model.predict(posTest)print&nbsp;"Prediction&nbsp;for&nbsp;negative&nbsp;test&nbsp;example:&nbsp;%g"&nbsp;%&nbsp;model.predict(negTest)&nbsp;</div>
                    </div>
    
        <div class='blog'>
            <h2><a href="#top" class='top'>回到顶部</a><a name="blog_652501" href="https://my.oschina.net/taogang/blog/652501">100 open source Big Data architecture papers</a></h2>
            <div class='outline'>
                <div class='date'>时间：2016-04-05 09:36:56</div>
                <div class='catalog'>分类：转贴的文章</div>
                                                                    <div class='url'>https://www.linkedin.com/pulse/100-open-source-big-data-architecture-papers-anil-madan?utm_source...</div>                            </div>
            <div class='content'>Big Data technology has been extremely disruptive with open source playing a dominant role in shaping its evolution. While on one hand it has been disruptive, on the other it has led to a complex ecosystem where new frameworks, libraries and tools are being released pretty much every day, creating confusion as technologists struggle and grapple with the deluge.If you are a Big Data enthusiast or a technologist ramping up (or scratching your head), it is important to spend some serious time deeply understanding the architecture of key systems to appreciate its evolution. Understanding the architectural components and subtleties would also help you choose and apply the appropriate technology for your use case. In my journey over the last few years, some literature has helped me become a better educated data professional. My goal here is to not only share the literature but consequently also use the opportunity to put some sanity into the labyrinth of open source systems.One caution, most of the reference literature included is hugely skewed towards deep architecture overview (in most cases original research papers) than simply provide you with basic overview. I firmly believe that deep dive will&nbsp;fundamentally help you understand the nuances, though would not provide you with any shortcuts, if you want to get a quick basic overview.Jumping right in…Key architecture layersFile Systems&nbsp;- Distributed file systems which provide storage, fault tolerance, scalability, reliability, and availability.Data Stores&nbsp;– Evolution of application databases into&nbsp;&nbsp;Polyglot&nbsp;&nbsp;storage with application specific databases instead of one size fits all. Common ones are Key-Value, Document, Column and Graph.Resource Managers&nbsp;– provide resource management capabilities and support schedulers for high utilization and throughput.Coordination&nbsp;– systems that manage state, distributed coordination, consensus and lock management.Computational Frameworks&nbsp;– a lot of work is happening at this layer with highly specialized compute frameworks for Streaming, Interactive, Real Time, Batch and Iterative Graph (BSP) processing. Powering these are complete computation runtimes like&nbsp;BDAS&nbsp;&nbsp;(Spark) &amp; Flink.Data&nbsp;Analytics&nbsp;&nbsp;–Analytical (consumption) tools and libraries, which support exploratory, descriptive, predictive, statistical analysis and machine learning.Data Integration&nbsp;– these include not only the orchestration tools for managing pipelines but also metadata management.Operational Frameworks&nbsp;– these provide scalable frameworks for monitoring &amp; benchmarking.Architecture EvolutionThe modern data architecture is evolving with a goal of reduced&nbsp;latency between data producers&nbsp;and consumers. This consequently is leading to real time and low latency processing, bridging the traditional batch and interactive layers into hybrid architectures like Lambda and Kappa.Lambda&nbsp;- Established architecture for a typical data pipeline.&nbsp;Mor&nbsp;e&nbsp;details.Kappa&nbsp;– An alternative architecture which moves the processing upstream to the Stream layer.SummingBird&nbsp;– a reference model on bridging the online and traditional processing models.&nbsp;Before you deep dive into the actual layers, here are some general documents which can provide you a great background on NoSQL, Data Warehouse Scale Computing and Distributed Systems.Data center as a computer&nbsp;– provides a great background on warehouse scale computing.NOSQL Data Stores&nbsp;– background on a diverse set of key-value, document and column oriented stores.NoSQL Thesis&nbsp;– great background on distributed systems, first generation NoSQL systems.Large Scale Data Management&nbsp;- covers the data model, the system architecture and the consistency model, ranging from traditional database vendors to new emerging internet-based enterprises.&nbsp;Eventual Consistency&nbsp;– background on the different consistency models for distributed systems.CAP Theorem&nbsp;– a nice background on CAP and its evolution.There also has been in the past a fierce debate between traditional Parallel DBMS with Map Reduce paradigm of processing.&nbsp;Pro&nbsp;parallel DBMS&nbsp;(&nbsp;another&nbsp;) paper(s) was rebutted by the pro&nbsp;MapReduce&nbsp;one. Ironically the&nbsp;&nbsp;Hadoop community from then has come full circle with the introduction of MPI style shared nothing based processing on Hadoop -&nbsp;&nbsp;SQL on Hadoo&nbsp;p.&nbsp;File Systems&nbsp;As the focus shifts to low latency processing, there is a shift from traditional disk based&nbsp;storage file systems to an&nbsp; emergence of in memory file systems - which drastically reduces the I/O &amp; disk serialization cost. Tachyon and Spark&nbsp;RDD&nbsp;are examples of that evolution.Google File System&nbsp;- The seminal work on Distributed File Systems which shaped the Hadoop File System.Hadoop File System&nbsp;– Historical context/architecture on evolution of HDFS.Ceph File System&nbsp;– An&nbsp;&nbsp;alternative&nbsp;&nbsp;to HDFS.&nbsp;Tachyon&nbsp;– An in memory storage system to handle the modern day low latency data processing.File Systems have also seen an evolution on the file formats and compression techniques. The following references gives you a great background on the merits of row and column formats and the shift towards newer nested column oriented formats which are highly efficient for Big Data processing. Erasure codes are using&nbsp;some innovative&nbsp;techniques to reduce the triplication (3 replicas) schemes without compromising data recoverability and availability.Column Oriented vs Row-Stores&nbsp;– good overview of data layout, compression and materialization.RCFile&nbsp;– Hybrid PAX structure which takes the best of both the column and row oriented stores.Parquet&nbsp;– column oriented format first covered in Google’s Dremel’s paper.ORCFile&nbsp;– an improved column oriented format used by Hive.Compression&nbsp;– compression techniques and their comparison on the Hadoop ecosystem.Erasure Codes&nbsp;– background on erasure codes and techniques; improvement on the default triplication on&nbsp;&nbsp;Hadoop&nbsp;&nbsp;to reduce storage cost.Data StoresBroadly, the distributed data stores are classified on&nbsp;ACID &amp; BASE stores depending on the continuum of strong to weak consistency respectively. BASE further is classified into KeyValue, Document, Column and Graph - depending on the underlying schema &amp; supported data structure. While there are multitude of systems and offerings in this space, I have covered few of the more prominent ones. I apologize if I have missed a significant one...BASEKey Value StoresDynamo&nbsp;– key-value distributed storage systemCassandra&nbsp;– Inspired by Dynamo; a multi-dimensional key-value/column oriented data store.Voldemort&nbsp;– another one inspired by Dynamo, developed at LinkedIn.Column Oriented StoresBigTable&nbsp;– seminal paper from Google on distributed column oriented data stores.HBase&nbsp;– while there is no definitive paper , this provides a good overview of the technology.Hypertable&nbsp;– provides a good overview of the architecture.Document Oriented StoresCouchDB&nbsp;– a popular document oriented data store.MongoDB&nbsp;– a good introduction to MongoDB architecture.GraphNeo4j&nbsp;– most popular Graph database.Titan&nbsp;– open source Graph database under the Apache license.ACIDI see a lot of evolution happening in the open source community which will try and catch up with what Google has done – 3 out of the prominent papers below are from Google , they have solved the globally distributed consistent data store problem.Megastore&nbsp;– a highly available distributed consistent database. Uses Bigtable as its storage subsystem.Spanner&nbsp;– Globally distributed synchronously replicated linearizable database which supports SQL access.MESA&nbsp;– provides consistency, high availability, reliability, fault tolerance and scalability for large data and query volumes.CockroachDB&nbsp;– An open source version of Spanner (led by former engineers) in active development.Resource ManagersWhile the first generation of Hadoop ecosystem started with monolithic schedulers like YARN, the evolution now is towards hierarchical schedulers (Mesos), that&nbsp;can manage distinct workloads, across different kind of compute workloads, to achieve higher utilization and efficiency.YARN&nbsp;– The next generation Hadoop compute framework.Mesos&nbsp;– scheduling between multiple diverse cluster computing frameworks.These are loosely&nbsp;coupled with schedulers whose primary function is&nbsp;schedule jobs based on scheduling policies/configuration.SchedulersCapacity Scheduler&nbsp;- introduction to different features of capacity scheduler.&nbsp;FairShare Scheduler&nbsp;- introduction to different features of fair scheduler.Delayed Scheduling&nbsp;- introduction to Delayed Scheduling for&nbsp;FairShare scheduler.Fair &amp; Capacity schedulers&nbsp;– a survey of Hadoop schedulers.CoordinationThese are systems that are used for coordination&nbsp;and state management across distributed data systems.Paxos&nbsp;– a simple version of the&nbsp;&nbsp;classical&nbsp;paper; used for distributed systems consensus and coordination.&nbsp;Chubby&nbsp;– Google’s distributed locking service that implements Paxos.Zookeeper&nbsp;– open source version inspired from Chubby though is general coordination service than simply a locking service&nbsp;Computational FrameworksThe&nbsp;execution runtimes provide an environment for running distinct kinds of compute. The most common runtimes areSpark&nbsp;– its popularity and adoption is challenging the traditional Hadoop ecosystem.&nbsp;Flink&nbsp;– very similar to Spark ecosystem; strength over Spark is in iterative processing.The frameworks broadly can be classified based on the model&nbsp;and latency of processingBatchMapReduce&nbsp;– The seminal paper from Google on MapReduce.MapReduce Survey&nbsp;– A dated, yet a good paper; survey of Map Reduce frameworks.Iterative (BSP)Pregel&nbsp;– Google’s paper on large scale graph processingGiraph&nbsp;-&nbsp;large-scale distributed Graph processing system modelled around PregelGraphX&nbsp;- graph computation framework that unifies graph-parallel and data parallel computation.Hama&nbsp;-&nbsp;general BSP computing engine on top of HadoopOpen source graph processing&nbsp;&nbsp;survey of open source systems modelled around Pregel BSP.StreamingStream&nbsp;Processing&nbsp;– A great overview of the distinct real time processing systems&nbsp;Storm&nbsp;– Real time big data processing systemSamza&nbsp;- stream processing framework from LinkedInSpark Streaming&nbsp;– introduced the micro batch architecture bridging the traditional batch and interactive processing.InteractiveDremel&nbsp;– Google’s paper on how it processes interactive big data workloads, which laid the groundwork for multiple open source SQL systems on Hadoop.Impala&nbsp;– MPI style processing on make Hadoop performant for interactive workloads.Drill&nbsp;– A open source implementation of Dremel.Shark&nbsp;– provides a good introduction to the data analysis capabilities on the Spark ecosystem.Shark&nbsp;– another great paper which goes deeper into SQL access.Dryad&nbsp;– Configuring &amp; executing parallel data pipelines using DAG.Tez&nbsp;– open source implementation of Dryad using YARN.BlinkDB&nbsp;- enabling interactive queries over data samples and presenting results annotated with meaningful error barsRealTimeDruid&nbsp;– a real time OLAP data store. Operationalized time series analytics databasesPinot&nbsp;– LinkedIn OLAP data store very similar to Druid.&nbsp;Data AnalysisThe analysis tools range from declarative languages like SQL to procedural languages like Pig. Libraries on the other hand are supporting out of the box implementations of the most common data mining and machine learning libraries.Tools&nbsp; &nbsp;Pig&nbsp;– Provides a good overview of Pig Latin.Pig&nbsp;– provide an introduction of how to build data pipelines using Pig.Hive&nbsp;– provides an introduction of Hive.Hive&nbsp;– another good paper to understand the motivations behind Hive at Facebook.Phoenix&nbsp;– SQL on Hbase.Join Algorithms for Map Reduce&nbsp;– provides a great introduction to different join algorithms on Hadoop.&nbsp;Join Algorithms for Map Reduce&nbsp;– another great paper on the different join techniques.LibrairesMLlib&nbsp;– Machine language framework on Spark.SparkR&nbsp;– Distributed R on Spark framework.Mahout&nbsp;– Machine learning framework on traditional Map Reduce.Data IntegrationData integration frameworks provide good mechanisms to ingest and outgest data between Big Data systems. It ranges from orchestration pipelines to metadata framework with support for lifecycle management and governance.Ingest/MessagingFlume&nbsp;– a framework for&nbsp;collecting, aggregating and moving large amounts of log data from many different sources to a centralized data store.Sqoop&nbsp;– a tool to move data between Hadoop and Relational data stores.Kafka&nbsp;– distributed messaging system for data processingETL/WorkflowCrunch&nbsp;– library&nbsp;for writing, testing, and running MapReduce pipelines.Falcon&nbsp;–&nbsp;data management framework that helps automate movement and processing of Big Data.Cascading&nbsp;– data manipulation through scripting.Oozie&nbsp;– a workflow scheduler system to manage Hadoop jobs.MetadataHCatalog&nbsp;-&nbsp;a table and storage management layer for Hadoop.SerializationProtocolBuffers&nbsp;– language neutral serialization format popularized by Google.&nbsp;Avro&nbsp;– modeled around Protocol Buffers for the Hadoop ecosystem.Operational FrameworksFinally the operational frameworks provide capabilities for metrics, benchmarking and performance optimization to manage workloads.Monitoring FrameworksOpenTSDB&nbsp;– a time series metrics systems built on top of HBase.Ambari&nbsp;-&nbsp;system for collecting, aggregating and serving Hadoop and system metricsBenchmarkingYCSB&nbsp;– performance evaluation of NoSQL systems.GridMix&nbsp;– provides benchmark for Hadoop workloads by running a mix of synthetic jobsBackground&nbsp;on big data benchmarking with the key challenges associated.SummaryI hope that the papers are useful as you embark or strengthen your journey. I am sure there are few hundred more papers that I might&nbsp;have inadvertently missed and a whole bunch of systems that &nbsp;I might be unfamiliar with - apologies in advance as don't mean to offend anyone though&nbsp;happy&nbsp;to be educated....</div>
                    </div>
    
        <div class='blog'>
            <h2><a href="#top" class='top'>回到顶部</a><a name="blog_657253" href="https://my.oschina.net/taogang/blog/657253">Spark 机器学习实践 ：Iris数据集的分类</a></h2>
            <div class='outline'>
                <div class='date'>时间：2016-04-12 12:10:22</div>
                <div class='catalog'>分类：机器学习</div>
                                                                            </div>
            <div class='content'>今天试用了一下Spark的机器学习，体验如下： 第一步，导入数据 我们使用Iris数据集，做一个分类，首先要把csv文件导入。这里用到了spark的csv包，不明白为什么这么常见的功能不是内置的，还需要额外加载。 --packages com.databricks:spark-csv_2.11:1.4.0 from&nbsp;pyspark.sql&nbsp;import&nbsp;SQLContextsqlContext&nbsp;=&nbsp;SQLContext(sc)df&nbsp;=&nbsp;sqlContext.read.format('com.databricks.spark.csv')&nbsp;&nbsp;&nbsp;&nbsp;.options(header='true',&nbsp;inferschema='true')&nbsp;&nbsp;&nbsp;&nbsp;.load('iris.csv')#&nbsp;Displays&nbsp;the&nbsp;content&nbsp;of&nbsp;the&nbsp;DataFrame&nbsp;to&nbsp;stdoutdf.show() 结果如下： +-----+------------+-----------+------------+-----------+-------+|rowid|Sepal.Length|Sepal.Width|Petal.Length|Petal.Width|Species|+-----+------------+-----------+------------+-----------+-------+|&nbsp;&nbsp;&nbsp;&nbsp;1|&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;5.1|&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;3.5|&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;1.4|&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;0.2|&nbsp;setosa||&nbsp;&nbsp;&nbsp;&nbsp;2|&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;4.9|&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;3.0|&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;1.4|&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;0.2|&nbsp;setosa||&nbsp;&nbsp;&nbsp;&nbsp;3|&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;4.7|&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;3.2|&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;1.3|&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;0.2|&nbsp;setosa||&nbsp;&nbsp;&nbsp;&nbsp;4|&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;4.6|&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;3.1|&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;1.5|&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;0.2|&nbsp;setosa||&nbsp;&nbsp;&nbsp;&nbsp;5|&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;5.0|&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;3.6|&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;1.4|&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;0.2|&nbsp;setosa||&nbsp;&nbsp;&nbsp;&nbsp;6|&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;5.4|&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;3.9|&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;1.7|&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;0.4|&nbsp;setosa||&nbsp;&nbsp;&nbsp;&nbsp;7|&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;4.6|&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;3.4|&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;1.4|&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;0.3|&nbsp;setosa||&nbsp;&nbsp;&nbsp;&nbsp;8|&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;5.0|&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;3.4|&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;1.5|&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;0.2|&nbsp;setosa||&nbsp;&nbsp;&nbsp;&nbsp;9|&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;4.4|&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;2.9|&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;1.4|&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;0.2|&nbsp;setosa||&nbsp;&nbsp;&nbsp;10|&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;4.9|&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;3.1|&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;1.5|&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;0.1|&nbsp;setosa||&nbsp;&nbsp;&nbsp;11|&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;5.4|&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;3.7|&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;1.5|&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;0.2|&nbsp;setosa||&nbsp;&nbsp;&nbsp;12|&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;4.8|&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;3.4|&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;1.6|&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;0.2|&nbsp;setosa||&nbsp;&nbsp;&nbsp;13|&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;4.8|&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;3.0|&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;1.4|&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;0.1|&nbsp;setosa||&nbsp;&nbsp;&nbsp;14|&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;4.3|&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;3.0|&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;1.1|&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;0.1|&nbsp;setosa||&nbsp;&nbsp;&nbsp;15|&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;5.8|&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;4.0|&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;1.2|&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;0.2|&nbsp;setosa||&nbsp;&nbsp;&nbsp;16|&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;5.7|&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;4.4|&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;1.5|&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;0.4|&nbsp;setosa||&nbsp;&nbsp;&nbsp;17|&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;5.4|&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;3.9|&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;1.3|&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;0.4|&nbsp;setosa||&nbsp;&nbsp;&nbsp;18|&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;5.1|&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;3.5|&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;1.4|&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;0.3|&nbsp;setosa||&nbsp;&nbsp;&nbsp;19|&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;5.7|&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;3.8|&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;1.7|&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;0.3|&nbsp;setosa||&nbsp;&nbsp;&nbsp;20|&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;5.1|&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;3.8|&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;1.5|&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;0.3|&nbsp;setosa|+-----+------------+-----------+------------+-----------+-------+only&nbsp;showing&nbsp;top&nbsp;20&nbsp;rows 第二步，提取特征 Spark要求把分类的标签（label）转换成数值进行计算，这一点没有scklearn方便。Spark提供了StringIndexer功能，可以把字符串转换为索引值。 from&nbsp;pyspark.ml.feature&nbsp;import&nbsp;StringIndexerindexer&nbsp;=&nbsp;StringIndexer(inputCol="Species",&nbsp;outputCol="categoryIndex")indexed&nbsp;=&nbsp;indexer.fit(df).transform(df)indexed.show() 提取后，categoryIndex这一列里面就是Species的索引值。 +-----+------------+-----------+------------+-----------+-------+-------------+|rowid|Sepal.Length|Sepal.Width|Petal.Length|Petal.Width|Species|categoryIndex|+-----+------------+-----------+------------+-----------+-------+-------------+|&nbsp;&nbsp;&nbsp;&nbsp;1|&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;5.1|&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;3.5|&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;1.4|&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;0.2|&nbsp;setosa|&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;2.0||&nbsp;&nbsp;&nbsp;&nbsp;2|&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;4.9|&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;3.0|&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;1.4|&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;0.2|&nbsp;setosa|&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;2.0||&nbsp;&nbsp;&nbsp;&nbsp;3|&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;4.7|&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;3.2|&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;1.3|&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;0.2|&nbsp;setosa|&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;2.0||&nbsp;&nbsp;&nbsp;&nbsp;4|&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;4.6|&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;3.1|&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;1.5|&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;0.2|&nbsp;setosa|&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;2.0||&nbsp;&nbsp;&nbsp;&nbsp;5|&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;5.0|&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;3.6|&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;1.4|&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;0.2|&nbsp;setosa|&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;2.0||&nbsp;&nbsp;&nbsp;&nbsp;6|&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;5.4|&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;3.9|&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;1.7|&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;0.4|&nbsp;setosa|&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;2.0||&nbsp;&nbsp;&nbsp;&nbsp;7|&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;4.6|&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;3.4|&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;1.4|&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;0.3|&nbsp;setosa|&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;2.0||&nbsp;&nbsp;&nbsp;&nbsp;8|&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;5.0|&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;3.4|&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;1.5|&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;0.2|&nbsp;setosa|&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;2.0||&nbsp;&nbsp;&nbsp;&nbsp;9|&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;4.4|&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;2.9|&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;1.4|&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;0.2|&nbsp;setosa|&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;2.0||&nbsp;&nbsp;&nbsp;10|&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;4.9|&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;3.1|&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;1.5|&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;0.1|&nbsp;setosa|&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;2.0||&nbsp;&nbsp;&nbsp;11|&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;5.4|&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;3.7|&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;1.5|&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;0.2|&nbsp;setosa|&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;2.0||&nbsp;&nbsp;&nbsp;12|&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;4.8|&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;3.4|&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;1.6|&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;0.2|&nbsp;setosa|&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;2.0||&nbsp;&nbsp;&nbsp;13|&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;4.8|&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;3.0|&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;1.4|&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;0.1|&nbsp;setosa|&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;2.0||&nbsp;&nbsp;&nbsp;14|&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;4.3|&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;3.0|&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;1.1|&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;0.1|&nbsp;setosa|&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;2.0||&nbsp;&nbsp;&nbsp;15|&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;5.8|&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;4.0|&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;1.2|&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;0.2|&nbsp;setosa|&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;2.0||&nbsp;&nbsp;&nbsp;16|&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;5.7|&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;4.4|&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;1.5|&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;0.4|&nbsp;setosa|&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;2.0||&nbsp;&nbsp;&nbsp;17|&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;5.4|&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;3.9|&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;1.3|&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;0.4|&nbsp;setosa|&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;2.0||&nbsp;&nbsp;&nbsp;18|&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;5.1|&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;3.5|&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;1.4|&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;0.3|&nbsp;setosa|&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;2.0||&nbsp;&nbsp;&nbsp;19|&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;5.7|&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;3.8|&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;1.7|&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;0.3|&nbsp;setosa|&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;2.0||&nbsp;&nbsp;&nbsp;20|&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;5.1|&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;3.8|&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;1.5|&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;0.3|&nbsp;setosa|&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;2.0|+-----+------------+-----------+------------+-----------+-------+-------------+only&nbsp;showing&nbsp;top&nbsp;20&nbsp;rows 第三步，模型训练和验证： from&nbsp;pyspark.sql&nbsp;import&nbsp;Rowfrom&nbsp;pyspark.mllib.linalg&nbsp;import&nbsp;Vectorsfrom&nbsp;pyspark.ml.classification&nbsp;import&nbsp;NaiveBayes#&nbsp;Load&nbsp;and&nbsp;parse&nbsp;the&nbsp;datadef&nbsp;parseRow(row):&nbsp;&nbsp;&nbsp;&nbsp;return&nbsp;Row(label=row["categoryIndex"],&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;features=Vectors.dense([row["Sepal.Length"],&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;row["Sepal.Width"],&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;row["Petal.Length"],&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;row["Petal.Width"]]))##&nbsp;Must&nbsp;convert&nbsp;to&nbsp;dataframe&nbsp;after&nbsp;mappingparsedData&nbsp;=&nbsp;indexed.map(parseRow).toDF()nb&nbsp;=&nbsp;NaiveBayes(smoothing=1.0,&nbsp;modelType="multinomial")model&nbsp;=&nbsp;nb.fit(parsedData)predict_data&nbsp;=&nbsp;model.transform(parsedData)traing_err&nbsp;=&nbsp;predict_data.filter(predict_data['label']&nbsp;!=&nbsp;predict_data['prediction']).count()&nbsp;total&nbsp;=&nbsp;predict_data.count()print&nbsp;traing_err,&nbsp;total,&nbsp;float(traing_err)/total 结果如下，在150个样本的训练集上，有7个预测错误： 7&nbsp;150&nbsp;0.0466666666667 这里要注意几点：   Spark有两组机器学习的接口pyspark.ml和pyspark.mllib, 前一个是1.3推出的，比较新，功能也更丰富，后一个是0.9版本推出的，功能少一些。这两组API是不兼容的，你可以选一组来使用。  新的接口要求数据集的类型是dataframe  这里把试用mllib的样例也放出来，大家可以比较一下。 from&nbsp;pyspark.mllib.classification&nbsp;import&nbsp;NaiveBayesfrom&nbsp;pyspark.mllib.regression&nbsp;import&nbsp;LabeledPointfrom&nbsp;pyspark.mllib.linalg&nbsp;import&nbsp;Vectors#&nbsp;Load&nbsp;and&nbsp;parse&nbsp;the&nbsp;datadef&nbsp;parseRow(row):&nbsp;&nbsp;&nbsp;&nbsp;return&nbsp;LabeledPoint(row["categoryIndex"],&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Vectors.dense([row["Sepal.Length"],row["Sepal.Width"],row["Petal.Length"],row["Petal.Width"]]))##&nbsp;Must&nbsp;convert&nbsp;to&nbsp;dataframe&nbsp;after&nbsp;mappingparsedData&nbsp;=&nbsp;indexed.map(parseRow)nb&nbsp;=&nbsp;NaiveBayes()model&nbsp;=&nbsp;nb.train(parsedData)labelsAndPreds&nbsp;=&nbsp;parsedData.map(lambda&nbsp;p:&nbsp;(p.label,&nbsp;model.predict(p.features)))trainErr&nbsp;=&nbsp;labelsAndPreds.filter(lambda&nbsp;(v,&nbsp;p):&nbsp;v&nbsp;!=&nbsp;p).count()&nbsp;/&nbsp;float(parsedData.count())print("Training&nbsp;Error&nbsp;=&nbsp;"&nbsp;+&nbsp;str(trainErr)) 结果和使用ml的是一样的： Training&nbsp;Error&nbsp;=&nbsp;0.0466666666667 然后我试用了SVM，代码如下： from&nbsp;pyspark.mllib.classification&nbsp;import&nbsp;SVMWithSGDfrom&nbsp;pyspark.mllib.regression&nbsp;import&nbsp;LabeledPointfrom&nbsp;pyspark.mllib.linalg&nbsp;import&nbsp;Vectors#&nbsp;Load&nbsp;and&nbsp;parse&nbsp;the&nbsp;datadef&nbsp;parseRow(row):&nbsp;&nbsp;&nbsp;&nbsp;return&nbsp;LabeledPoint(row["categoryIndex"],&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Vectors.dense([row["Sepal.Length"],row["Sepal.Width"],row["Petal.Length"],row["Petal.Width"]]))##&nbsp;Must&nbsp;convert&nbsp;to&nbsp;dataframe&nbsp;after&nbsp;mappingparsedData&nbsp;=&nbsp;indexed.map(parseRow)nb&nbsp;=&nbsp;SVMWithSGD()model&nbsp;=&nbsp;nb.train(parsedData,&nbsp;iterations=10)labelsAndPreds&nbsp;=&nbsp;parsedData.map(lambda&nbsp;p:&nbsp;(p.label,&nbsp;model.predict(p.features)))trainErr&nbsp;=&nbsp;labelsAndPreds.filter(lambda&nbsp;(v,&nbsp;p):&nbsp;v&nbsp;!=&nbsp;p).count()&nbsp;/&nbsp;float(parsedData.count())print("Training&nbsp;Error&nbsp;=&nbsp;"&nbsp;+&nbsp;str(trainErr)) 结果出错了： Py4JJavaError:&nbsp;An&nbsp;error&nbsp;occurred&nbsp;while&nbsp;calling&nbsp;o3397.trainSVMModelWithSGD.:&nbsp;org.apache.spark.SparkException:&nbsp;Input&nbsp;validation&nbsp;failed.at&nbsp;org.apache.spark.mllib.regression.GeneralizedLinearAlgorithm.run(GeneralizedLinearAlgorithm.scala:251)at&nbsp;org.apache.spark.mllib.api.python.PythonMLLibAPI.trainRegressionModel(PythonMLLibAPI.scala:94)at&nbsp;org.apache.spark.mllib.api.python.PythonMLLibAPI.trainSVMModelWithSGD(PythonMLLibAPI.scala:233)at&nbsp;sun.reflect.NativeMethodAccessorImpl.invoke0(Native&nbsp;Method)at&nbsp;sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)at&nbsp;sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)at&nbsp;java.lang.reflect.Method.invoke(Method.java:497)at&nbsp;py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:231)at&nbsp;py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:381)at&nbsp;py4j.Gateway.invoke(Gateway.java:259)at&nbsp;py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:133)at&nbsp;py4j.commands.CallCommand.execute(CallCommand.java:79)at&nbsp;py4j.GatewayConnection.run(GatewayConnection.java:209)at&nbsp;java.lang.Thread.run(Thread.java:745) 从这个结果完全看不出任何端倪。后来查找到了后台日志发现了原因： ERROR&nbsp;DataValidators:&nbsp;Classification&nbsp;labels&nbsp;should&nbsp;be&nbsp;0&nbsp;or&nbsp;1.&nbsp;Found&nbsp;50&nbsp;invalid&nbsp;labels 原来Spark实现的SVM方法只能支持二分类，不支持大于二的分类。这个有点坑呀，scklearn好像是支持的。虽然SVM理论是基于二元分类的，但是有办法扩展。 最后分享一个我在提取分类索引的时候的一个坑，因为觉得字符串映射为数值本身逻辑比较简单，我就自己实现了一个，然后去做map。 ##&nbsp;???&nbsp;does&nbsp;not&nbsp;worklabels&nbsp;=&nbsp;dict()def&nbsp;get_label(s):&nbsp;&nbsp;&nbsp;&nbsp;if&nbsp;labels.get(s)&nbsp;is&nbsp;None:&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;print&nbsp;s&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;l&nbsp;=&nbsp;len(labels)&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;labels[s]&nbsp;=&nbsp;l&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;return&nbsp;labels.get(s)#&nbsp;Load&nbsp;and&nbsp;parse&nbsp;the&nbsp;datadef&nbsp;parsePoint(row):&nbsp;&nbsp;&nbsp;&nbsp;return&nbsp;LabeledPoint(get_label(row["Species"]),&nbsp;[row["Sepal.Length"],row["Sepal.Width"]])parsedData&nbsp;=&nbsp;df.map(parsePoint) 然而这样做是错的，因为传入map的labels是immutable的，在map方法中是无法修改labels的值的。这样才能保证在分布式运行是的无状态和并行。大家以后用的时候要小心。</div>
                    </div>
    
        <div class='blog'>
            <h2><a href="#top" class='top'>回到顶部</a><a name="blog_686568" href="https://my.oschina.net/taogang/blog/686568">用JS实现简单的神经网络算法</a></h2>
            <div class='outline'>
                <div class='date'>时间：2016-06-03 13:13:01</div>
                <div class='catalog'>分类：机器学习</div>
                                                                            </div>
            <div class='content'>笔者尝试用JavaScript实现最简单的神经网络算法。 神经网络简介 神经网络试图模拟大脑的神经元之间的关系来处理信息。它的计算模型通常需要大量彼此连接的节点。每个神经元通过某种特殊的输出函数来处理来自其它相邻神经元的加权输入值。 神经元之间的信息传递的强度，用所谓的加权值来定义，算法会不断的调整加权值来实现自我的学习过程。  神经网络分为多层，如上图，有输入层，隐藏层和输出层。 JS线性代数包 神经网络的计算涉及到大量的矩阵计算，有许多的线性代数的开源的软件，Python下有著名的numpy，非常有名。&nbsp;Javascript也有几个：   http://numericjs.com/index.php  https://github.com/jstat/jstat  https://mkaz.tech/javascript-linear-algebra-calculator.html&nbsp;  我使用了numericjs，效果还不错。推荐大家可以试试。 两层神经网络 我们有一些简单的输入输出的数据用来训练神经网络。这里每一行代表一条数据。输入有三个参数，输出是一个。          Inputs 0    Inputs 1    Inputs 2    Output          0    0    1    0          1    1    1    1          1    0    1    1          0    1    1    0       首先我们实现一个最简单的神经网络，没有隐藏层，输入直连输出。  因为输入是三个参数，输出是一个，所以我们的神经网络输入层是三个节点，输出是1个。 // Sigmod functionfunction nonlin(x, deriv) {  if (deriv) {    return numeric.mul(x, numeric.sub(1, x));  }  return numeric.div(1, numeric.add(1, numeric.exp(numeric.neg(x))));}function train_neural(X, y, iteration) {  // initialize weights  var syn0 = numeric.sub(numeric.mul(2, numeric.random([3, 1])), 1);  //Training loop  var i = 0;  for (; i &lt; iteration; i++) {    var l0 = X;    var l1 = nonlin(numeric.dot(l0, syn0));    var l1_error = numeric.sub(y, l1);    var l1_delta = numeric.mul(l1_error, nonlin(l1, true));    syn0 = numeric.add(syn0, numeric.dot(numeric.transpose(l0), l1_delta));    }   }}//Initial input/ouput valuesvar X = [  [0, 0, 1],  [0, 1, 1],  [1, 0, 1],  [1, 1, 1]];var y = [  [0],  [0],  [1],  [1]];train_neural(X, y, 1000); 简单介绍一下训练的代码和过程   X 输入数据  y 输出数据  nonlin， S函数  l0，网络第一层，这是等于输入数据  l1，网络第二层，这里就是输出层  syn0，第一层网络的权重  训练的迭代过程就是先给出一个初始的权重，利用这个权重算出一个输出值，用目标结果减去这个值，得到一个差异值，再利用这个差异值对权重进行修正。 1000次迭代后的网络输出： ［0.03，0.02， 0.979， 0.974］ 1000次迭代后的syn0权重值： ［7.266，－0.221，－3.415］ 这里我们发现第一个节点的权重较大，这个和我们的数据是一致的，通过观察数据我们也可以发现，输出值和第一列的输入值是强相关。如果增加迭代的次数，这个值会更大。 三层神经网络          Inputs 0    Inputs 1    Inputs 2    Output          0    0    1    0          0    1    1    1          1    0    1    1          1    1    1    0       现在我们有了一组新的数据，通过观察发现，第三列和结果完全无关，第一列和第二列相同时结果为0，否则为1。这是一种非线性的关系，为了有效学习我们增加一层，网络变成了这个样子。  // Sigmod functionfunction nonlin(x, deriv) {  if (deriv) {    return numeric.mul(x, numeric.sub(1, x));  }  return numeric.div(1, numeric.add(1, numeric.exp(numeric.neg(x))));}function train_neural(X, y, iteration) {  // initialize weights  var syn0 = [    [-0.1653904, 0.11737966, -0.71922612, -0.60379702],    [0.60148914, 0.93652315, -0.37315164, 0.38464523],    [0.7527783, 0.78921333, -0.82991158, -0.92189043]  ];  var syn1 = [    [-0.66033916],    [0.75628501],    [-0.80330633],    [-0.15778475]  ];  //Training loop  var i = 0;  for (; i &lt; 1000; i++) {    var l0 = X;    var l1 = nonlin(numeric.dot(l0, syn0));    var l2 = nonlin(numeric.dot(l1, syn1));    var l2_error = numeric.sub(y, l2);    var l2_delta = numeric.mul(l2_error, nonlin(l2, true));    var l1_error = numeric.dot(l2_delta, numeric.transpose(syn1));    var l1_delta = numeric.mul(l1_error, nonlin(l1, true));    syn1 = numeric.add(syn1, numeric.dot(numeric.transpose(l1), l2_delta));    syn0 = numeric.add(syn0, numeric.dot(numeric.transpose(l0), l1_delta));  }}//Initial input/output valuesvar X = [  [0, 0, 1],  [0, 1, 1],  [1, 0, 1],  [1, 1, 1]];var y = [  [0],  [1],  [1],  [0]];train_neural(X, y, 1000); 训练的过程和之前的两层差别不大，只是多了一层。通过增加的这一层，可以有效的学习数据中的复杂非线性的关联关系。 经过1000次迭代， 输出值为：［0.02，0.95，0.94，0.05］ syn0 ：&nbsp;  &nbsp; 如果大家对训练的过程有兴趣，可以运行我的code   http://codepen.io/gangtao/pen/yJLdXG  http://codepen.io/gangtao/pen/jrOjpX  &nbsp;</div>
                            <div class='commentList'>
                    <h3>评论列表</h3>
                                    </div>
                    </div>
    
        <div class='blog'>
            <h2><a href="#top" class='top'>回到顶部</a><a name="blog_693877" href="https://my.oschina.net/taogang/blog/693877"> 一道据说是苹果的面试题</a></h2>
            <div class='outline'>
                <div class='date'>时间：2016-06-18 15:46:38</div>
                <div class='catalog'>分类：数学</div>
                                                                            </div>
            <div class='content'>这是一道据说来自苹果的面试题目，求下图橙色部分围成的面积：据说要15分钟给出答案，这里我思考了一下，给出我的分析。其实就是计算上图中的四边形（其实是正方形）和4个弧形围出的面积。作出上图的辅助线，ADC是正三角形，角ADC是60度，角EDA是30度，角BDC也是三十度，所以角ADB是30度。最后弧形围成的面积就等于30度角扇形面积减去三十度角等腰三角形（腰边长等于正方形边长）面积。求面积的具体公式我就不在这里给出了。示意的代码见&nbsp;http://codepen.io/gangtao/full/ezdJRV/&nbsp;另外，为了作出这个例子，我做了一个SVG path的Editor／Viewer，可以根据代码实时的显示Path的绘制结果。&nbsp;&nbsp;&nbsp;</div>
                    </div>
    
        <div class='blog'>
            <h2><a href="#top" class='top'>回到顶部</a><a name="blog_777780" href="https://my.oschina.net/taogang/blog/777780">Go语言的类IPython 交互式编程界面</a></h2>
            <div class='outline'>
                <div class='date'>时间：2016-10-29 16:27:54</div>
                <div class='catalog'>分类：工作日志</div>
                                                                            </div>
            <div class='content'>Bret Victor 的 Inventing on principle 是我看到过的最令人激动和震撼的演示，没有之一。虽然这位前苹果公司的UI大牛早在2012年就作出的这次演示，但他的影响一直没有减弱，编写程序过程中的任何变化，应该直接的产生反馈，让程序猿能够看到结果，或者说创造者需要对自己的创造的东西有实时的反馈。 之前一直在使用Python，非常喜欢IPythonn Notebook，使用IPython Notenook来快速的完成一些原型非常的方便。现在由于项目的需要，要开始使用Go语言，我就在想，有没有可以使用Go的IPython环境呢？知乎上还有一个相关的帖子，可惜上面并没有给出有效的回答。 我做了些小功课，结果并不完美，这里分享给大家。 官方版 Go Playground&nbsp; 开始学习Go语言最好的资源就是官方的Tour了，大家可以一边学习，一边运行Go的示例程序，直接获得运行结果。完美体现Inventing on principle的理念。  这个Tour内嵌了一个Go的Playgound，大家可以在github上找到该项目的代码。  该项目包含一个前端和一个容器化的后台Sandbox，以保证程序运行的安全性。 然而go playground还有一些限制：   不能import 用户定义的包  编辑器弱，没有语法高亮，没有提示，没有undo ...  没有Ipython那种分段式的交互  XIAM版Go Playground  XIAM的go playground在官方的playground的基础上做了重大的改进。包括：   支持用户自定义的包  支持unsafe sandbox，用户可以访问网络，文件系统等  前端的容器化  想要使用自定义的包，需要修改相应sandbox的Dockerfile FROM xiam/go-playground/unsafeboxRUN go get github.com/myuser/mypackageRUN go get github.com/otheruser/otherpackageENTRYPOINT ["/go/bin/sandbox"] 然后重新构建容器的Image就好了。 虽然我们解决了自定义包的问题，但是，这个编辑器还是太弱了，而且也缺乏IPython的分段式交互。有没有更好的呢？ GopherNotes  Jupyter的Notebook其实可以支持不同的语言内核， GopherNotes项目为Jupyter提供了Go语言的内核。 该项目受到已经不在维护的Gore&nbsp;（基于igo内核） 的启发 。  以上是我使用Gophernotes的一次测试，当我运行一个循环的时候，如果写成一行，In［7］，一切OK。到但是当我写成三行后，In［8］，就无法输出正确的结果了。 后台给出的错误是： Error running goimports:/tmp/979860191/func_proxy.go:4:4: expected declaration, found 'for'[I 08:18:56.621 NotebookApp] Saving file at /Untitled.ipynb 我没有搞明白为什么，报了个bug，如果有搞明白的小伙伴请告诉我。 至此，虽然没有找到一个好用的，但是我对Gophernotes抱以很大的希望，希望它能够早日解决一些基本的问题。 &nbsp;</div>
                    </div>
    
        <div class='blog'>
            <h2><a href="#top" class='top'>回到顶部</a><a name="blog_778136" href="https://my.oschina.net/taogang/blog/778136">容器集群管理平台的比较</a></h2>
            <div class='outline'>
                <div class='date'>时间：2016-10-30 11:38:07</div>
                <div class='catalog'>分类：架构设计</div>
                                                                            </div>
            <div class='content'>容器化和微服务是当前最热话题，不久之前，笔者（据说因为现在都不用笔了，“笔者”的称谓已经不合适了，因为输入用键盘，叫“键人”更为合适）参加QCon上海一个微服务监控的Session，场面爆棚，我不得不在拥挤的过道听完了整个session。随着要管理的容器越来越多，容器的集群管理平台成为了刚需！Docker SwarmSwarm是Docker公司在2014年12月初新发布的容器集群管理工具。它可以把多个主机变成一个虚拟的Docker主机来管理。Swarm使用Go语言开发，并且开源，在github上可以找到它的全部source code。Swarm使用标准的Docker API，给Docker用户带来无缝的集群使用体验。2016年7月， Swarm已经被整合进入Docker Engine。功能Docker Swarm提供API 和CLI来在管理运行Docker的集群，它的功能和使用本地的Docker并没有本质的区别。但是可以通过增加Node带来和好的扩展性。理论上，你可以通过增加节点（Node）的方式拥有一个无限大的Docker主机。Swarm并不提供UI，需要UI的话，可以安装UCP，不过很不幸，这个UCP是收费的。架构Swarm的架构并不复杂，可以说非常简单。Manager负责容器的调度，Node负责容器的运行，Node运行Docker Daemon和Manager之间通过HTTP来通信。Docker Client通过Manager上暴露的标准Docker API来使用Docker的功能。Swarm的集群协调和业务发现可以支持不同的第三方组件，包括：ConsulEtcdZooKeeperDocker Hub如果对集群协调的概念不熟悉，可以参考我的另一篇博客《使用Python进行分布式系统协调 （ZooKeeper，Consul， etcd ）》Swarm的基本容器调度策略有三种：spread &nbsp;容器尽可能分布在不同的节点上binpack 容器尽可能分布在同一个节点上random 容器分布在随机的节点上Swarm集群的高可用配置很容易，以下图为例：manager配置在不同的AWS AZ中，通过领导选举选出Primary manager。多个节点分布在不同的AZ中，同时Consul／etcd／ZooKeeper也需要配成冗余的。特点Docker Swarm的特点是配置和架构都很简单，使用Docker原生的API，可以很好的融合Docker的生态系统。&nbsp;KubernetesKubernetes是Google开发的一套开源的容器应用管理系统，用于管理应用的部署，维护和扩张。利用Kubernetes能方便地管理跨机器运行容器化的应用。Kubernetes也是用Go语言开发的，在github上可以找到源代码。Kubernetes 源于谷歌公司的内部容器管理系统Borg，经过了多年的生产环境的历炼，所以功能非常强大。功能Kubernetes主要提供一下的功能：使用Docker对应用程序包装(package)、实例化(instantiate)、运行(run)。以集群的方式运行、管理跨机器的容器。解决Docker跨机器容器之间的通讯问题。Kubernetes的自我修复机制使得容器集群总是运行在用户期望的状态。应用的高可用和靠扩展支持应用的在线升级（Rolling Update）支持跨云平台（IaaS）的部署为了支持这些功能，Kubernetes做了做了很多的抽象概念，所以，刚开始使用Kubernetes，需要学习不少的新概念，包括：Pod&nbsp;Pod是Kubernetes的基本操作单元，把相关的一个或多个容器构成一个Pod，通常Pod里的容器运行相同的应用，或者是相关的应用。Pod包含的容器运行在同一个Minion(Host)上，看作一个统一管理单元，共享相同的volumes和network namespace/IP和Port空间。JobJob是一个生命周期比较短的应用，一般只会在出错的情况下重启，可以通过配置Job的并发和运行次数来扩展JobServiceService是一个生命周期比较长的应用，会在任何退出时重启，可以通过配置Service的复制（Replica）来达到高扩展和高可用Recplica ControllerReplication Controller确保任何时候Kubernetes集群中有指定数量的pod副本(replicas)在运行， 如果少于指定数量的pod副本(replicas)，Replication Controller会启动新的Container，反之会杀死多余的以保证数量不变。Replication Controller使用预先定义的pod模板创建pods，一旦创建成功，pod 模板和创建的pods没有任何关联，可以修改pod 模板而不会对已创建pods有任何影响，也可以直接更新通过Replication Controller创建的pods以上是一些核心概念，除了这些，Kubernetes还提供其它一些概念，来支持应用程序的运维，包括：Label对系统中的对象通过Label的方式来管理Namespace对对象，资源分组，可以用于支持多租户Config Map提供全局的配置数据存储总之，功能强大，系统概念繁多，比较复杂。Kubernetes支持安装UI的addon，来管理整个系统：架构下图是Kubernetes的基本架构：MasterMaster定义了Kubernetes 集群Master/API Server的主要声明，Client(Kubectl)调用Kubernetes API，管理Kubernetes主要构件Pods、Services、Minions、容器的入口。Master由API Server、Scheduler以及Registry等组成。Scheduler收集和分析当前Kubernetes集群中所有Minion节点的资源(内存、CPU)负载情况，然后依此分发新建的Pod到Kubernetes集群中可用的节点。由于一旦Minion节点的资源被分配给Pod，那这些资源就不能再分配给其他Pod， 除非这些Pod被删除或者退出， 因此，Kubernetes需要分析集群中所有Minion的资源使用情况，保证分发的工作负载不会超出当前该Minion节点的可用资源范围MinionMinion负责运行Pod，Service，Jobs， Minion通过Kubelet和Master通信。Proxy解决了外部网络能够访问跨机器集群中容器提供的应用服务。&nbsp;etcd负责集群的协调和服务发现特点Kubernetes提供了很多应用级别的管理能力，包括高可用可高扩展，当然为了支持这些功能，它的架构和概念都比较复杂，当然我觉得为了获得这些功能，值！&nbsp;Apache Mesos&nbsp;Mesos是为软件定义数据中心而生的操作系统，跨数据中心的资源在这个系统中被统一管理。Mesos的初衷并非管理容器，只是随着容器的发展，Mesos加入了容器的功能。Mesos可以把不同机器的计算资源统一管理，就像同一个操作系统，用于运行分布式应用程序。Mesos的起源于Google的数据中心资源管理系统Borg。你可以从WIRED杂志的这篇文章中了解更多关于Borg起源的信息及它对Mesos影响。功能Mesos的主要功能包括：高度的可扩展和高可用可自定义的两级调度提供API进行应用的扩展跨平台下图是Mesos的管理界面：架构Mesos的基本架构如下&nbsp;MasterMaster负责资源的统一协调和Slave的管理。&nbsp;Master协调全部的Slave，并确定每个节点的可用资源，聚合计算跨节点的所有可用资源的报告，然后向注册到Master的Framework（作为Master的客户端）发出资源邀约。Mesos实现了两级调度架构，它可以管理多种类型的应用程序。第一级调度是Master的守护进程，管理Mesos集群中所有节点上运行的Slave守护进程。集群由物理服务器或虚拟服务器组成，用于运行应用程序的任务，比如Hadoop和MPI作业。第二级调度由被称作Framework的“组件”组成。SlaveSalve运行执行器（Executor）进程来运行任务。FrameworkFramework包括调度器（Scheduler）和执行器（Executor）进程，其中每个节点上都会运行执行器。Mesos能和不同类型的Framework通信，每种Framework由相应的应用集群管理。Framework可以根据应用程序的需求，选择接受或拒绝来自master的资源邀约。一旦接受邀约，Master即协调Framework和Slave，调度参与节点上任务，并在容器中执行，以使多种类型的任务ZooKeeperZookeeper负责集群的协调，Master的领导选举等特点Mesos相比Kubernetes和Swarm更为成熟，但是Mesos主要要解决的是操作系统级别的抽象，并非为了容器专门设计，如果用户出了容器之外，还要集成其它的应用，例如Hadoop，Spark，Kafka等，Mesos更为合适。Mesos是一个更重量级的集群管理平台，功能更丰富，当然很多功能要基于各种Framework。Mesos的扩展性非常好，最大支持50000节点，如果对扩展性要求非常高的话么，Mesos是最佳选择。&nbsp;AWS ECSECS （Amazon EC2 Container Service ）是亚马逊开发出的高度可扩展的高性能容器集群服务。在托管的 Amazon EC2 实例集群上轻松运行容器应用和服务。他最大的好处就是在云上，不需要自己管理数据中心的机器和网络。功能ECS继承了AWS服务的高扩展和高可用性，安全也可以得到保证。在基本容器管理的基础上，ECS使用Task和Service的概念来管理应用。Task类似Docker Compose，使用一个JSON描述要运行的应用。Service是更高一层的抽象，包含多个task的运行实例，通过修改task实例的数量，可以控制服务的伸缩。同时Service可以保证指定数量的Task在运行，当出现错误的时候，重启失败的Task架构下图是ECS的架构图：使用EC2，ELB，安全组等大家熟悉的AWS的概念，AWS用户可以轻松管理你的容器集群。并且不需要付初基本资源以外的费用。通过ECS agent可以是Container连接到ECS集群。ECS Agent使用Go开发，已开源。我们并不清楚ECS的调度策略，但是AWS提供了一个例子，如果继承第三方的调度策略。通过Cloud Watch Log，我们可以很方便的对整个集群进行监控。特点如果你是一个AWS的重度用户，ECS是个不错的选择，因为你可以是把你的容器集群运行在AWS的云上，管理起来非常方便。&nbsp;比较这里对几个平台做一个简单的比较：&nbsp;&nbsp;SwarmKubernetesMesosECS基本功能Docker原生的集群管理工具开源的容器集群工具，提供应用级别的部署，维护和扩张功能基于数据中心的操作系统基于云的高可用，高扩展容器集群高层次抽象无PodJobService无TaskService应用扩展性无支持Framework 支持支持应用高可用性无支持Framework 支持支持集群协调和服务发现etcdZooKeeperConsuletcdZooKeeper／调度策略（Schedule）内置，可扩展内置两级别，可扩展可扩展监控Logging DriverHeapter，ELK Addon内置CloudWatch用户界面CLIAPIUCP UICLIAPIUIAPIUICLIAPIAWS Console开发语言GoGoJavaNA开源是是是否最大支持管理节点数官方1000非官方5000&nbsp;官方1000非官方200010000／&nbsp;总结四个平台，Swarm是最轻量级的，功能也最简单，适于有大量Docker实例环境的用户。Kubernetes增加了很多应用级别的功能，适用于快速应用的部署和维护。Mesos最重，适用于已经有了相当的应用和容器混合的环境。ECS则推荐给AWS的用户或者不希望自己管理数据中心的云用户。&nbsp;相关链接巅峰对决之Swarm、Kubernetes、MesosSwarm v. Fleet v. Kubernetes v. MesosContainer Orchestration Tools: Compare Kubernetes vs Docker SwarmContainer Orchestration Tools: Compare Kubernetes vs MesosCompare Kubernetes vs ECS (Amazon EC2 Container Service)Evaluating Container Platforms at&nbsp;ScaleDocker Swarm Wins Scaling Benchmark but Don’t Take That as Gospel</div>
                            <div class='commentList'>
                    <h3>评论列表</h3>
                                    </div>
                    </div>
    
        <div class='blog'>
            <h2><a href="#top" class='top'>回到顶部</a><a name="blog_868767" href="https://my.oschina.net/taogang/blog/868767">持续交付的架构成熟度模型</a></h2>
            <div class='outline'>
                <div class='date'>时间：2017-03-27 23:08:22</div>
                <div class='catalog'>分类：架构设计</div>
                                                                            </div>
            <div class='content'>随着云和容器技术的发展，大家对DevOps和CI／CD的重要性有了更深入的认识。今天我们就讨论一下架构设计如何更好的支持CI／CD。什么是持续集成，交付和部署（CI／CD）Martin Fowler 和 Kent Beck 首次提出 Continuous Integration （简称：CI），将之描述为：持续集成是一种软件开发实践：许多团队频繁地集成他们的工作，每位成员通常进行 日常集成，进而每天会有多种集成。每个集成会由自动的构建（包括测试）来尽可能快地 检测错误。许多团队发现这种方法可以显著的减少集成问题并且可以使团队的开发更加快捷。持续集成强调开发人员提交了新代码之后，立刻进行构建、（单元）测试。根据测试结果，我们可以确定新代码和原有代码能否正确地集成在一起。持续交付(简称：CD )持续交付在持续集成的基础上，将集成后的代码部署到更贴近真实运行环境的「类生产环境」（production-like environments）中。比如，我们完成单元测试后，可以把代码部署到连接数据库的 Staging 环境中更多的测试。如果代码没有问题，可以继续手动部署到生产环境中。持续部署 (居然简称也是CD )持续部署则是在持续交付的基础上，把部署到生产环境的过程自动化。什么是成熟度模型软件开发者对CMM（Capability Maturity Model &nbsp;能力成熟度模型）这个词应该不陌生，记得我毕业后的第一家单位最早搞ISO9000，后来又要上CMM，后来评了个CMM三级，据说CMM五级的公司都牛B的不行。我一直对这种东西不太感冒，摩托罗拉据说就是CMM五，然而... ... &nbsp;西方人的思维喜欢搞标准化，软件成熟度模型就是一种标准化的模型，把软件组织分成五个等级，一级为初始级，二级为可重复级，三级为已定义级，四级为已管理级，五级为优化级。现在已经没有什么软件企业还搞这个CMM了。不过，这个成熟度模型还是有人在用，这不，有人提出了持续交付的成熟度模型。持续交付的成熟度模型Andreas Rehn，Tobias Palmborg，Patrik Boström提出了持续交付的成熟度模型，针对文化和组织，设计和架构，构建和部署，测试和验证，以及信息和报表五个维度分成了基本（Base），起步（Beginner），中等（Intermediate），高级（Advanced），专家（Expert）五个等级。持续交付和架构设计我们今天主要看看架构的设计对持续交付的影响，先来看看模型是如何定义的。对于大多数的组织来说，为了支持持续交付而重新设计架构并不是一个吸引人的选项，所以该模型可以用来评估组织究竟何种境地。下面就是该模型对五个不同等级关于架构设计的描述。基本（Base）该组织通常在开发，构建，发型一个或者多个具有巨石架构的旧系统。拥有复杂的技术栈，但已经开始考虑统一技术和平台。起步 （Beginner）在这个等级上，巨石结构的系统被分为小的模块。这样的小模块更利于开发，构建和部署。但是这些模块还不能像独立组件一样的发布。这样做很自然的导致使用API来描述内部的依赖关系，并系统的管理第三方的库。在这个等级，通常会使用版本控制来记录和管理变化。中等 （Intermediate）达到中等水平需要有一个坚实的架构基础来支持持续交付，需要采用抽象分支和其它特征隐藏来最小化代码库的分支数量。模块进化为自描述和独立部署的组件。在这个阶段，组织很自然地会把分散的，随意管理的应用的运行配置用版本控制系统进行管理，并把它们看作是代码的一部分。高级 （Advanced）在这个等级，整个软件系统被划分为自包含的组件并严格采用API来进行组件间的通信，这样每个组件才能独立的部署和发布。每个组件都是自包含的，可发布的，具有独立商业价值的单元，该单元可以有很小和很频繁的发布周期。发布新的功能，监视并验证商业价值变的很容易。通过应用收集可视化的业务指标是该阶段软件设计和架构要考虑的一个重要部分。专家 （Expert）在专家级，组织进一步的进化基于组件的架构，把基础设施作为代码以尽可能的减少共享的基础设施，并绑定到应用组件。结果就是系统是可已通过源代码控制系统重建，从操作系统直至应用。这样做可以减少大量的复杂性和其它工具以及技术的开销，例如灾备系统来保证生产环境是可以随时重建的。和虚拟化技术一起，这样做对于手工搭建测试和生产系统带来了很多的灵活性。通过以上描述（翻译水平有限，请谅解），你可能还是不能明白这尼玛究竟说了些什么，我来解读一下：微服务 （Micro Service）随着成熟度的提高，软件架构从巨石架构，到模块化，到组件化，到最终组件通过标准API通信的类似微服务的架构。要做到持续交付，最好把复杂的软件分成独立的服务，每一个服务都可以独立运行。就像凯文凯利在失控一书中提到的，去中心化的分布系统，每一个组件都能独立运作，都很简单，组合在一起，变的无比牛B。生命体，计算机，无不是这样的系统。减少分支 （Minimal Branch）在我们日常的开发中，软件的分支通常非常多，如下图：持续交付的高成熟度表现为尽可能少的分支。最理想的情况，开发直接在Release分支上，提交代码后马上在测试环境下部署测试，测试通过，直接部署到生产系统。这样做，对自动化测试要求非常的高。配置即代码 （Configuration as Code）应用的配置文件必须提交到代码库中管理。这个似乎和架构关系不大。收集业务指标 （Push Business Metrics）收集业务指标是持续交付的一个重要环节，当我的一个新的代码提交后，业务指标直接反馈给相应的决策者，例如性能是变好还是变坏，用户是变多还是变少，等等。所以在软件设计时，如何收集这些业务指标，是需要考虑的。基础设施即代码 （Infrastructure as Code）容器技术对于持续交付的影响可以说是颠覆性的，这里基础设施即代码，大家可以很自然的联想到容器，以前我们要测试，或者部署某个应用，需要把该应用运行在不同的硬件和操作系统下，在新的交付模式中，应用是和它所在的基础设施也就是容器一起交付的。以前对于不同的应用，有不同的交付产物，可能是可执行文件，可能是war包，可能是安装程序，在容器时代，交付物变成了容器镜像（image），于是基础设施变成了代码的一部分（使用Dockerfile来构建镜像）。参考文献http://tracks.roojoom.com/r/35499&nbsp;https://www.infoq.com/articles/Continuous-Delivery-Maturity-Modelhttp://info.thoughtworks.com/rs/thoughtworks2/images/Continuous%20Delivery%20_%20A%20Maturity%20Assessment%20ModelFINAL.pdf&nbsp;</div>
                    </div>
    
        <div class='blog'>
            <h2><a href="#top" class='top'>回到顶部</a><a name="blog_877437" href="https://my.oschina.net/taogang/blog/877437">微服务中的模式和反模式</a></h2>
            <div class='outline'>
                <div class='date'>时间：2017-04-12 11:22:00</div>
                <div class='catalog'>分类：架构设计</div>
                                                                            </div>
            <div class='content'>微服务中的常见设计模式软件开发者对“四人帮”的《设计模式》一书应该都很熟悉，微服务中也会有一些常见的模式：部署模式如何部署服务是微服务中的一个重要问题，微服务的部署方式非常灵活，有以下的不同选项可供选择 （参考 http://www.open-open.com/lib/view/open1462434484246.html）多服务共享主机／虚机单服务部署单一主机／虚机单服务部署单一容器（Docker）无服务部署（serverless），例如AWS Lambda使用服务部署平台 （Kubernetes，Docker Swarm，Mesos， AWS ECS）不同的部署方式各有优缺点，重点推荐使用容器编排系统的服务部署平台，能够提供各种灵活的部署方案。横向关注点微服务的开发过程中常常会花很多时间来处理一些各个服务都会遇到的问题，例如如何管理配置信息，例如用户名和口令，服务器的网络地址，等等日志管理健康检查业务度量数据（Metrics）的收集和分析分布服务的追踪这里推荐使用一个稳定的微服务框架来处理这些问题，例如基于Java的spring boot，基于Golang的Micro等API网关&nbsp;API网关类似服务代理，所有的客户端都通过API网关提供的统一服务API来消费服务内容。下面是几个开源的API GatewayKong （ https://github.com/Mashape/kong ）APIAxle (&nbsp;http://apiaxle.com/&nbsp;)Tyk (&nbsp;http://tyk.io/&nbsp;)apiGrove (&nbsp;http://apigrove.net/&nbsp;)WSO2 API Manager (&nbsp;http://wso2.com/products/api-manager/&nbsp;)服务发现服务发现是指API网关或者客户端如何获得微服务的地址，主要有以下几种发现方式：客户端发现服务器端发现这种方案中的Router可以并入API网关，客户端直接和网关通信。两种方案需要用到服务注册，，区别在于是否把服务注册直接暴露给客户端使用。常见的提供服务发现的注册开源解决方案有：Apache ZookeeperConsulEtcd断路器当微服务系统中的某个服务出现问题的时候，或者网络出现时延的时候，调用客户端会被阻塞，导致大量的调用占用大量的资源。这时候需要引入类似断路器效果的代理，当出现不健康的服务的时候，断路器会返回出错，阻止更多的客户端掉用，直至服务的健康状态恢复。netflix的hystrix提供了类似的服务&nbsp;https://github.com/Netflix/Hystrix数据管理在设计微服务的时候要考虑是否每一个服务拥有自己的数据库或者是共享数据库每个服务拥有自己的数据库共享数据库这两种方式各有优缺点：独立数据库使得各个服务完全解耦合，并且可以根据需要选用不同种类的数据库，但是没有办法或者很难在服务之间共享数据共享数据库能简化维护和技术栈，但是数据库成为所有服务的依赖，系统更多的耦合，带来了不灵活，没有办法根据业务需要选择不同的数据库种类。微服务中的反模式相对于《设计模式》，《反模式》一书可能知道的相对少一点，其实同样的道理，反模式归纳总结了一些常见的容易犯的设计问题，那么，微服务中有哪些反模式呢？聚合混乱软件设计的一个主要思想“高内聚，低耦合”同样适用于微服务，随着系统的发展，应该避免某一个服务变的一场庞大，或者服务之间不必要的过多依赖。不认真对待自动化持续集成和交付和微服务相辅相成，自动化的测试，集成，交付和部署是微服务成败的关键。一个自动化成都不高的微服务是很难成功的。层级的软件架构在设计微服务的时候，应该尽可能避免分层的架构，服务之间更多应该是流式调用。例如为所有的服务提供一个数据接入层的数据服务，似乎不是一个好的选择，因为这样的化就使得所有的服务依赖该数据服务。微服务更多应该基于业务来设计，每个服务应该自包含。以下的架构虽然是一种层级架构，但也是可以采用的，条件是不同的服务不应该共享数据。依赖客户签核当服务有不同的客户渠道来消费的时候，不应该依赖客户的签核，自动化的测试应该覆盖所有的使用场景。手工化的配置管理应该尽量避免手工化地配置管理，实现自动化避免版本管理在微服务中，如果你的系统只有一个版本，那么这肯定是有问题的。前向兼容是一个需要支持的目标，也就是说不同的客户端版本不应该收到服务升级的影响。这也就意味这API一旦发布，就不应该有不兼容的修改。为每一个服务创建网关这个就不用多说了，看着就很傻&nbsp;&nbsp;参考http://microservices.io/patterns/microservices.htmlhttps://www.infoq.com/articles/seven-uservices-antipatterns</div>
                    </div>
    
        <div class='blog'>
            <h2><a href="#top" class='top'>回到顶部</a><a name="blog_877726" href="https://my.oschina.net/taogang/blog/877726">微服务部署方式的演进</a></h2>
            <div class='outline'>
                <div class='date'>时间：2017-04-12 16:41:19</div>
                <div class='catalog'>分类：架构设计</div>
                                                                            </div>
            <div class='content'>Mutable Monster Server最早的巨石架构，应用本身包含状态，整个应用部署在物理节点上，要升级通常需要对整个应用升级，一般需要停止服务。并且这样的架构很难横向扩展。Immutable Server and Reverse Proxy过渡到微服务后，应用是不可变的服务部署在容器或虚机上，服务自身不包含状态，利用反向代理提供访问。这样的架构便于升级和横向扩展，当需要升级的时候，分配新的物理节点，不是新版本的应用到该节点。重新配置代理，切换到新版本的服务。然后下线旧版本。这样的部署方式可以很方便的实现在线升级。同时，水平扩展也变的容易。Immutable Microservices不可变的微服务部署和前一种模式类似，但是当升级时，不需要配置新的物理节点，只需要创建新的容器就好了。&nbsp;</div>
                    </div>
    
        <div class='blog'>
            <h2><a href="#top" class='top'>回到顶部</a><a name="blog_885550" href="https://my.oschina.net/taogang/blog/885550">开源中国用户分析</a></h2>
            <div class='outline'>
                <div class='date'>时间：2017-04-23 18:15:19</div>
                <div class='catalog'>分类：日常记录</div>
                                                                            </div>
            <div class='content'>加入开源中国也有超过三年的时间了，觉得开源中国已经越办越好了，突然很想知道它究竟有多好，我是不是开源中国最老的用户，我有176个开源中国的积分能够排名第几，带着这些问题，我抓取了部分开源中国的用户信息，做了一个简单的分析。 数据获取 要获得用户数据，可以通过开源中国的网页来进行。这个是我的主页面  这个页面包含了用户的基本信息，包括用户名，积分，粉丝，关注等等。 点击粉丝链接可以获得所有的粉丝的情况  然后我们就可以通过这些链接，迭代的找到所有相关连的数据了。 工具选取 这次的数据抓取我选用了requests和pyquery requests是一个非常好用的python的http／rest的客户端，比之python自带的urllib要好用很多，推荐使用。 pyquery是用来解析和操纵html和DOM文档，提供类似jquery的语法，比诸如beatifulSoap要好用不少，尤其如果你是一个前段开发者，熟悉jquery，那就更方便了。大家可以参考我的另一篇博客了解更多的背景信息。 爬取网页数据 为了抓取网页的内容，我们可用chrome自带的工具来查看网页的DOM结构：  核心的代码如下： def get_user_info(url):    try:        r = requests.get(url + "/fans", headers=headers)        doc = pq(r.text)        user_info = dict()        # get user information        user_info["url"] = url        user_info["nickname"] = doc.find(".user-info .nickname").text()        user_info["post"] = doc.find(".user-info .post").text()        user_info["address"] = doc.find(".user-info .address").text()        user_info["score"] = doc.find(".integral .score-num").text()        user_info["fans_number"] = doc.find(".fans .score-num").text()        user_info["follow_number"] = doc.find(".follow .score-num").text()        join_time = doc.find(".join-time").text()        user_info["join_time"] = join_time[            4:15].encode("ascii", "ignore").strip()        # get fans        user_info["fans"] = get_relations(doc)        # get follows, fellow is a wrong spelling        rf = requests.get(url + "/fellow", headers=headers)        user_info["follow"] = get_relations(pq(rf.text))        return user_info    except Exception as e:        return None get_user_info() 方法通过给定的用户url来获取用户信息，首先利用requests的get方法得到用户网页，然后用pyquery抽取出昵称nickname，职位post，地址address，积分score，粉丝数fans_number，关注数follow_number。并得到所有的关注和粉丝的url。这里有一件事比较尴尬，关注的url模式中，关注的英文单词拼错了，应该是follow，而实际上使用的却是fellow，前端程序员也要学好英语呀！ def get_relations(basedoc):    result = list()    try:        doc = basedoc        fans = doc.find(".fans-item")        flist = [fans.eq(i) for i in range(len(fans))]        while True:            for fan in flist:                username = fan.find(".username").text()                url = fan.find(".username a").attr("href")                result.append({"name": username, "link": url})            pages = doc.find("#friend-page-pjax a")            if len(pages) == 0:                break            last_link = pages.eq(len(pages) - 1).attr("href")            last_number = pages.eq(len(pages) - 1).text()            if last_number.encode("utf-8").isdigit():                break            r = requests.get(BASE_URL + "/" + last_link, headers=headers)            doc = pq(r.text)            fans = doc.find(".fans-item")            flist = [fans.eq(i) for i in range(len(fans))]        return result    except Exception as e:        return result get_relations()方法通过循环的方式找到所有的关注或粉丝的url链接。 最后实现一个抓数据的类Scraper： class Scarper(threading.Thread):    def __init__(self, threadName, queue):        super(Scarper, self).__init__(name=threadName)        self._stop = False        self._base_url = BASE_URL        self._base_user = "masokol"        self._check_point = dict()        self._task_queue = queue    def _pull(self, url):        user = get_user_info(url)        if user is None:            return        self._write(user)        for u in user["fans"]:            logger.debug("check a fan {}".format(json.dumps(u)))            if not self._check_point.has_key(u["link"]):                logger.debug("put one task {}".format(u["link"]))                self._task_queue.put(u)        for u in user["follow"]:            logger.debug("check a follow {}".format(json.dumps(u)))            if not self._check_point.has_key(u["link"]):                logger.debug("put one task {}".format(u["link"]))                self._task_queue.put(u)    def _write(self, user):        if self._check_point.has_key(user["url"]):            return        logger.debug(json.dumps(user))        # TODO support unicode logging here        logger.info("name={}, join={}, post={}, address={}, score={}, fans_number={}, follow_number={}".format(            user["nickname"].encode("utf8"),            user["join_time"],            user["post"].encode("utf8"),            user["address"].encode("utf8"),            user["score"].encode("utf8"),            user["fans_number"].encode("utf8"),            user["follow_number"].encode("utf8")))        self._check_point[user["url"]] = True    def init(self):        url = self._base_url + "/" + self._base_user        r = requests.get(url, headers=headers)        self._pull(url)    def run(self):        global IS_TERM        logger.debug("start working")        try:            while True:                logger.debug("pull one task ...")                item = self._task_queue.get(False)                logger.debug("get one task {} ".format(json.dumps(item)))                self._pull(item["link"])                time.sleep(0.1)                if IS_TERM:                    break        except KeyboardInterrupt:            sys.exit()        except Exception as e:            print e 这里面有几个点要注意下：   起始用户我选用了“masokol”，理论上选哪个用户作为起始用户关系不大，假定所有的用户都是关联的，当然这个假定不一定成立。也许存在一个孤岛，孤岛里的用户和其它用户都不关联。如果起始用户在这个孤岛里，那么就只能抓取顾岛内的用户。  利用队列构造生产者消费者模型，这样做的好处是可以利用多线程来提高抓取效率，但是实际操作中，我并没有开多线程，因为不希望给oschina带来太多的网络负载。大家如果要运行我的程序也请注意，友好使用，不要编程ddos攻击。另外利用queue可以把递归调用变为循环，避免stack overflow  checkpoint用来记录抓取的历史，避免重复抓数据。这里是用url作为key的一个数据字典dict来做checkpoint  完整的代码请参考 https://github.com/gangtao/oschina_user_analysis&nbsp; 这里是一个数据抓取程序的最简单实现，若要做到真正好用，有以下几点需要考虑：   持久化任务队列 在该实现中，任务队列在内存中，这样就很难做分布式的扩展，也没办法做到错误恢复，如果程序或数据抓取的节点出问题，则抓取中断。可以考虑使用Kafka，RabbitMQ，SQS，Kenisis来实现这个任务队列  持久化checkpoint 同样的内存内的checkpoint虽然效率高可是无法从错误中恢复，而且如果数据量大，还存在内存不够用的情况  提高并发 在本实现中，并没有利用并发来提高数据吞吐，主要是不想给oschina带来高的负载。通过配置多线程／多进程等手段，可以有效的提高数据抓取的吞吐量。  优化异常处理 在该实现中，大部分错误都没有处理，直接返回。  利用云和无服务 利用云或者无服务（serverless，例如AWS Lamba）技术可以把数据采集服务化，并且可以做到高效的扩展。  数据分析 利用Splunk可以高效的分析日志文件，我在我的Splunk中创建了一个文件的Monitor，这样就可以一边抓取数据，一边分析啦。  这里关键的设置是sourcetype＝oschina_user 好了上分析结果：  以上几个指标是已分析的用户数，平均积分，平均粉丝数和平均关注数。我的176分和105个粉丝都刚好超过平均值一点点呀。 下面是对应的SPL（Splunk Processing Language），因为日志输出是使用的key＝value格式，不需要额外抽取，直接可以用SPL来分析。 sourcetype=oschina_user | stats countsourcetype=oschina_user | stats avg(score)sourcetype=oschina_user | stats avg(fans_number)sourcetype=oschina_user | stats avg(follow_number) &nbsp;  sourcetype=oschina_user | table name, score | sort -score | head 10 这张表是最高积分榜，红薯两万多分是自己改得数据库吧，太不像话了。另外几位大神，jfinal是量子通信的技术副总裁；南湖船老大失业中，大概不想透漏工作信息吧，小小编辑就不说了，如梦技术是皕杰 - 后端工程师。  sourcetype=oschina_user | stats count by addr0 | sort -count | head 5 （注意：这里的addr0字段需要额外的从地址字段中抽取出来，因为原字段包含省份／地区两级信息） 用户来自哪里，北广上占据前三不意外，上海排第三要加油呦。  sourcetype=oschina_user | table name, fans_number |sort - fans_number | head 5sourcetype=oschina_user | stats count by post | sort -count | head 5sourcetype=oschina_user | table name, follow_number | sort -follow_number | head 5 拥有最多粉丝和关注数的的信息。Post是职业信息，都是程序员毫无新意。  sourcetype=oschina_user | stats count by join_year | sort join_year （注意：这里的join_year字段需要额外的从join字段抽取出来。） 这个厉害了，是用户加入oschina的趋势图，2014年以后咋是下降的呢，难道是发展的不好？我是不是发现了什么？红薯要加油了呀！也许是我抓取的用户数量还太少，不能反应真实情况吧。 我的爬虫还在慢慢跑，有了最新的发现会告诉大家！ 后记 最新的数据更新到4800，后面不一定会继续抓数据，大家看看就好。     Top Rank的排名变化不大  平局积分下降到38.5，平均粉丝数下降到21，看来我开始抓取的用户都是核心用户，积分高，粉丝多  这个“我的名字叫李猜”关注了13万用户，你想干啥？  最新的数据现实开源中国用户数量增长显著，17年这还不到一半，新用户的数量明显超过了2016，恭喜红薯了  还有很多可以抓去的信息，例如用户性别，发表的博客数量等等，留个大家去扩展吧。  应红薯的要求，代码也放到了码云上&nbsp;http://git.oschina.net/gangtao/oschina_user_analysis&nbsp;  Splunk真的是太好用了（此处应有掌声）</div>
                            <div class='commentList'>
                    <h3>评论列表</h3>
                                    </div>
                    </div>
    
        <div class='blog'>
            <h2><a href="#top" class='top'>回到顶部</a><a name="blog_890269" href="https://my.oschina.net/taogang/blog/890269">使用开源Echarts为Splunk打造类似语法驱动的分析可视化</a></h2>
            <div class='outline'>
                <div class='date'>时间：2017-05-02 14:02:14</div>
                <div class='catalog'>分类：数据可视化</div>
                                                                            </div>
            <div class='content'>Splunk是业内领先的机器数据平台，有非常易用的用户界面的可视化的选项。Splunk的可视化图表是使用开源的highcharts构建的。但是Splunk内置的可视化选项不够灵活，不能动态的进行数据到图表的绑定。可喜的是，最新版本的Splunk提供了客户自定义图表的功能， 利用该功能，我们可以打造一个类似Tableau或者ggplot的语法驱动的可视化工具。 代码在   github&nbsp;https://github.com/gangtao/echarts_for_splunk&nbsp;  码云&nbsp;http://git.oschina.net/gangtao/echarts_for_splunk  主要逻辑代码代码是echarts_app/appserver/static/visualizations/echarts/src/visualization_source.js，不超过600行代码，大家有兴趣可以去看一下。 安装比较简单，拷贝echarts_app到SPLUNK_HOME/etc/apps目录下, 然后在 echarts_app/appserver/static/visualizations/echarts/ 在运行以下命令： npm installnpm run build 当然，你需要在本机事先安装好Splunk的最近版本和npm，并设置SPLUNK_HOME的环境变量。 目录下自带了几个数据的样本，下面就详细说明一下如何使用： 数据绑定 先解释一下数据绑定，Splunk的SPL返回一个Table结构的数据表，利用该表我们可以把某一列数据绑定在某个可视化的选项上。 例如以下的SPL： source="drinks.csv" | table country, wine_servings,  beer_servings, spirit_servings, total_litres_of_pure_alcohol | head 5 会返回这样的数据表  &nbsp; &nbsp; &nbsp; 我们用上表中每一列的索引来进行绑定，这里country的索引是0，beer_servings的索引是2。 点击可视化选项Format，可以看到，凡是名字中包含Binding的选项，都可以进行数据的绑定。，绑定时，直接输入对应的逗号分割索引列表就好了，例如，要绑定第2和3列数据，输入1，2  单轴 饼图 SPL: source="drinks.csv" | table country, wine_servings,  beer_servings, spirit_servings, total_litres_of_pure_alcohol | head 5 语法设置： Coordinates -&gt; Coordinates Type ＝ Single Axis Single Axis -&gt; Axis Binding = 0 &nbsp;(country) Data Series -&gt; Data Type = Pie Data Series -&gt; Data Color Binding = 1 (wine_servings)  单轴散点图 SPL: source="drinks.csv" | table country, wine_servings,  beer_servings, spirit_servings, total_litres_of_pure_alcohol  语法设置： Coordinates -&gt; Coordinates Type ＝ Single Axis Single Axis -&gt; Axis Binding = 0 &nbsp;(country) Data Series -&gt; Data Type = Scatter Data Series -&gt; Data Color Binding = 1 (wine_servings)  X－Y 坐标系 柱状图 SPL source="drinks.csv" | table country, wine_servings,  beer_servings, spirit_servings, total_litres_of_pure_alcohol | head 10 语法设置： Coordinates -&gt; Coordinates Type ＝ X-Y X-Y&nbsp;Axis -&gt; X-Axis Binding = 0 (country) X-Y&nbsp;Axis -&gt; X-Axis Type&nbsp;= Category X-Y&nbsp;Axis -&gt; Y-Axis Binding = 1,2 (wine_servings, &nbsp;beer_servings) X-Y&nbsp;Axis -&gt; Y-Axis Type = Value Data Series -&gt; Data Type = Bar  堆叠（Stack）模式只要利用echarts的工具箱，点击换为堆叠／换为平铺，直接可以切换  修改&nbsp;Data Series -&gt; Data Type = Bar 可获得折线（Line）图  同样可以切换为堆积模式   折线图和区域（Area）图本质是一样的，只需要选择&nbsp;Data Series -&gt; Show Area&nbsp;＝ True／False  同样的可以切换为堆叠（stack）模式 对换X-Y轴的绑定可以得到Bar Chart X-Y&nbsp;Axis -&gt; X-Axis Binding = 1,2 (wine_servings, &nbsp;beer_servings) X-Y&nbsp;Axis -&gt; X-Axis Type&nbsp;= Value X-Y&nbsp;Axis -&gt; Y-Axis Binding = 0 (country) X-Y&nbsp;Axis -&gt; Y-Axis Type = Category  散点图 SPL source="drinks.csv" | table country, wine_servings,  beer_servings, spirit_servings, total_litres_of_pure_alcohol  语法设置： Coordinates -&gt; Coordinates Type ＝ X-Y X-Y&nbsp;Axis -&gt; X-Axis Binding = 1 (wine_servings) X-Y&nbsp;Axis -&gt; X-Axis Type&nbsp;= Value X-Y&nbsp;Axis -&gt; Y-Axis Binding = 2 (beer_servings) X-Y&nbsp;Axis -&gt; Y-Axis Type = Value Data Series -&gt; Data Type = Scatter   增加对第四列的分析，绑定为颜色 Data Series -&gt; Data Color Binding = 3 （spirit_servings）  增加对第四列的分析，绑定为数据点的大小 Data Series -&gt; Data Size Binding = 4 （total_litres_of_pure_alcohol）  地图 等值线图 (Choropleth Map) SPL source="drinks.csv" | table country, wine_servings,  beer_servings, spirit_servings, total_litres_of_pure_alcohol  语法设置： Coordinates -&gt; Coordinates Type ＝ Geomap Geomap -&gt; Map Type = World Geomap -&gt; Geo Naming Binding = 0 (country) Data Series -&gt; Data Type = Map Data Series -&gt; Data Color Binding = 1 (wine_servings)  &nbsp; 散点图 SPL: source="police_killings.csv" | table latitude,longitude,p_income 语法设置： Coordinates -&gt; Coordinates Type ＝ Geomap Geomap -&gt; Map Type = World Geomap -&gt; Longitude and Latitude Binding = 1,0 (longitude,latitude) Data Series -&gt; Data Type = Scatter Data Series -&gt; Data Color Binding = 2&nbsp;(p_income)  切换为美国地图&nbsp;Geomap -&gt; Map Type = USA  增加绑定数据p_income到点的大小  缺省内置了世界地图，中国地图和美国地图，如果用户需要增加其他地图，需要调用echarts.registerMap方法来增加新的地图。 其它设置 echart提供非常多的可视化的visual选项，这里更多的为了数据分析，并没有提供很多和分析无关的选项。在Settings里面，用户可以选择修改颜色绑定的默认值和数据点最大最小的范围值（单位是像素）。  &nbsp; 总结 相比起Splunk内置的图表，使用语法驱动的图表更为灵活，但是用户需要理解每一个绑定的含义，有一定的难度，但相信如果用户熟悉话，应该是很好用的。欢迎大家来指教。</div>
                            <div class='commentList'>
                    <h3>评论列表</h3>
                                    </div>
                    </div>
    
        <div class='blog'>
            <h2><a href="#top" class='top'>回到顶部</a><a name="blog_983586" href="https://my.oschina.net/taogang/blog/983586">大数据搜索选开源还是商业软件？ElasticSearch 对比 Splunk </a></h2>
            <div class='outline'>
                <div class='date'>时间：2017-06-19 16:31:07</div>
                <div class='catalog'>分类：工作日志</div>
                                                                            </div>
            <div class='content'>本文就架构，功能，产品线，概念等方面就ElasticSearch和Splunk做了一下全方位的对比，希望能够大家在制定大数据搜索方案的时候有所帮助。简介ElasticSearch （1）（2）是一个基于Lucene的开源搜索服务。它提供了一个分布式多用户能力的全文搜索引擎，基于RESTful web接口。Elasticsearch是用Java开发的，并作为Apache许可条款下的开放源码发布，是当前流行的企业级搜索引擎。设计用于云计算中，能够达到实时搜索，稳定，可靠，快速，安装使用方便。ELK是ElasticSearch，Logstash，Kibana的缩写，分别提供搜索，数据接入和可视化功能，构成了Elastic的应用栈。Splunk&nbsp;是大数据领域第一家在纳斯达克上市公司，Splunk提供一个机器数据的引擎。使用 Splunk 可收集、索引和利用所有应用程序、服务器和设备（物理、虚拟和云中）生成的快速移动型计算机数据 。从一个位置搜索并分析所有实时和历史数据。 使用 Splunk&nbsp;处理计算机数据，可让您在几分钟内（而不是几个小时或几天）解决问题和调查安全事件。监视您的端对端基础结构，避免服务性能降低或中断。以较低成本满足合规性要求。关联并分析跨越多个系统的复杂事件。获取新层次的运营可见性以及 IT 和业务智能。根据最新的数据库引擎排名显示，Elastic，Solr和Splunk分别占据了数据库搜索引擎的前三位。从趋势上来看，Elastic和Splunk上升明显，Elastic更是表现出了非常强劲的势头。基本概念Elastic准实时(NRT)Elasticsearch是一个准实时性的搜索平台，从数据索引到数据可以被搜索存在一定的时延。索引（Index）索引是有共同特性的文档的集合，索引有自己的名字，可以对索引执行搜索，更新，删除等操作。类型（Type）每个索引可以包含一个或者多个类型，类型可以看作一个索引数据的逻辑分组，通常我们会把拥有相同字段的文档定义为同一个类型。文档（Document）文档是索引信息的基本单元。Elastic中文档表现为JSON对象，文档物理存贮在索引中，并需要被制定一个类型。因为表现为JSON， 很自然的，文档是由一个个的字段（Feilds）组成，每个字段是一个名值对（Name Value Pair）评分（score）Elastic是基于Lucene构建的，所以搜索的结果会有一个打分。来评价搜索结果和查询的相关性。下图是一个Elastic的搜索在Kibana中看到的例子，原始的数据是一个简单的日志文件：我们通过logstash索引到Elasticsearch后，就可以搜索了。Splunk实时性Splunk同样是准实时的，Splunk的实时搜索（Realtime Search）可以提供不间断的搜索结果的数据流。事件（Event）对应于Elastic的文档，Splunk的数据索引的基本单元是事件，每一个事件包含了一组值，字段，时间戳。Splunk的事件可以是一段文本，一个配置文件，一段日志或者JSON对象。字段（Fields）字段是可以被搜索的名值对，不同的事件可能拥有不同的字段。Splunk支持索引时（index time）和搜索时（search time）的字段抽取（fields extraction）索引（Indexes）类似Elastic的索引，所有的事件物理存储在索引上，可以把索引理解为一个数据库的表。知识对象（Knowledge Object）Splunk的知识对象提供对数据进一步的解释，分类，增强等功能，包括：字段（fields），字段抽取（fields extraction），事件类型（event type），事务（transaction），查找（lookups），标签（tags），别名（aliases），数据模型（data model）等等。下图是一个Splunk的搜索在Splunk客户端看到的和前一个例子同样的日志数据的搜索结果。从基本概念上来看，Elasticsearch和Splunk基本一致。从例子中我们可以看到很多的共性，事件／文档，时间戳，字段，搜索，时间轴图等等。其中有几个主要的差别：Elastic不支持搜索时的字段抽取，也就是说Elastic的文档中的所有字段在索引时已经固定了，而Splunk支持在搜索时，动态的抽取新的字段Elastic的搜索是基于评分机制的，搜索的结果有一个打分，而Splunk没有对搜索结果评分Splunk的知识对象可以提供对数据更高级，更灵活的管理能力。用户接口ElasticSearch提供REST API来进行集群的管理，监控，健康检查索引的管理（CURD）搜索的执行，包括排序，分页，过滤，脚本，聚合等等高级的搜索功能。Elasticsearch 本身并没有提供任何UI的功能，搜索可以用Kibana，但是没有管理UI还是让人不爽的，好在开源的好处就是会有很多的开发者来构建缺失的功能：ElasticHQcerebro&nbsp;(推荐，界面干净，我喜欢)dejavu另一选择就是安装X-Pack，这个是要收费的。Splunk作为企业软件，管理及访问接口比较丰富，除了REST API 和命令行接口，Splunk的UI非常友好易用，基本上所有的功能都能通过集成的UI来使用。同时提供以下接口REST APISplunk UICLI功能数据接入和获取Elastic栈使用Logstash和Beats来进行数据的消化和获取。Logstash用jruby实现，有点像一个数据管道，把输入的数据进行处理，变形，过滤，然后输出到其它地方。Logstash 设计了自己的 DSL，包括有区域，注释，数据类型(布尔值，字符串，数值，数组，哈希)，条件判断，字段引用等。Logstash的数据管道包含三个步骤，Input，Filter和Output，每一步都可以通过plugin来扩展。另外Input和Output还支持配置Codecs，完成对输入输出数据的编解码工作。Logstash支持的常见的Input包含File，syslog，beats等。Filter中主要完成数据的变形处理，可以增删改字段，加标签，等等。作为一个开源软件，Output不仅仅支持ElasticSearch，还可以和许多其它软件集成和目标，Output可以是文件，graphite，数据库，Nagios，S3，Hadoop等。在实际运用中，logstash 进程会被分为两个不同的角色。运行在应用服务器上的，尽量减轻运行压力，只做读取和转发，这个角色叫做 shipper；运行在独立服务器上，完成数据解析处理，负责写入 Elasticsearch 的角色，叫 indexer。logstash 作为无状态的软件，配合消息队列系统，可以很轻松的做到线性扩展Beats是 Elastic&nbsp;从 packetbeat 发展出来的数据收集器系统。beat 收集器可以直接写入 Elasticsearch，也可以传输给 Logstash。其中抽象出来的 libbeat，提供了统一的数据发送方法，输入配置解析，日志记录框架等功能。开源社区已经贡献了许多的beats种类。因为Beats是使用Golang编写的，效率上很不错。Splunk使用Farwarder和Add-ons来进行数据的消化和获取。Splunk内置了对文件，syslog，网络端口等input的处理。当配置某个节点为Forwarder的时候，Splunk Forwarder可以作为一个数据通道把数据发送到配置好的indexer去。这时候，它就类似logstash。这里一个主要的区别就是对数据字段的抽取，Elastic必须在logstash中通过filter配置或者扩展来做，也就是我们所说的Index time抽取，抽取后不能改变。Splunk支持Index time的抽取，但是更多时候，Splunk 在index time并不抽取而是等到搜索是在决定如何抽取字段。对于特定领域的数据获取，Splunk是用Add-on的形式。Splunk 的App市场上有超过600个不同种类的Add-on。用户可以通过特定的Add-on或者自己开发Add-on来获取特定的数据。对于大数据的数据采集，大家也可以参考我的另一篇博客。数据管理和存储ElasticSearch的数据存贮模型来自于Lucene，基本原理是实用了倒排表。大家可以参考这篇文章。Splunk的核心同样是倒排表，推荐大家看这篇去年Splunk Conf上的介绍，Behind the Magnifying Glass: How Search WorksSplunk的Event存在许多Buckets中，多个Buckets构成逻辑分组的索引分布在Indexer上。每个Bucket中都是倒排表的结构存储数据，原始数据通过gzip压缩。搜索时，利用Bloom filter定位数据所在的bucket。在对数据的存储管理上，Elastic 和Splunk都是利用了倒排表。Splunk对数据进行压缩，所以存储空间的占用要少很多，尤其考虑到大部分数据是文本，压缩比很高的，当然这会损失一部分性能用于数据的解压。数据分析和处理对数据的处理分析，ElasticSearch主要使用 Search API来实现。而Splunk则提供了非常强大的SPL，相比起ES的Search API，Splunk的SPL要好用很多，可以说SPL就是非结构化数据的SQL。无论是利用SPL来开发分析应用，还是直接在Splunk UI上用SPL来处理数据，SPL都非常易用。开源社区也在试图为Elastic增加类似SPL的DSL来改善数据处理的易用性。例如：https://github.com/chenryn/ESPL&nbsp;从这篇反馈可以看出，ES的search还有许多的不足。作为对此的响应，Elastic推出了painless script，该功能还处于实验阶段。数据展现和可视化Kibana是一个针对Elasticsearch的开源分析及可视化平台，用来搜索、查看交互存储在Elasticsearch索引中的数据。使用Kibana，可以通过各种图表进行高级数据分析及展示。Splunk集成了非常方便的数据可视化和仪表盘功能，对于SPL的结果，可以非常方便的通过UI的简单设置进行可视化的分析，导出到仪表盘。下图的比较来自https://www.itcentralstation.com/products/comparisons/kibana_vs_splunk在数据可视化的领域的排名，Splunk仅仅落后于Tableau而已扩展性从扩展性的角度来看，两个平台都拥有非常好的扩展性。Elastic栈作为一个开源栈，很容易通过Plugin的方式扩展。包括：ElasticSearch Plugin&nbsp;Kibana PluginLogstash PluginBeats PlatformSplunk提供一系列的扩展点支持应用和Add-on的开发， 在http://dev.splunk.com/可以找到更多的信息和文档。包括：Web FrameworkSDKModular Input... ...比起Elastic的Plugin，Splunk的扩展概念上比较复杂，开发一个App或者Add-on的门槛都要相对高一些。做为一个数据平台，Splunk应该在扩展性上有所改进，使得扩展变的更为容易和简单。架构Elastic Stack如上图所示，ELK是一套栈，Logstash提供数据的消化和获取，Elasticsearch对数据进行存储，索引和搜索，而Kibana提供数据可视化和报表的功能。SplunkSplunk的架构主要有三个角色：IndexerIndexer提供数据的存储，索引，类似Elasticsearch的作用Search HeadSearch Head负责搜素，客户接入，从功能上看，一部分是Kibana，因为Splunk的UI是运行在Search Head上的，提供所有的客户端和可视化的功能，还有一部分，是提供分布式的搜索功能，包含对搜索的分发到Indexer和搜索结果的合并，这一部分功能对应在Elasticsearch上。ForwarderSplunk的Forwarder负责数据接入，类似Logstash除了以上的三个主要的角色，Splunk的架构中还有：Deployment Server，License Server，Master Cluster Node，Deployer等。Splunk和ELK的基本架构非常类似，但是ELK的架构更为简单和清楚，Logstash负责数据接入，Kibana负责数据展现，所有的复杂性在Elasticsearch中。Splunk的架构更为复杂一些，角色的类型也更多一些。如果装单机版本，Splunk更容易，因为所有的功能一次性就装好了，而ELK则必须分别安装E/L/K，从这一点上来看，Splunk有一定的优势。分布集群和扩展性ElasticSearchElasticSearch是为分布式设计的，有很好的扩展性，在一个典型的分布式配置中，每一个节点（node）可以配制成不同的角色，如上图所示：Client Node，负责API和数据的访问的节点，不存储／处理数据Data Node，负责数据的存储和索引Master Node， 管理节点，负责Cluster中的节点的协调，不存储数据。每一种角色可以通过ElasticSearch的配置文件或者环境变量来配置。每一种角色都可以很方便的Scale，因为Elastic采用了对等性的设计，也就是所有的角色是平等的，（Master Node会进行Leader Election，其中有一个是领导者）这样的设计使得在集群环境的伸缩性非常好，尤其是在容器环境，例如Docker Swarm或者Kubernetes中使用。参考：https://elk-docker.readthedocs.io/#elasticsearch-clusterhttps://github.com/pires/kubernetes-elasticsearch-clusterSplunkSplunk作为企业级的分布式机器数据的平台，拥有强大的分布式配置，包括跨数据中心的集群配置。Splunk提供两种集群，Indexer集群和Search Head集群。Splunk&nbsp;Indexer集群如上图所示，Splunk的indexer集群主要由三种角色：Master Node，Master Node负责管理和协调整个的集群，类似ES的Master。但是只有一个节点，不支持多Master（最新版本6.6）。Master Node负责协调Peer Node之间的数据复制告诉Search Head数据在哪里Peer Node的配置管理Peer Node故障时的故障恢复Peer Nodes，负责数据索引，类似ES的Data Node，Peer Node负责存储索引数据发送／接收复制数据到其他Peer节点响应搜索请求Search Head，负责数据的搜索和客户端API访问，类似ES的Client Node，但不完全相同。Search Head负责发送搜索请求到Peer Nodes，并对搜索的结果进行合并。有人会问，那Master是不是集群中的单点故障？What if Master node goes down？Splunk的回答是否。即使Master 节点出现故障，Peer Nodes仍然可以正常工作，除非，同时有Peer Node出现故障。http://docs.splunk.com/Documentation/Splunk/6.6.1/Indexer/Whathappenswhenamasternodegoesdownhttps://answers.splunk.com/answers/129446/why-does-master-node-continue-to-be-single-point-of-failure-in-clustering.htmlSplunk Search Header 集群Search Head集群是由一组Search Head组成，它们共享配置，搜索任务等状态。该Cluster主要有以下角色：Deployer， 负责分发状态和应用到peersCluster Member，其中有一个是Captain，负责协调。Cluster Memeber之间会互相通信，来保证状态一致。Load Balancer是个可选项，可以负责Search的接入。Search Peers，负责数据索引的&nbsp;Indexer Nodes另外Splunk还曾经提供过一个功能叫做Search Head Pooling，不过现在已经Depecated了。Indexer集群可以和Search Head集群一起配置，构成一个分布式的Splunk配置。相比较ES的相对比较简单的集群配置，Splunk的集群配置比较复杂，ES中所有每一个节点可以灵活的配置角色，并且可以相对比较容易的扩展，利用例如Kubernetes的Pod的复制可以很容易的扩展每一个角色。扩展Splunk相对比较困难，要做到动态的伸缩，需要比较复杂的配置。大家可以参考这里，在容器环境里配置一个Splunk的集群需要比较多的布置，例如在这个Master的配置中，用户需要考虑：如何配置License修改缺省的用户名口令为每一个Search Head配置Search Head Cluster等待Splunk进程成功启动配置业务发现安装应用... ...并且集群的扩展很难直接利用容器编排平台提供的扩展接口，这一点Splunk还有很多提高的空间。产品线ElasticElastic的产品线除了大家熟悉的ELK（ElasticSearch，Logstash，Kikana），主要包含Beats&nbsp;Beats是一个开源组件，提供一个代理，把本地抓到的数据传送到ElasticSearchElastic Cloud， Elasti提供的云服务X-Pack， Elastic的扩展组件，提供安全，告警，监控，机器学习和图处理能力。主要功能需要付费使用。SplunkSplunk的产品线包括Splunk Enterprise&nbsp;Splunk Cloud， Splunk运营的云服务，跑在AWS上Splunk Light，Splunk Light版本，功能有所精简，面向中小企业Hunk， Splunk on HadoopApps ／ Add-ons, &nbsp;Splunk提供大量的应用和数据获取的扩展，可以参考&nbsp;http://apps.splunk.com/Splunk ITSI （IT Service Intelligence）， Splunk为IT运维专门开发的产品Splunk ES （Enterprise Security）， Splunk为企业安全开发的产品，这个是Splunk 公司的拳头产品，连续被Gartner评为SIEM领域的领导者，挑战了该行业的传统巨鳄IBM，HPSplunk UBA （User Behavior Analytic）， UBA是Splunk在15年收购的Caspidia带来的基于机器学习的安全产品。从产品线的角度来看，Splunk除了提供基本平台，在IT运维和安全领域都有自己的拳头产品。Elastic缺乏某个领域的应用。价格价格是大家非常关心的一个因素Elastic的基本组件都是开源的，参看下表，X-pack中的一些高级功能需要付费使用。包含安全，多集群，报表，监控等等。云服务的价格参考下图，ES的云是按照所使用的资源来收费，从这里选取的区域可以看出，ES的云也是运行在AWS上的。下图中的配置每月需要花费200美元左右。（不同区域的收费不同）同时，除了Elastic自己，还有许多其他公司也提供Elastic Search的云服务，例如Bonsai，Qbox.io等。SplunkSplunk Enterprise是按照数据每日的流量按年或者无限制事件付费，每天1GB的话，每年是2700美元，每个月也是差不多200块。如果每天的数据量少于500M，可以使用Splunk提供的免费License，只是不能用安全，分布式等高级功能，500M可以做很多事情了。云服务的价格就要便宜多了，每天5GB，每年只要2430元，每个月不到200块。当然因为计费的方式不同，和Elastic的云就不好比较了。另外因为是在AWS上，中国的用户，呵呵了。总结大数据的搜索平台已经成为了众多企业的标配，Elastic栈和Splunk是其中最为优秀和流行的选择。两者都有各自的优点和值得改进的地方。希望本文能够在你的大数据平台的选型上，有所帮助。也希望大家来和我交流，共同成长。参考文档ELKElasticSearch 参考文档https://www.elastic.co/guide/en/elasticsearch/reference/current/index.htmlGithub上收集的ElasticSearch相关开源软件列表&nbsp;https://github.com/dzharii/awesome-elasticsearch知乎ElaticSearch专题&nbsp;https://www.zhihu.com/topic/19899427/hot中文书&nbsp;https://github.com/chenryn/ELKstack-guide-cn中文书&nbsp;https://www.gitbook.com/book/wizardforcel/mastering-elasticsearch/detailsSplunkSplunk 文档&nbsp;https://docs.splunk.com/DocumentationSplunk电子书&nbsp;https://www.splunk.com/web_assets/v5/book/Exploring_Splunk.pdfSplunk 开发文档&nbsp;http://dev.splunk.com/getstartedSplunk 应用市场&nbsp;http://apps.splunk.com/Splunk 快速参考&nbsp;https://www.splunk.com/content/dam/splunk2/pdfs/solution-guides/splunk-quick-reference-guide.pdf其它https://www.upguard.com/articles/splunk-vs-elkhttps://db-engines.com/en/system/Elasticsearch%3BSplunkhttps://www.searchtechnologies.com/blog/log-analytics-tools-open-source-vs-commercialhttp://www.learnsplunk.com/splunk-vs-elk-stack.htmlhttps://www.slideshare.net/hepterida/splunk-vs-elkhttp://blog.takipi.com/log-management-tools-face-off-splunk-vs-logstash-vs-sumo-logic/http://blog.takipi.com/splunk-vs-elk-the-log-management-tools-decision-making-guide/https://www.edureka.co/blog/splunk-vs-elk-vs-sumologichttps://www.youtube.com/watch?v=ElMZqeogc3w&nbsp;(请自行翻墙)</div>
                            <div class='commentList'>
                    <h3>评论列表</h3>
                                    </div>
                    </div>
    
        <div class='blog'>
            <h2><a href="#top" class='top'>回到顶部</a><a name="blog_1530258" href="https://my.oschina.net/taogang/blog/1530258">把代码执行演示嵌在你的PPT中</a></h2>
            <div class='outline'>
                <div class='date'>时间：2017-09-06 20:15:41</div>
                <div class='catalog'>分类：架构设计</div>
                                                                            </div>
            <div class='content'>“Talk is cheap, show me your code!” 当一个程序员在做技术分享的时候， 代码演示经常是不可或缺的一个环节。然而在你的演示PPT和代码运行之间切换是一件非常恼人事情，而且非常影响演示的节奏和流畅性。要做一个完美的技术分享，能不能把代码的运行嵌入到PPT中呢？ 当然可以，我前不久在公司内部分享了一起关于Python的小谜题，大家可以到斗鱼去观看这次分享的视频录播&nbsp;（很糟糕的是在36分钟后，摄像头偏移了，拍了大半天挂钟） 为了实现代码嵌入ppt，我用到的关键技术包括：   Microsoft PowerPoint  Glot Code Runner  Docker  首先，演示常用的是微软的PowerPoint（至于苹果的Keynote，我暂时还没有找到解决方案），所以你需要安装PowerPoint，在最新的Office套件中，微软提供了一个Webview的插件，有了这个插件，可以直接把一个Web页面，嵌入到Office文档中，自然也就包含了PPT。  注意，为了安全性的考虑，嵌入的web页面必须是https的。 好了有了这个，我们就可以把一个代码运行的web页面直接嵌入到演示文档中，剩下的事情，就是做一个可以运行不同代码的Web应用了。 Glot&nbsp;是一个开源的在线代码运行平台。其架构如下图所示。  利用Glot的代码运行的核心功能，我们就可以很方便的开发一个可以运行各种代码的web应用。 利用容器，我们可以把整个应用分为以下的几个层次：   Base 提供基本的代码运行的环境，包含了代码执行的必要的解释器和编译器。在本次演示中，我使用了golang:latest，但是碰巧的是这个镜像是拥有Python的解释器的，我的代码演示都是python，省去了我安装Python的步骤。如果是别的不同的语言的演示，注意安装对应的环境。  Code Runner&nbsp;https://github.com/gangtao/code_runner 我的Code Runner是对Glot的Code Runner的一个增强，该项目提供一个运行代码的服务。 项目的Dockerfile： FROM golang:latestMAINTAINER gangtao@outlook.comENV GOPATH=/home/glotENV GOROOT=/usr/local/go# Add userRUN groupadd glotRUN useradd -m -d /home/glot -g glot -s /bin/bash glot# Copy filesAdd ./build/release/server /home/glot/# Add ./vendor/. /home/glot/srcUSER glotWORKDIR /home/glot/# generate certificateRUN go run $GOROOT/src/crypto/tls/generate_cert.go --host localhostEXPOSE 8080# CMD ["/home/glot/runner"]ENTRYPOINT ["/home/glot/server"] 通过Dockerfile我们可以看出该项目主要的内容：利用Glot实现代码运行，产生证书用于https，利用echo实现web 服务。 Web服务的代码如下： package code_runnerimport ("fmt""github.com/prasmussen/glot-code-runner/cmd""github.com/prasmussen/glot-code-runner/language""io/ioutil""os""path/filepath")type Payload struct {Language string          `json:"language"`Files    []*InMemoryFile `json:"files"`Stdin    string          `json:"stdin"`Command  string          `json:"command"`}type InMemoryFile struct {Name    string `json:"name"`Content string `json:"content"`}type Result struct {Stdout string `json:"stdout"`Stderr string `json:"stderr"`Error  string `json:"error"`}func Run(payload *Payload) *Result {// Ensure that we have at least one fileif len(payload.Files) == 0 {exitF("No files given\n")}// Check if we support given languageif !language.IsSupported(payload.Language) {exitF("Language '%s' is not supported\n", payload.Language)}// Write files to diskfilepaths, err := writeFiles(payload.Files)if err != nil {exitF("Failed to write file to disk (%s)", err.Error())}var stdout, stderr string// Execute the given command or run the code with// the language runner if no command is givenif payload.Command == "" {stdout, stderr, err = language.Run(payload.Language, filepaths, payload.Stdin)} else {workDir := filepath.Dir(filepaths[0])stdout, stderr, err = cmd.RunBashStdin(workDir, payload.Command, payload.Stdin)}result := &amp;Result{Stdout: stdout,Stderr: stderr,Error:  errToStr(err),}return result}// Writes files to disk, returns list of absolute filepathsfunc writeFiles(files []*InMemoryFile) ([]string, error) {// Create temp dirtmpPath, err := ioutil.TempDir("", "")if err != nil {return nil, err}paths := make([]string, len(files), len(files))for i, file := range files {path, err := writeFile(tmpPath, file)if err != nil {return nil, err}paths[i] = path}return paths, nil}// Writes a single file to diskfunc writeFile(basePath string, file *InMemoryFile) (string, error) {// Get absolute path to file inside basePathabsPath := filepath.Join(basePath, file.Name)// Create all parent dirserr := os.MkdirAll(filepath.Dir(absPath), 0775)if err != nil {return "", err}// Write file to diskerr = ioutil.WriteFile(absPath, []byte(file.Content), 0664)if err != nil {return "", err}// Return absolute path to filereturn absPath, nil}func exitF(format string, a ...interface{}) {fmt.Fprintf(os.Stderr, format, a...)os.Exit(1)}func errToStr(err error) string {if err != nil {return err.Error()}return ""}  可以通过容器方便的启动该服务，然后就可以通过Rest请求，执行Python（Golang）的代码。   curl \  -X POST \  http://localhost:8080/run \  -H 'Content-Type: application/json' \  -d '{"language":"python","files":[{"name":"main.py","content":"print(42)"}]}'{"stdout":"42\n","stderr":"","error":""} 在这个项目中，我用了echo来实现一个轻量级的Web服务，而没有使用Glot自带的基于Ruby的服务，这样做的好处是技术栈的统一，因为echo和glot的核心都是用的Golang。   Code Runner Web&nbsp;https://github.com/gangtao/code_runner_web 有了服务，下面就是前端的UI了。 UI使用了codemirror来做编辑器。Dockerfile如下： FROM naughtytao/code_runnerMAINTAINER gangtao@outlook.comAdd ./static /home/glot/static 运行Code Runner Web后，就可以在以下的web界面中输入你想要运行的结果，并实时的显示想要运行的结果了。    好了，剩下的还有一些事情要做，就是准备你的演示代码，大家可以参考这个项目，这里缺省是将所有的代码片段放在code/python/ 目录下。运行：http://localhost:8080/#2&nbsp;就会加载第二个代码片段。 这个是嵌入后的效果：  好了，是不是很Coooooool呢？ 另外利用容器来构建应用真的非常非常方便。</div>
                            <div class='commentList'>
                    <h3>评论列表</h3>
                                    </div>
                    </div>
    
        <div class='blog'>
            <h2><a href="#top" class='top'>回到顶部</a><a name="blog_1544709" href="https://my.oschina.net/taogang/blog/1544709">图解机器学习</a></h2>
            <div class='outline'>
                <div class='date'>时间：2017-09-28 13:15:41</div>
                <div class='catalog'>分类：机器学习</div>
                                                                            </div>
            <div class='content'>警告：多图杀猫！每当提到机器学习，大家总是被其中的各种各样的算法和方法搞晕，觉得无从下手。确实，机器学习的各种套路确实不少，但是如果掌握了正确的路径和方法，其实还是有迹可循的，这里我推荐SAS的Li Hui的这篇博客，讲述了如何选择机器学习的各种方法。&nbsp;另外，Scikit-learn&nbsp;也提供了一幅清晰的路线图给大家选择：其实机器学习的基本算法都很简单，下面我们就利用二维数据和交互图形来看看机器学习中的一些基本算法以及它们的原理。（另外向Bret Victor致敬，他的&nbsp;Inventing on principle&nbsp;深深的影响了我）所有的代码即演示可以在我的Codepen的这个Collection中找到。首先，机器学习最大的分支的监督学习和无监督学习，简单说数据已经打好标签的是监督学习，而数据没有标签的是无监督学习。从大的分类上看，降维和聚类被划在无监督学习，回归和分类属于监督学习。无监督学习如果你的数据都没有标签，你可以选择花钱请人来标注你的数据，或者使用无监督学习的方法。首先你可以考虑是否要对数据进行降维。降维降维顾名思义就是把高维度的数据变成为低维度。常见的降维方法有PCA, LDA, SVD等。主成分分析 PCA降维里最经典的方法是主成分分析PCA，也就是找到数据的主要组成成分，抛弃掉不重要的成分。这里我们先用鼠标随机生成8个数据点，然后绘制出表示主成分的白色直线。这根线就是二维数据降维后的主成分，蓝色的直线是数据点在新的主成分维度上的投影线，也就是垂线。主成分分析的数学意义可以看成是找到这根白色直线，使得投影的蓝色线段的长度的和为最小值(严格地说应该是平方和最小)。See the Pen ML Explained PCA by gangtao (@gangtao) on CodePen.更多PCA的相关例子，可以参考：D3 http://bl.ocks.org/hardbyte/40cd6622cffbe98055d3http://setosa.io/ev/principal-component-analysis/聚类因为在非监督学习的环境下，数据没有标签，那么能对数据所做的最好的分析除了降维，就是把具有相同特质的数据归并在一起，也就是聚类。层级聚类 Hierachical Cluster该聚类方法用于构建一个拥有层次结构的聚类，&nbsp;如上图所示，层级聚类的算法非常的简单：初始时刻，所有点都自己是一个聚类找到距离最近的两个聚类（刚开始也就是两个点），形成一个聚类两个聚类的距离指的是聚类中最近的两个点之间的距离重复第二步，直到所有的点都被聚集到聚类中。See the Pen ML Explained Hierarchical Clustering by gangtao (@gangtao) on CodePen.KMeansKMeans中文翻译K均值算法，是最常见的聚类算法。随机在图中取K（这里K=3）个中心种子点。然后对图中的所有点求到这K个中心种子点的距离，假如点P离中心点S最近，那么P属于S点的聚类。接下来，我们要移动中心点到属于他的“聚类”的中心。然后重复第2）和第3）步，直到，中心点没有移动，那么算法收敛，找到所有的聚类。KMeans算法有几个问题：如何决定K值，在上图的例子中，我知道要分三个聚类，所以选择K等于3，然而在实际的应用中，往往并不知道应该分成几个类由于中心点的初始位置是随机的，有可能并不能正确分类，大家可以在我的Codepen中尝试不同的数据如下图，如果数据的分布在空间上有特殊性，KMeans算法并不能有效的分类。中间的点被分别归到了橙色和蓝色，其实都应该是蓝色。See the Pen ML Explained KMeans by gangtao (@gangtao) on CodePen.DBSCANDBSCAN（Density-Based Spatial Clustering of Applications with Noise）中文是基于密度的聚类算法。DBSCAN算法基于一个事实：一个聚类可以由其中的任何核心对象唯一确定。算法的具体聚类过程如下：扫描整个数据集，找到任意一个核心点，对该核心点进行扩充。扩充的方法是寻找从该核心点出发的所有密度相连的数据点（注意是密度相连）。遍历该核心点的邻域内的所有核心点（因为边界点是无法扩充的），寻找与这些数据点密度相连的点，直到没有可以扩充的数据点为止。最后聚类成的簇的边界节点都是非核心数据点。之后就是重新扫描数据集（不包括之前寻找到的簇中的任何数据点），寻找没有被聚类的核心点，再重复上面的步骤，对该核心点进行扩充直到数据集中没有新的核心点为止。数据集中没有包含在任何簇中的数据点就构成异常点。See the Pen ML Explained DBSCAN by gangtao (@gangtao) on CodePen.如上图所示，DBSCAN可以有效的解决KMeans不能正确分类的数据集。并且不需要知道K值。当然，DBCSAN还是要决定两个参数，如何决定这两个参数是分类效果的关键因素：一个参数是半径（Eps），表示以给定点P为中心的圆形邻域的范围；另一个参数是以点P为中心的邻域内最少点的数量（MinPts）。如果满足：以点P为中心、半径为Eps的邻域内的点的个数不少于MinPts，则称点P为核心点。&nbsp;监督学习监督学习中的数据要求具有标签。也就是说针对已有的结果去预测新出现的数据。如果要预测的内容是数值类型，我们称作回归，如果要预测的内容是类别或者是离散的，我们称作分类。其实回归和分类本质上是类似的，所以很多的算法既可以用作分类，也可以用作回归。回归线性回归线性回归是最经典的回归算法。在统计学中，线性回归（Linear regression）是利用称为线性回归方程的最小二乘函数对一个或多个自变量和因变量之间关系进行建模的一种回归分析。 这种函数是一个或多个称为回归系数的模型参数的线性组合。 只有一个自变量的情况称为简单回归，大于一个自变量情况的叫做多元回归。See the Pen ML Explained Linear Regression by gangtao (@gangtao) on CodePen.如上图所示，线性回归就是要找到一条直线，使得所有的点预测的失误最小。也就是图中的蓝色直线段的和最小。这个图很像我们第一个例子中的PCA。仔细观察，分辨它们的区别。如果对于算法的的准确性要求比较高，推荐的回归算法包括：随机森林，神经网络或者Gradient Boosting Tree。如果要求速度优先，建议考虑决策树和线性回归。分类逻辑回归逻辑回归虽然名字是回归，但是却是个分类算法。因为它和SVM类似是一个二分类，数学模型是预测1或者0的概率。所以我说回归和分类其实本质上是一致的。See the Pen ML Explained Logistic Regression by gangtao (@gangtao) on CodePen.这里要注意逻辑回归和线性SVM分类的区别，可以阅读：https://www.zhihu.com/question/26768865http://blog.jobbole.com/98635/支持向量机 SVM如果对于分类的准确性要求比较高，可使用的算法包括Kernel SVM，随机森林，神经网络以及Gradient Boosting Tree。给定一组训练实例，每个训练实例被标记为属于两个类别中的一个或另一个，SVM训练算法创建一个将新的实例分配给两个类别之一的模型，使其成为非概率二元线性分类器（准确的说，SVM不是线性分类器）。SVM模型是将实例表示为空间中的点，这样映射就使得单独类别的实例被尽可能宽的明显的间隔分开。然后，将新的实例映射到同一空间，并基于它们落在间隔的哪一侧来预测所属类别。See the Pen ML Explained SVM by gangtao (@gangtao) on CodePen.如上图所示，SVM算法就是在空间中找到一条直线，能够最好的分割两组数据。使得这两组数据到直线的距离的绝对值的和尽可能的大。See the Pen ML Explained SVM Kernels by gangtao (@gangtao) on CodePen.上图示意了不同的核方法的不同分类效果。决策树如果要求分类结果是可以解释的，可以考虑决策树或者逻辑回归。决策树（decision tree）是一个树结构（可以是二叉树或非二叉树）。其每个非叶节点表示一个特征属性上的测试，每个分支代表这个特征属性在某个值域上的输出，而每个叶节点存放一个类别。使用决策树进行决策的过程就是从根节点开始，测试待分类项中相应的特征属性，并按照其值选择输出分支，直到到达叶子节点，将叶子节点存放的类别作为决策结果。决策树可以用于回归或者分类，下图是一个分类的例子。See the Pen ML Explained Decision Tree by gangtao (@gangtao) on CodePen.如上图所示，决策树把空间分割成不同的区域。&nbsp;朴素贝叶斯当数据量相当大的时候，朴素贝叶斯方法是一个很好的选择。15年我在公司给小伙伴们分享过bayers方法，可惜speaker deck被墙了，如果有兴趣可以自行想办法。See the Pen ML Explained Naive Bayes by gangtao (@gangtao) on CodePen.如上图所示，大家可以思考一下左下的绿点对整体分类结果的影响。KNNKNN分类可能是所有机器学习算法里最简单的一个了。See the Pen ML Explained KNN by gangtao (@gangtao) on CodePen.如上图所示，K=3，鼠标移动到任何一个点，就找到距离该点最近的K个点，然后，这K个点投票，多数表决获胜。就是这么简单。&nbsp;总结本文利用二维交互图帮助大家理解机器学习的基本算法，希望能增加大家对机器学习的各种方法有所了解。所有的代码可以在参考中找到。欢迎大家来和我交流。&nbsp;参考：代码和演示动画我的Codepen Collection&nbsp;包含了所有的演示代码我的github包含了所有的演示动画基于JavaScript的机器学习的类库和演示Machine learning tools in JavaScript&nbsp;基于JavaScript的机器学习库，本文中的一些演示用到了该库。另一个基于JavaScript的机器学习库，没有前一个功能多，也没有前一个活跃，但是有很好的演示不错的演示，有三种回归和一个聚类如果你像想要自己构建机器学的算法，可以用到的一些数学基础类库Numeric Javascript&nbsp;是基于JavaScript的数值计算和分析的类库，提供线性代数，复数计算等功能。Mathjs&nbsp;另一个基于JavaScript的数学计算库，这个和前一个可以看作是和Python的numpy／scipy／sympy 对应JavaScript的库。Victorjs&nbsp;2D向量库推荐一些机器学习的路线图https://ml-cheatsheet.readthedocs.io/en/latest/10大机器学习算法&nbsp;https://www.gitbook.com/book/wizardforcel/dm-algo-top10http://blogs.sas.com/content/subconsciousmusings/2017/04/12/machine-learning-algorithm-use/http://scikit-learn.org/stable/tutorial/machine_learning_map/index.html工具把mov文件在线转换为动图 https://convertio.co/zh/mov-gif/&nbsp;或者&nbsp;https://cloudconvert.com/mov-to-gifgif 编辑工具&nbsp;https://ezgif.com&nbsp;最后感谢我的朋友Zidong的意见，我对本文做了一些修改。把逻辑回归提前介绍，并加了一些注释，修正了一些不太准确的地方。</div>
                            <div class='commentList'>
                    <h3>评论列表</h3>
                                    </div>
                    </div>
    
        <div class='blog'>
            <h2><a href="#top" class='top'>回到顶部</a><a name="blog_1579204" href="https://my.oschina.net/taogang/blog/1579204">用Python实现一个大数据搜索引擎</a></h2>
            <div class='outline'>
                <div class='date'>时间：2017-11-24 10:49:23</div>
                <div class='catalog'>分类：架构设计</div>
                                                                            </div>
            <div class='content'>搜索是大数据领域里常见的需求。Splunk和ELK分别是该领域在非开源和开源领域里的领导者。本文利用很少的Python代码实现了一个基本的数据搜索功能，试图让大家理解大数据搜索的基本原理。 布隆过滤器 （Bloom Filter） 第一步我们先要实现一个布隆过滤器。 布隆过滤器是大数据领域的一个常见算法，它的目的是过滤掉那些不是目标的元素。也就是说如果一个要搜索的词并不存在与我的数据中，那么它可以以很快的速度返回目标不存在。 让我们看看以下布隆过滤器的代码： class Bloomfilter(object):    """    A Bloom filter is a probabilistic data-structure that trades space for accuracy    when determining if a value is in a set.  It can tell you if a value was possibly    added, or if it was definitely not added, but it can't tell you for certain that    it was added.    """    def __init__(self, size):        """Setup the BF with the appropriate size"""        self.values = [False] * size        self.size = size    def hash_value(self, value):        """Hash the value provided and scale it to fit the BF size"""        return hash(value) % self.size    def add_value(self, value):        """Add a value to the BF"""        h = self.hash_value(value)        self.values[h] = True    def might_contain(self, value):        """Check if the value might be in the BF"""        h = self.hash_value(value)        return self.values[h]    def print_contents(self):        """Dump the contents of the BF for debugging purposes"""        print self.values   基本的数据结构是个数组（实际上是个位图，用1/0来记录数据是否存在），初始化是没有任何内容，所以全部置False。实际的使用当中，该数组的长度是非常大的，以保证效率。  利用哈希算法来决定数据应该存在哪一位，也就是数组的索引  当一个数据被加入到布隆过滤器的时候，计算它的哈希值然后把相应的位置为True  当检查一个数据是否已经存在或者说被索引过的时候，只要检查对应的哈希值所在的位的True／Fasle  看到这里，大家应该可以看出，如果布隆过滤器返回False，那么数据一定是没有索引过的，然而如果返回True，那也不能说数据一定就已经被索引过。在搜索过程中使用布隆过滤器可以使得很多没有命中的搜索提前返回来提高效率。 我们看看这段 code是如何运行的： bf = Bloomfilter(10)bf.add_value('dog')bf.add_value('fish')bf.add_value('cat')bf.print_contents()bf.add_value('bird')bf.print_contents()# Note: contents are unchanged after adding bird - it collidesfor term in ['dog', 'fish', 'cat', 'bird', 'duck', 'emu']:    print '{}: {} {}'.format(term, bf.hash_value(term), bf.might_contain(term)) 结果： [False, False, False, False, True, True, False, False, False, True][False, False, False, False, True, True, False, False, False, True]dog: 5 Truefish: 4 Truecat: 9 Truebird: 9 Trueduck: 5 Trueemu: 8 False 首先创建了一个容量为10的的布隆过滤器  然后分别加入 ‘dog’，‘fish’，‘cat’三个对象，这时的布隆过滤器的内容如下：  然后加入‘bird’对象，布隆过滤器的内容并没有改变，因为‘bird’和‘fish’恰好拥有相同的哈希。  最后我们检查一堆对象（'dog', 'fish', 'cat', 'bird', 'duck', 'emu'）是不是已经被索引了。结果发现‘duck’返回True，2而‘emu’返回False。因为‘duck’的哈希恰好和‘dog’是一样的。  分词&nbsp; 下面一步我们要实现分词。 分词的目的是要把我们的文本数据分割成可搜索的最小单元，也就是词。这里我们主要针对英语，因为中文的分词涉及到自然语言处理，比较复杂，而英文基本只要用标点符号就好了。 下面我们看看分词的代码： def major_segments(s):    """    Perform major segmenting on a string.  Split the string by all of the major    breaks, and return the set of everything found.  The breaks in this implementation    are single characters, but in Splunk proper they can be multiple characters.    A set is used because ordering doesn't matter, and duplicates are bad.    """    major_breaks = ' '    last = -1    results = set()    # enumerate() will give us (0, s[0]), (1, s[1]), ...    for idx, ch in enumerate(s):        if ch in major_breaks:            segment = s[last+1:idx]            results.add(segment)            last = idx    # The last character may not be a break so always capture    # the last segment (which may end up being "", but yolo)        segment = s[last+1:]    results.add(segment)    return results 主要分割 主要分割使用空格来分词，实际的分词逻辑中，还会有其它的分隔符。例如Splunk的缺省分割符包括以下这些，用户也可以定义自己的分割符。 ] &lt; &gt; ( ) { } | ! ; , ' " * \n \r \s \t &amp; ? + %21 %26 %2526 %3B %7C %20 %2B %3D -- %2520 %5D %5B %3A %0A %2C %28 %29 def minor_segments(s):    """    Perform minor segmenting on a string.  This is like major    segmenting, except it also captures from the start of the    input to each break.    """    minor_breaks = '_.'    last = -1    results = set()    for idx, ch in enumerate(s):        if ch in minor_breaks:            segment = s[last+1:idx]            results.add(segment)            segment = s[:idx]            results.add(segment)            last = idx    segment = s[last+1:]    results.add(segment)    results.add(s)    return results 次要分割 次要分割和主要分割的逻辑类似，只是还会把从开始部分到当前分割的结果加入。例如“1.2.3.4”的次要分割会有1，2，3，4，1.2，1.2.3 def segments(event):    """Simple wrapper around major_segments / minor_segments"""    results = set()    for major in major_segments(event):        for minor in minor_segments(major):            results.add(minor)    return results 分词的逻辑就是对文本先进行主要分割，对每一个主要分割在进行次要分割。然后把所有分出来的词返回。 我们看看这段 code是如何运行的： for term in segments('src_ip = 1.2.3.4'):        print term src1.21.2.3.4src_ip311.2.3ip2=4 搜索 好了，有个分词和布隆过滤器这两个利器的支撑后，我们就可以来实现搜索的功能了。 上代码： class Splunk(object):    def __init__(self):        self.bf = Bloomfilter(64)        self.terms = {}  # Dictionary of term to set of events        self.events = []        def add_event(self, event):        """Adds an event to this object"""        # Generate a unique ID for the event, and save it        event_id = len(self.events)        self.events.append(event)        # Add each term to the bloomfilter, and track the event by each term        for term in segments(event):            self.bf.add_value(term)            if term not in self.terms:                self.terms[term] = set()            self.terms[term].add(event_id)    def search(self, term):        """Search for a single term, and yield all the events that contain it"""                # In Splunk this runs in O(1), and is likely to be in filesystem cache (memory)        if not self.bf.might_contain(term):            return        # In Splunk this probably runs in O(log N) where N is the number of terms in the tsidx        if term not in self.terms:            return        for event_id in sorted(self.terms[term]):            yield self.events[event_id]   Splunk代表一个拥有搜索功能的索引集合  每一个集合中包含一个布隆过滤器，一个倒排词表（字典），和一个存储所有事件的数组  当一个事件被加入到索引的时候，会做以下的逻辑       为每一个事件生成一个unqie id，这里就是序号    对事件进行分词，把每一个词加入到倒排词表，也就是每一个词对应的事件的id的映射结构，注意，一个词可能对应多个事件，所以倒排表的的值是一个Set。倒排表是绝大部分搜索引擎的核心功能。      当一个词被搜索的时候，会做以下的逻辑       检查布隆过滤器，如果为假，直接返回    检查词表，如果被搜索单词不在词表中，直接返回    在倒排表中找到所有对应的事件id，然后返回事件的内容      我们运行下看看把： s = Splunk()s.add_event('src_ip = 1.2.3.4')s.add_event('src_ip = 5.6.7.8')s.add_event('dst_ip = 1.2.3.4')for event in s.search('1.2.3.4'):    print eventprint '-'for event in s.search('src_ip'):    print eventprint '-'for event in s.search('ip'):    print event src_ip = 1.2.3.4dst_ip = 1.2.3.4-src_ip = 1.2.3.4src_ip = 5.6.7.8-src_ip = 1.2.3.4src_ip = 5.6.7.8dst_ip = 1.2.3.4 是不是很赞！ 更复杂的搜索 更进一步，在搜索过程中，我们想用And和Or来实现更复杂的搜索逻辑。 上代码： class SplunkM(object):    def __init__(self):        self.bf = Bloomfilter(64)        self.terms = {}  # Dictionary of term to set of events        self.events = []        def add_event(self, event):        """Adds an event to this object"""        # Generate a unique ID for the event, and save it        event_id = len(self.events)        self.events.append(event)        # Add each term to the bloomfilter, and track the event by each term        for term in segments(event):            self.bf.add_value(term)            if term not in self.terms:                self.terms[term] = set()                        self.terms[term].add(event_id)    def search_all(self, terms):        """Search for an AND of all terms"""        # Start with the universe of all events...        results = set(range(len(self.events)))        for term in terms:            # If a term isn't present at all then we can stop looking            if not self.bf.might_contain(term):                return            if term not in self.terms:                return            # Drop events that don't match from our results            results = results.intersection(self.terms[term])        for event_id in sorted(results):            yield self.events[event_id]    def search_any(self, terms):        """Search for an OR of all terms"""        results = set()        for term in terms:            # If a term isn't present, we skip it, but don't stop            if not self.bf.might_contain(term):                continue            if term not in self.terms:                continue            # Add these events to our results            results = results.union(self.terms[term])        for event_id in sorted(results):            yield self.events[event_id] 利用Python集合的intersection和union操作，可以很方便的支持And（求交集）和Or（求合集）的操作。 运行结果如下： s = SplunkM()s.add_event('src_ip = 1.2.3.4')s.add_event('src_ip = 5.6.7.8')s.add_event('dst_ip = 1.2.3.4')for event in s.search_all(['src_ip', '5.6']):    print eventprint '-'for event in s.search_any(['src_ip', 'dst_ip']):    print event src_ip = 5.6.7.8-src_ip = 1.2.3.4src_ip = 5.6.7.8dst_ip = 1.2.3.4 &nbsp; 总结 以上的代码只是为了说明大数据搜索的基本原理，包括布隆过滤器，分词和倒排表。如果大家真的想要利用这代码来实现真正的搜索功能，还差的太远。所有的内容来自于Splunk Conf2017。大家如果有兴趣可以去看网上的视频。   视频  Slides</div>
                            <div class='commentList'>
                    <h3>评论列表</h3>
                                    </div>
                    </div>
    
        <div class='blog'>
            <h2><a href="#top" class='top'>回到顶部</a><a name="blog_1608835" href="https://my.oschina.net/taogang/blog/1608835">使用Heapster和Splunk监控Kubernetes运行性能</a></h2>
            <div class='outline'>
                <div class='date'>时间：2018-01-18 02:40:57</div>
                <div class='catalog'>分类：架构设计</div>
                                                                            </div>
            <div class='content'>Kubernetes已经成为容器编排的事实上的王者，连Docker都已经向K8s女王大人低头。对于Kubernetes的cluster的数据收集和监控已经成为IT运维的一个重要话题。我们今天来看一看如何利用Splunk最新的Metrics Store来对Kubernetes的集群进行性能监控。 部署架构 下图是该方案的部署架构，主要包括：   利用Heapster收集K8s的性能数据，包含CPU，Memory，Network，File System等  利用Heapster的Statsd Sink，发送数据到Splunk的Metrics Store  利用Splunk的搜索命令和仪表盘功能对性能数据进行监控   前期准备 前期主要要准备好两件事：   编译最新的Heapster的镜像，并上传到某个公共的Docker镜像仓库，例如docker hub  在Splunk中配置Metrics Store和对应的网络输入（Network Input UDP／TCP）  这里主要要做的选择是Statsd的传输协议用UDP还是TCP。这里我推荐使用TCP。 最新的Heapster代码支持不同的Backend，包含了log, influxdb, stackdriver, gcp monitoring, gcp logging, statsd, hawkular-metrics, wavefront, openTSDB, kafka, riemann, elasticsearch等等。因为Splunk的Metrics Store支持statsd协议，所以可以很容易的和Heapster集成。 首先我们需要利用最新的heapster代码，编译一个容器镜像，因为docker hub上的heapsterd的官方镜像的版本比较旧，并不支持statsd。所以需要自己编译。 mkdir myheapstermkdir myheapster/srcexport GOPATH=myheapstercd myheapster/srcgit clone https://github.com/kubernetes/heapster.gitcd heapstermake container 运行以上的命令来编译最新的heapster镜像。 注意，heapster缺省使用udp协议，如果想要使用tcp，需要修改代码 https://github.com/kubernetes/heapster/blob/master/metrics/sinks/statsd/statsd_client.go&nbsp; func (client *statsdClientImpl) open() error {var err errorclient.conn, err = net.Dial("udp", client.host)if err != nil {glog.Errorf("Failed to open statsd client connection : %v", err)} else {glog.V(2).Infof("statsd client connection opened : %+v", client.conn)}return err} 把udp改成tcp。 我在docker hub上放了两个镜像，分别对应udp版本的tcp版本，大家可以直接使用   naughtytao/heapster-amd64:v1.5.0-beta.3 udp  naughtytao/heapster-amd64:v1.5.0-beta.4 tcp  然后需要在Splunk中配置Metrics Store，参考这个文档  安装配置Heapster 在K8s上部署heapster比较容易，创建对应的yaml配置文件，然后用kubectl命令行创建就好了。 以下是Deployment和Service的配置文件： deployment.yaml apiVersion: extensions/v1beta1kind: Deploymentmetadata:  name: heapster  namespace: kube-systemspec:  replicas: 1  template:    metadata:      labels:        task: monitoring        k8s-app: heapster        version: v6    spec:      containers:      - name: heapster        image: naughtytao/heapster-amd64:v1.5.0-beta.3        imagePullPolicy: Always        command:        - /heapster        - --source=kubernetes:https://kubernetes.default        - --sink=statsd:udp://ip:port?numMetricsPerMsg=1 service.yaml apiVersion: v1kind: Servicemetadata:  labels:    task: monitoring    # For use as a Cluster add-on (https://github.com/kubernetes/kubernetes/tree/master/cluster/addons)    # If you are NOT using this as an addon, you should comment out this line.    kubernetes.io/cluster-service: 'true'    kubernetes.io/name: Heapster  name: heapster  namespace: kube-systemspec:  ports:  - port: 80    targetPort: 8082  selector:    k8s-app: heapster 注意这里deployment的--sink的配置，ip是Splunk的IP或者主机名，port的对应的Splunk的data input的端口号。当使用udp协议的时候，需要配置的numMetricsPerMsg的值比较小，当这个值比较大的时候，会出message too long的error。当使用tcp的时候可以配置较大的数值。 运行 kubectl apply -f *.yaml 来部署heapster 如果正常运行，对应的heapster pod的日志如下 I0117 18:10:56.054746       1 heapster.go:78] /heapster --source=kubernetes:https://kubernetes.default --sink=statsd:udp://ec2-34-203-25-154.compute-1.amazonaws.com:8124?numMetricsPerMsg=10I0117 18:10:56.054776       1 heapster.go:79] Heapster version v1.5.0-beta.4I0117 18:10:56.054963       1 configs.go:61] Using Kubernetes client with master "https://kubernetes.default" and version v1I0117 18:10:56.054978       1 configs.go:62] Using kubelet port 10255I0117 18:10:56.076200       1 driver.go:104] statsd metrics sink using configuration : {host:ec2-34-203-25-154.compute-1.amazonaws.com:8124 prefix: numMetricsPerMsg:10 protocolType:etsystatsd renameLabels:map[] allowedLabels:map[] customizeLabel:0x15fc8c0}I0117 18:10:56.076248       1 driver.go:104] statsd metrics sink using configuration : {host:ec2-34-203-25-154.compute-1.amazonaws.com:8124 prefix: numMetricsPerMsg:10 protocolType:etsystatsd renameLabels:map[] allowedLabels:map[] customizeLabel:0x15fc8c0}I0117 18:10:56.076272       1 heapster.go:202] Starting with StatsD SinkI0117 18:10:56.076281       1 heapster.go:202] Starting with Metric SinkI0117 18:10:56.090229       1 heapster.go:112] Starting heapster on port 8082 在Splunk中进行监控 好了如果一切正常的化，heapster会用statsd的协议和格式发送metrics到Splunk的metrics store。 然后就可以用利用SPL的mstats和mcatalog命令来分析，监控metrics数据了。 以下搜索语句列出所有的Metrics | mcatalog values(metric_name)  以下搜索语句列出整个cluster的CPU使用，我们可以用Area或者Line Chart来可视化搜索结果。 | mstats avg(_value) WHERE metric_name=cluster.cpu/usage_rate span=30m  kube-system namespace的对应内存使用情况 | mstats avg(_value) WHERE metric_name=namespace.kube-system.memory/usage span=30m  大家可以把自己感兴趣的分析结果放在Dashboard中，利用Realtime设置进行监控。  好了，更多的分析选项可以参考Splunk文档。 &nbsp; 参考   https://github.com/DataDog/the-monitor/blob/master/kubernetes/how-to-collect-and-graph-kubernetes-metrics.md  https://kubernetes.io/docs/tasks/debug-application-cluster/resource-usage-monitoring/  https://kubernetes.io/docs/tasks/debug-application-cluster/core-metrics-pipeline/  https://itnext.io/kubernetes-monitoring-with-prometheus-in-15-minutes-8e54d1de2e13  http://docs.splunk.com/Documentation/Splunk/7.0.1/Metrics/GetStarted&nbsp;</div>
                            <div class='commentList'>
                    <h3>评论列表</h3>
                                    </div>
                    </div>
    
        <div class='blog'>
            <h2><a href="#top" class='top'>回到顶部</a><a name="blog_1627590" href="https://my.oschina.net/taogang/blog/1627590">一个利用Tensorflow求解几何问题的例子</a></h2>
            <div class='outline'>
                <div class='date'>时间：2018-03-02 09:44:49</div>
                <div class='catalog'>分类：机器学习</div>
                                                                            </div>
            <div class='content'>知乎上有一个问题，内容是已知空间三个点的坐标，求三个点所构成的圆的圆心坐标（编程实现）？ 根据圆的定义，这道题的核心就是找到一个点，到已知的三个点的距离相等，利用数学知识可以求解如下：   例如 ：给定a（x1,y1） b（x2,y2） c（x3,y3）求外接圆心坐标O（x，y） 1. 首先，外接圆的圆心是三角形三条边的垂直平分线的交点，我们根据圆心到顶点的距离相等，可以列出以下方程： &nbsp; &nbsp; &nbsp;&nbsp; (x1-x)*(x1-x)+(y1-y)*(y1-y)=(x2-x)*(x2-x)+(y2-y)*(y2-y); &nbsp; &nbsp; &nbsp;&nbsp; (x2-x)*(x2-x)+(y2-y)*(y2-y)=(x3-x)*(x3-x)+(y3-y)*(y3-y); 2.化简得到： &nbsp; &nbsp; &nbsp; &nbsp; 2*(x2-x1)*x+2*(y2-y1)y=x2^2+y2^2-x1^2-y1^2; &nbsp; &nbsp; &nbsp; &nbsp; 2*(x3-x2)*x+2*(y3-y2)y=x3^2+y3^2-x2^2-y2^2; &nbsp; &nbsp; &nbsp; &nbsp; 令:A1=2*(x2-x1)； &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; B1=2*(y2-y1)； &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; C1=x2^2+y2^2-x1^2-y1^2; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; A2=2*(x3-x2)； &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; B2=2*(y3-y2)； &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; C2=x3^2+y3^2-x2^2-y2^2; &nbsp; &nbsp; &nbsp; &nbsp; 即:A1*x+B1y=C1; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; A2*x+B2y=C2; 3.最后根据克拉默法则： &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; x=((C1*B2)-(C2*B1))/((A1*B2)-(A2*B1))； &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; y=((A1*C2)-(A2*C1))/((A1*B2)-(A2*B1))；  当然，我们今天不是来学习数学公式和数学推导的。Tensorflow是google开源的一款深度学习的工具，其实我们可以利用Tensoflow提供了强大的数学计算能力来求解类似的数学问题。 这道题，我们可以利用梯度下降算法，因为圆心是一个最优解，任何其它点都不满条件。（前提是这三个点不在一条直线上，否则是没有解的） 好了，我们先看代码先，然后在解释。 import tensorflow as tfimport numpy# Parameterslearning_rate = 0.1training_epochs = 3000display_step = 50# Training Data, 3 points that form a triangeltrain_X = numpy.asarray([3.0,6.0,9.0])train_Y = numpy.asarray([7.0,9.0,7.0])# tf Graph InputX = tf.placeholder("float")Y = tf.placeholder("float")# Set vaibale for centercx = tf.Variable(3, name="cx",dtype=tf.float32)cy = tf.Variable(3, name="cy",dtype=tf.float32)# Caculate the distance to the center and make them as equal as possibledistance = tf.pow(tf.add(tf.pow((X-cx),2),tf.pow((Y-cy),2)),0.5)mean = tf.reduce_mean(distance)cost = tf.reduce_sum(tf.pow((distance-mean),2)/3)# Gradient descentoptimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(cost)# Initialize the variables (i.e. assign their default value)init = tf.global_variables_initializer()# Start trainingwith tf.Session() as sess:    sess.run(init)    # Fit all training data    for epoch in range(training_epochs):        sess.run(optimizer, feed_dict={X: train_X, Y: train_Y})        c = sess.run(cost, feed_dict={X: train_X, Y:train_Y})        if (c - 0) &lt; 0.0000000001:            break        #Display logs per epoch step        if (epoch+1) % display_step == 0:            c = sess.run(cost, feed_dict={X: train_X, Y:train_Y})            m = sess.run(mean, feed_dict={X: train_X, Y:train_Y})            print "Epoch:", '%04d' % (epoch+1), "cost=", "{:.9f}".format(c), \                "CX=", sess.run(cx), "CY=", sess.run(cy), "Mean=", "{:.9f}".format(m)    print "Optimization Finished!"    training_cost = sess.run(cost, feed_dict={X: train_X, Y: train_Y})    print "Training cost=", training_cost, "CX=", round(sess.run(cx),2), "CY=", round(sess.run(cy),2), "R=", round(m,2), '\n' 运行以上的python代码，结果如下： Epoch: 0050 cost= 0.290830940 CX= 5.5859795 CY= 2.6425467 Mean= 5.657848835Epoch: 0100 cost= 0.217094064 CX= 5.963002 CY= 3.0613017 Mean= 5.280393124Epoch: 0150 cost= 0.173767462 CX= 5.997781 CY= 3.5245996 Mean= 4.885882378Epoch: 0200 cost= 0.126330480 CX= 5.9999194 CY= 4.011508 Mean= 4.485837936Epoch: 0250 cost= 0.078660280 CX= 5.9999976 CY= 4.4997787 Mean= 4.103584766Epoch: 0300 cost= 0.038911112 CX= 5.9999976 CY= 4.945466 Mean= 3.775567770Epoch: 0350 cost= 0.014412695 CX= 5.999998 CY= 5.2943544 Mean= 3.535865068Epoch: 0400 cost= 0.004034557 CX= 5.999998 CY= 5.5200934 Mean= 3.390078306Epoch: 0450 cost= 0.000921754 CX= 5.999998 CY= 5.6429324 Mean= 3.314131498Epoch: 0500 cost= 0.000187423 CX= 5.999998 CY= 5.7023263 Mean= 3.278312683Epoch: 0550 cost= 0.000035973 CX= 5.999998 CY= 5.7292333 Mean= 3.262284517Epoch: 0600 cost= 0.000006724 CX= 5.999998 CY= 5.7410445 Mean= 3.255288363Epoch: 0650 cost= 0.000001243 CX= 5.999998 CY= 5.746154 Mean= 3.252269506Epoch: 0700 cost= 0.000000229 CX= 5.999998 CY= 5.7483506 Mean= 3.250972748Epoch: 0750 cost= 0.000000042 CX= 5.999998 CY= 5.749294 Mean= 3.250416517Epoch: 0800 cost= 0.000000008 CX= 5.999998 CY= 5.749697 Mean= 3.250178576Epoch: 0850 cost= 0.000000001 CX= 5.999998 CY= 5.749871 Mean= 3.250076294Epoch: 0900 cost= 0.000000000 CX= 5.999998 CY= 5.7499437 Mean= 3.250033140Optimization Finished!Training cost= 9.8869656e-11 CX= 6.0 CY= 5.75 R= 3.25  经过900多次的迭代，圆心位置是（6.0，5.75），半径是3.25。 # Parameterslearning_rate = 0.1training_epochs = 3000display_step = 50   learning_rate 是梯度下降的速率，这个值越大，收敛的越快，但也有可能会错过最优解  training_epochs是学习迭代的次数  display_step是每多少次迭代显示当前的计算结果  # Training Data, 3 points that form a triangeltrain_X = numpy.asarray([3.0,6.0,9.0])train_Y = numpy.asarray([7.0,9.0,7.0])# tf Graph InputX = tf.placeholder("float")Y = tf.placeholder("float")# Set vaibale for centercx = tf.Variable(3, name="cx",dtype=tf.float32)cy = tf.Variable(3, name="cy",dtype=tf.float32)   train_X,train_Y是三个点的x，y坐标，这里我们选了（3，7）（6，9）（9，7）三个点  X，Y是计算的输入，在计算过程中我们会使用训练数据输入X，Y  cx，cy是我们想要找的圆心点，初始值设置为（3，3），一般的学习算法会使用随机的初始值，这里我选了三角形中的一个点，这样做一般会减少迭代的次数。  # Caculate the distance to the center and make them as equal as possibledistance = tf.pow(tf.add(tf.pow((X-cx),2),tf.pow((Y-cy),2)),0.5)mean = tf.reduce_mean(distance)cost = tf.reduce_sum(tf.pow((distance-mean),2)/3)# Gradient descentoptimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(cost) 这几行代码是算法的核心。   distance是利用两个点的距离公式算出三个点到圆心的距离  mean是三个距离的平均值  cost是三个距离的方差，我们的目标是让三个点到圆心的距离一样，也就是方差最小（cx/cy为圆心的时候，这个方差为零）  optimizer是梯度下降的训练函数，目标是使得cost（方差）最小  下面就是训练的过程了： # Start trainingwith tf.Session() as sess:    sess.run(init)    # Fit all training data    for epoch in range(training_epochs):        sess.run(optimizer, feed_dict={X: train_X, Y: train_Y})        c = sess.run(cost, feed_dict={X: train_X, Y:train_Y})        if (c - 0) &lt; 0.0000000001:            break        #Display logs per epoch step        if (epoch+1) % display_step == 0:            c = sess.run(cost, feed_dict={X: train_X, Y:train_Y})            m = sess.run(mean, feed_dict={X: train_X, Y:train_Y})            print "Epoch:", '%04d' % (epoch+1), "cost=", "{:.9f}".format(c), \                "CX=", sess.run(cx), "CY=", sess.run(cy), "Mean=", "{:.9f}".format(m)    print "Optimization Finished!"    training_cost = sess.run(cost, feed_dict={X: train_X, Y: train_Y})    print "Training cost=", training_cost, "CX=", round(sess.run(cx),2), "CY=", round(sess.run(cy),2), "R=", round(m,2), '\n'   初始化tf的session  开始迭代  计算cost值，当cost小于一定的值的时候，推出迭代，说明我们已经找到了圆心  最后打印出训练的结果  原题目是空间上的点，我的例子是平面上的点，其实没有本质差别。可以加一个Z轴的数据。这个题，三维其实是多余的，完全可以把空间上的三个点投影到平面上来解决。 我用JS实现过另一个算法，但是不是总收敛。大家可以参考着看一看：  其中，绿色三个点条件点，红色的圆是最终的学习结果，黄色的中心点学习的轨迹。 利用这个例子，我想说的是：   Tensorflow不仅仅是一个深度学习的工具，它提供了强大的数据计算能力，可以用于解决很多的数学问题  机器学习的本质是通过一组数据来找到答案，这也是数学的作用，所以很多的数学问题都可以用机器学习的思路来解决。  &nbsp;</div>
                            <div class='commentList'>
                    <h3>评论列表</h3>
                                    </div>
                    </div>
    
        <div class='blog'>
            <h2><a href="#top" class='top'>回到顶部</a><a name="blog_1790365" href="https://my.oschina.net/taogang/blog/1790365">在浏览器中进行深度学习：TensorFlow.js (一）基本概念</a></h2>
            <div class='outline'>
                <div class='date'>时间：2018-04-06 03:17:11</div>
                <div class='catalog'>分类：机器学习</div>
                                                                            </div>
            <div class='content'>作为deeplearn.js的继任者，tensoflow.js&nbsp;支持在浏览器或者nodejs中利用JavaScript来运行深度学习。并且能够支持GPU和现有的Tensorflow的模型。该项目的首页上有几个很酷炫的演示。作为热爱机器学习和前端数据可视化的我怎能不心动呢。快来和我一起来看看如何利用tensorflow.js来进行深度学习之旅吧。 线性代数（Linear algebra）是深度学习的数学基础。张量（Tensor）是线性代数的基本数学概念和运算单元。我们来了解一下tensorflow.js中有关张量的基本概念和运算。 张量的概念 标量 Scalar 标量就是一个简单的数字，概念就是只有大小，没有方向。 const scalar = tf.scalar(5);print_tensor([scalar]);   注意：print_tensor()是我实现的一个JS方法，在浏览器中以表格的形式显示张量的内容，最高两个维度。输入是一个张量的数组。如果不想在浏览器中看到结果，也可以调用tensor.print()方法，在console中查看张量的内容。   以上的代码，创建了一个数值为5，维度（Rank）0 的标量。 向量／矢量 Vector 向量就是一个一维数组，概念就是有大小和方向。 const vector = tf.tensor1d([0, 1, 2, 3, 4]);print_tensor([vector]);  以上的代码，创建了一个数值为[0,1,2,3,4]，维度&nbsp;1&nbsp;的向量。形状（Shape）描述了各个维度的容量或者说大小，这里是5. 张量 Tensor 矩阵（Matrix）拥有两个维度，在更高的维度，就是张量。其实所有的标量，矢量，矩阵都可以用张量表示，只是维度不同。 在Tensoflow中Rank表示了维度的等级，对应如下      标量 0&nbsp; tf.scalar   矢量 1&nbsp; &nbsp;tf.tensor1d   矩阵 2&nbsp; tf.tensor2d   张量 2+ tf.tensor3d, tf.tensor4d    const matrix = tf.tensor2d([[1.0, 2.0, 3.0], [10.0, 20.0, 30.0]]);print_tensor([matrix]);  以上的代码，创建了一个数值为[[1.0, 2.0, 3.0], [10.0, 20.0, 30.0]]，维度 2&nbsp;的矩阵。形状是2X3，两行，三列。 一张图来直观说明：  tensorflow.js提供了对张量的创建和生成，变形，切片和合并等方法，用于对数据的准备。 张量的运算 张量-标量运算 张量和标量之间的数学运算（加减乘除幂）比较简单，只要直接把张量的每一个元素和标量进行对应的数学运算就好了。 const t = tf.tensor([[1.0, 2.0, 3.0], [10.0, 20.0, 30.0]]);const s = tf.scalar(3);print_tensor([t,s,t.add(s),t.mul(s)])  张量和标量的数学运算不会改变张量的形状，并且满足算术运算的基本定律，例如加法交换律，乘法结合律等等。 张量-张量加减乘 具有相同形状的张量可以进行加减操作。只要把对应的元素做加减就好了。 const a = tf.tensor1d([1, 2, 3, 4]);const b = tf.tensor1d([10, 20, 30, 40]);print_tensor([a,b,a.add(b),a.sub(b),a.mul(b)])  张量-张量 点积 张量的积 tensor.matMul() 在深度学习中被大量的应用，下图给出了一个矩阵点积的例子：  简单说一个m*n的矩阵和一个n*k的矩阵的点积得到一个m*k的矩阵。 const a = tf.tensor2d([[1, 2], [3, 4]]);const b = tf.tensor2d([[5, 6, 7], [7, 8, 9]]);print_tensor([a, b, a.matMul(b)]);    注意，矩阵的点积不满足乘法交换律，AB != BA const a = tf.tensor2d([[1, 2], [3, 4]]);const b = tf.tensor2d([[5, 6], [7, 8]]);print_tensor([a, b, a.matMul(b), b.matMul(a)]);    矩阵转置&nbsp; (Transpose) 矩阵的转置就是将矩阵的所有元素绕着一条从第1行第1列元素出发的右下方45度的射线作镜面反转，即得到矩阵的转置。  const a = tf.tensor2d([[1, 2], [3, 4],[5,6]]);print_tensor([a, a.transpose()]);    单位矩阵（Identity Matrix） 就像数学中的1，任何数字和1相乘都是这个数本身。单位矩阵的特性就是任何矩阵和单位矩阵的点积的结果都是原矩阵本身。单位矩阵就是一个n*n的矩阵对角线上的元素都是1，其它为0。  利用tf.oneHot() 可以生成一个单位矩阵。 const identity = tf.oneHot(tf.tensor1d([0, 1, 2]),  3);print_tensor([identity]);    我们以后再去研究利用tf.initializers.identity来初始化单位矩阵。  矩阵的逆&nbsp; (Inverse) 如果一个矩阵和另一矩阵相乘得到单位矩阵，那么这个矩阵就是该矩阵的逆。 tensorflow 提供了矩阵的逆操作，tf.matrix_inverse 然而tensorflow.js中并没有对应的操作。（考虑到当前的版本是0.6.1，我们就忍了。另外据说深度学习并没有用到任何矩阵的逆操作。） 注意，并不是所有的矩阵都是可逆的，有些矩阵找不到逆矩阵。 进一步了解矩阵的逆可以参考这篇文章 两个矩阵的除法可以理解为一个矩阵和另一个矩阵的逆的点积。 Reduction 深度学习中常常需要对张量各个维度进行统计。例如求均值，总和等等。tensorflow.js提供了相应的操作。 const x = tf.tensor2d([[1, 2, 3],[4,5,6]]);print_tensor([x, x.mean(),x.max(),x.min(), x.sum()]);    这里给出了一个矩阵的所有元素的均值，最大值，最小值和总和，也可以针对行和列做相应的操作。 下面的代码对于每一列做相应的统计。 const x = tf.tensor2d([[1, 2, 3],[4,5,6]]);print_tensor([x, x.mean(0),x.max(0),x.min(0), x.sum(0)]);  除了上面提到的操作，tensorflow.js还提供了：   基本的数学运算诸如三角函数，对数，开方等  逻辑操作  ... ...  总结 本文描述了张量的基本概念和操作，并给出了tensorflow.js中的代码例子。线性代数作为机器学习，尤其是深度学习的数学基础，值得大家去了解和掌握。 本文中所有的代码示例大家都可以在我的codepen中运行。 参考   Tensor PlayGround  可汗学院线性代数  Beginner Tensorflowjs Examples in Javascript&nbsp;  Tensorflow.js官网  Tensorflow.js代码  11 Javascript Machine Learning Libraries To Use In Your&nbsp;App</div>
                    </div>
    
        <div class='blog'>
            <h2><a href="#top" class='top'>回到顶部</a><a name="blog_1793835" href="https://my.oschina.net/taogang/blog/1793835">在浏览器中进行深度学习：TensorFlow.js (二）第一个模型，线性回归</a></h2>
            <div class='outline'>
                <div class='date'>时间：2018-04-11 11:27:25</div>
                <div class='catalog'>分类：机器学习</div>
                                                                            </div>
            <div class='content'>笔者在上一篇文章中介绍了TensorFlow.js中的基本概念，以及机器学习的数学基础，线性代数的基本知识。在这一遍文章里，我们来看一看如何利用TensorFlow.js来构建数学模型，以及进行学习的基本过程。 学习的过程基本如下：   准备训练数据  构建一个模型  利用训练数据和模型，进行迭代的学习  模型训练完毕，用这个模型对新的数据进行预测（这里我们先略过对模型的验证部分）  好了，我们以最简单的线性回归为例子，看看这个过程。 准备数据  如上图所示，我在二维坐标系中生成了7个点，让它们在我假想的某条直线附近。我以这几个点作为我的训练数据。 训练数据的初始化代码如下，这里tx是所有点数据的x坐标，ty是所有点数据的坐标。 const train_x = tf.tensor1d(tx);const train_y = tf.tensor1d(ty); 模型选择   所有的模型都是错的，有的模型更好。  所谓的模型，也就是一个函数f，对应于某个输入数据，计算出某些输出数据。模型可以复杂，可以简单。简单的模型不一定不好，负责的模型也不一定好。 我们用线性模型举例，数学上就是假定 Y = wX + b 在这个模型中，有两个参数需要确定，w和b。 模型既然是个函数，那么它的代码也就很容易理解了： const f = x =&gt; w.mul(x).add(b); 当然你也可以这样写： const f = function(x){  return w.mul(x).add(b);  }} 迭代学习 学习的过程我们称作训练，训练通常是一个迭代的过程，这个过程中，通常需要这几样东西：   一个损失函数（loss function），损失函数定义了模型是不是足够好，通常loss越小越好。  一个优化器 （optimizer），优化器通过某种算法来决定如何改变参数的值，使得损失函数最小化。  迭代循环， 通过循环 -&gt; 调用优化器，得到新的参数，计算损失, 最终当损失足够小时，可以认为训练结束了。  训练代码如下： 初始化参数，这里使用随机数来作为参数的初始值。（注意，初始参数并不总是随机选择的。） const w = tf.variable(tf.scalar(Math.random()));const b = tf.variable(tf.scalar(Math.random()));  初始化学习参数，   numIterations是迭代的次数，一般次数越多，模型的拟合就越好，但是就需要花费越多的计算  learningRate是学习率，这个值越大，学的速度就越快，但是也会更加容易错过极值点。  const numIterations = 200;const learningRate = 1; 选择一个优化器，这里我选择了adam。TensorFlow.js提供了多种优化器，例如sgd，momentum等等，大家可以根据自己的需要来选择。 const optimizer = tf.train.adam(learningRate); 对于损失函数，我们采用的是均方差&nbsp;  const loss = (pred, label) =&gt; pred.sub(label).square().mean(); 或者可以写作： function loss(predictions, labels) {  const meanSquareError = predictions.sub(labels).square().mean();  return meanSquareError;} 然后就是训练的过程啦： for (let iter = 0; iter &lt; numIterations; iter++) {    optimizer.minimize(() =&gt; {      const loss_var = loss(f(train_x), train_y);      loss_var.print();      return loss_var;    })} 在训练过程中，我们调用tensor的print()方法打印出损失的值，看看训练过程是不是收敛。当选择的模型，参数，优化器不合适的时候，有可能训练过程并不收敛。 训练的结果我们就等到了w和b的值。也就是确定了直线的斜率和截距。 大家可以尝试我的演示代码  我们可以看到学习过程中是如何慢慢收敛到最后的结果的直线。 总结 本文描述了一个使用tensoflow.js来进行最简单的线性回归模型的学习的过程。希望大家可以通过这个简单的例子了解机器学习的基本思路。 参考   在浏览器中进行深度学习：TensorFlow.js (一）基本概念</div>
                    </div>
    
        <div class='blog'>
            <h2><a href="#top" class='top'>回到顶部</a><a name="blog_1797230" href="https://my.oschina.net/taogang/blog/1797230">在浏览器中进行深度学习：TensorFlow.js (三）更多的基本模型</a></h2>
            <div class='outline'>
                <div class='date'>时间：2018-04-18 13:25:26</div>
                <div class='catalog'>分类：机器学习</div>
                                                                            </div>
            <div class='content'>在上一次的博客中，我们介绍了如果实现一个最简单的线性回归的模型，今天我们来看一下，如何利用同样的思路实现更多的模型。 逻辑回归 逻辑回归并非只能实现二分类，我们下面就看一个利用逻辑回归（Multinomial logistic regression）实现多分类的例子。 这个是训练数据：  这个是分类的结果。我们可以看到对某些点，蓝色和橙色，分类效果比较好；而对于绿色和红色的点，分类的结果不是很理想。  代码在这里： function logistic_regression(train_data, train_label) {    const numIterations = 100;  const learningRate = 0.1;  const optimizer = tf.train.adam(learningRate);  //Caculate how many category do we have  const number_of_labels = Array.from(new Set(train_label)).length;  const number_of_data = train_label.length;    const w = tf.variable(tf.zeros([2,number_of_labels]));  const b = tf.variable(tf.zeros([number_of_labels]));    const train_x = tf.tensor2d(train_data);  const train_y = tf.tensor1d(train_label);    function predict(x) {    return tf.softmax(tf.add(tf.matMul(x, w),b));  }  function loss(predictions, labels) {    const y = tf.oneHot(labels,number_of_labels);    const entropy = tf.mean(tf.sub(tf.scalar(1),tf.sum(tf.mul(y, tf.log(predictions)),1)));    return entropy;  }    for (let iter = 0; iter &lt; numIterations; iter++) {    optimizer.minimize(() =&gt; {      const loss_var = loss(predict(train_x), train_y);      loss_var.print();      return loss_var;    })      }} 逻辑回归和之前的线性回归的过程基本类似，有几个要注意的地方：   训练数据       train_x是每一个2维矩阵，每一个点数据就是一个由x,y坐标组成的二元数组 [x,y]    train_y是每一点的分类，从0开始      预测模型， 对于softmax这个模型，简单说，就是二元逻辑回归向更多元素向量的扩展。有兴趣进一步了解的可以去看下面的这两篇文章：        关于softmax和逻辑回归的关系    softmax    知乎关于softmax的问答      损失，损失函数用交叉熵，可以参考这篇文章。        这里在计算损失的时候，对于lable，调用tf.oneHot()方法，把对Label数据变换为以下形式 // LabelsTensor    [0, 0, 1, 1, 2, 2]// OneHotTensor    [[1, 0, 0],     [1, 0, 0],     [0, 1, 0],     [0, 1, 0],     [0, 0, 1],     [0, 0, 1]] &nbsp;       K近邻 K近邻是一个特别简单的算法，简单到没有训练的过程。大家在我的另一篇关于机器学习的博客里，可以找到对这个算法的可视化介绍。 利用TensorflowJS也可以实现该算法。下面的代码使用L1距离来实现近邻算法（k=1） function knn(train_data,train_label) {  const train_x = tf.tensor2d(train_data);    return function(x) {    var result = [];    x.map(function(point){      const input_tensor = tf.tensor1d(point);      const distance = tf.sum(tf.abs(tf.sub(input_tensor, train_x)),1);      const index = tf.argMin(distance, 0);      result.push(train_label[index.dataSync()[0]]);    });    return result;  };} 参考：机器学习下的各种norm到底是个什么东西？ 这个算法虽然简单，但是计算量不小，预测的效果似乎也不比逻辑回归差呢。我们注意类似的数据和逻辑回归的差异。  某个数学题 同样的，对于一般的学习问题，TensorflowJS自然是不在话下。参考笔者的另一篇博客：一个利用Tensorflow求解几何问题的例子。 代码如下： function train(train_data) {  const numIterations = 200;  const learningRate = 0.05;  const optimizer = tf.train.sgd(learningRate);    const training_data = tf.tensor2d(train_data);  const center = tf.variable(tf.tensor1d([Math.random()* Math.floor(domain_max),Math.random()* Math.floor(domain_max)]));    // Caculate the distance of this center point to the each point in the training data  const distance = function() {    return tf.pow(tf.sum(tf.pow(tf.sub(training_data, center),tf.scalar(2)),1),tf.scalar(1/2));  }  // Mean Square Error  const loss = function(dis) {    return tf.sum(tf.pow(tf.sub(dis,tf.mean(dis)),tf.scalar(2)));  }  for (let iter = 0; iter &lt; numIterations; iter++) {    var result = {};    optimizer.minimize(() =&gt; {      const loss_var = loss(distance());      loss_var.print();      result.loss = loss_var.dataSync();      return loss_var;    })  }  return center;} 运行效果如下：  &nbsp; 参考   在浏览器中进行深度学习：TensorFlow.js (一）基本概念  在浏览器中进行深度学习：TensorFlow.js (二）第一个模型，线性回归  一个利用Tensorflow求解几何问题的例子  图解机器学习  Codepen演示代码</div>
                    </div>
    
        <div class='blog'>
            <h2><a href="#top" class='top'>回到顶部</a><a name="blog_1803051" href="https://my.oschina.net/taogang/blog/1803051">在浏览器中进行深度学习：TensorFlow.js (四）用基本模型对MNIST数据进行识别</a></h2>
            <div class='outline'>
                <div class='date'>时间：2018-04-29 09:55:02</div>
                <div class='catalog'>分类：机器学习</div>
                                                                            </div>
            <div class='content'>在了解了TensorflowJS的一些基本模型的后，大家会问，这究竟有什么用呢？我们就用深度学习中被广泛使用的MINST数据集来进行一下手写识别的操作。 MINST数据集 MINST是一组0到9的手写数字。就像这个：  这组数据出现在各种深度学习的入门和例子中，有点像传统机器学习中的Iris数据集。被各种使用。 TensorflowJS提供了一个关于训练MINST数据集的例子。  为了便于重用，笔者把其中提供数据的代码提取出来。大家可以参考。该部分主要功能有：   class MnistData（）提供数据类  load（）对Minst数据的异步加载  nextTrainBatch（batchSize）从训练数据集中随机返回batchSize的数据，虽然MINST数据集是有限的，但是调用nextBatch总是能返回数据。  nextTestBatch（batchSize）从测试数据集中随机返回batchSize的数据    let data;  async function load() {    data = new MnistData();    await data.load();  }  async function mnist() {    await load();    console.log("Data loaded!");  }  mnist().then(function(){    const train_data = data.nextTrainBatch(batch_size);    // 使用train_data数据  });  返回的train_data的格式如下：   train_data.xs 是shape为[16, 784]的张量，16是数据的个数，784 = 28*28，是二维图像打平后的数据。  train_data.labels是shape为[16, 10]的张量，16是数据的个数，每一个数据标签是一个有是个数值的向量。分别对应0-9  请参考的我的代码演示：  逻辑回归 在我的前一篇文章《在浏览器中进行深度学习：TensorFlow.js (三）更多的基本模型》中，我们使用逻辑回归演示了对空间二维数据点的分类算法。这一次我就拿MINST数据集来试一试。 训练算法代码如下： function logistic_regression(train_data) {  const batch_size = 1000;  const numIterations = 1000;  const number_of_labels = 10;  let loss_results = [];    const learningRate = 0.15;  const optimizer = tf.train.sgd(learningRate);    const w = tf.variable(tf.zeros([784,number_of_labels]));  const b = tf.variable(tf.zeros([number_of_labels]));    function predict(x) {    return tf.softmax(tf.add(tf.matMul(x, w),b));  }  function loss(predictions, labels) {    const entropy = tf.mean(tf.sub(tf.scalar(1),tf.sum(tf.mul(labels, tf.log(predictions)),1)));    return entropy;  }    for (let iter = 0; iter &lt; numIterations; iter++) {    const batch = train_data.nextTrainBatch(batch_size);    const train_x = batch.xs;    const train_y = batch.labels;    optimizer.minimize(() =&gt; {      const loss_var = loss(predict(train_x), train_y);      loss_results.push({          x:new Date().getTime(),           y:loss_var.dataSync()[0]      });      return loss_var;    })     train_x.dispose();    train_y.dispose();  }    return {     model : predict,    loss : loss_results  };} 大家可以比较一下和之前的例子的区别，可以发现几个小的差异：   之前的例子数据和数据的标签是分开的，这里数据和标签在train_data的xs和labels属性中。  逻辑回归的连个变量w和b的shape不一样，因为这些变量要和问题域的数据形状保持一致。  loss_result用于训练完成后返回训练过程中损失的变化。  调用dispose方法释放资源。  请参考的我的代码演示：  BatchSize=16，迭代100次，准确率为0.9。白底黑字的是预测错误的数据。 这个是验证准确率的代码： const test_batch = data.nextTestBatch(100);const prediction = train_result.model(test_batch.xs);const correct_prediction = tf.equal(tf.argMax(prediction,1), tf.argMax(test_batch.labels, 1));const accuracy = tf.mean(tf.cast(correct_prediction,'float32')); 过程入下：   从测试数据中取100个点  对这100点做预测  计算有哪些点是预测正确的。tf.argMax(prediction,1) 返回预测结果在维度1上最大值所在的索引。因为对于标签而言，[1,0,0,0,0,0,0,0,0,0]表示0，[0,1,0,0,0,0,0,0,0,0]表示1，所以最大值的索引，其实就是1的位置，也就是预测的结果。tf.equal检查连个张量是不是相等。  计算正确预测的数据的均值，也就是准确率。   增大迭代次数，准确率并没有提高。  增大BatchSize对结果也没有改善。 测试下来，逻辑回归的算法应用在MINST数据集上准确率在90%左右。 近邻算法 同样的我们也试一下近邻算法在MINST数据上的分类效果。 function nearest_neighbers(train_data) {  const train_xs = train_data.xs;  const train_labels = train_data.labels;  let results;    return function(x) {    for (let i = 0; i &lt; x.shape[0]; i++) {      const xs = x.slice([i, 0], [1, x.shape[1]]);      const distance = tf.sum(tf.abs(tf.sub(xs, train_xs)),1);      const label_index = tf.argMin(distance, 0);      if (results ) {        results = results.concat(train_labels.gather(label_index.expandDims(0)));      } else {        results = train_labels.gather(label_index.expandDims(0));      }    }        return results;  };} 该算法采用L1距离来预测，简单的说就是计算目标张量离训练数据哪一个近，就分为那个类。 这里：   对每一输入数据，计算和训练数据各个点的距离，找到最近的点，可以理解为找到长得最像的图像。  所有的输入点都计算完后，利用tf.concat方法把结果合并在一个张量中返回。     我们发现，因为不存在迭代，分类的准确率和训练数据的规模关系明显：   100 &gt; 0.71  1000 &gt; 0.88  5000 &gt; 0.92  还有如果你选择训练集size为1你会发现：  如上图所示，只有数字3能被准确分类。我可以肯定，那个训练数据就是3 。 好了，至今，这个系列已经是第四篇了，说好的深度学习呢？作者该不是个骗子吧？请大家稍安勿躁，我保证后面一定有深度学习的内容。&nbsp;  参考   在浏览器中进行深度学习：TensorFlow.js (三）更多的基本模型  在浏览器中进行深度学习：TensorFlow.js (二）第一个模型，线性回归  在浏览器中进行深度学习：TensorFlow.js (一）基本概念</div>
                    </div>
    
        <div class='blog'>
            <h2><a href="#top" class='top'>回到顶部</a><a name="blog_1807936" href="https://my.oschina.net/taogang/blog/1807936">在浏览器中进行深度学习：TensorFlow.js (五）构建一个神经网络</a></h2>
            <div class='outline'>
                <div class='date'>时间：2018-05-07 04:50:37</div>
                <div class='catalog'>分类：机器学习</div>
                                                                            </div>
            <div class='content'>这一次我终于可以开始真正的深度学习了，从一个神经网络开始。 神经网络（Neural Network）是深度学习的基础，基本概念包括：神经元，层，反向传播等等。如果细讲我估计没有五到十篇文章那是讲不完的。简单说它模拟了大脑神经元工作的方式，利用把多个神经元组合成网络结构的模型来对数据进行分类。    神经网络是一个多层结构的反馈网络，包括输入，输出和隐藏层。  每一层由若干个神经元组成。  整个网络利用反向传播，反馈输出的结果和期望值的差异来进行学习。  可以理解网络是一个函数ouput=function(input), 随着网络层次的加深，神经网络可以模拟一个非常复杂的非线性函数，当然学习的成本就更高，因为要学习的参数会随着层数和每一层的神经元的个数增加而增加。  TensorFlowJs提供了对神经网络／深度神经网络提供了很好的支持。包括:模型 tf.model, 层 tf.layer。 下面我们就看看如果利用TensorFlowJS来构建一个简单的神经网络来进行MINST数据的手写识别。 构建网络 function nn_model() {  const model = tf.sequential();  model.add(tf.layers.dense({    units: 32, inputShape: [784]  }));  model.add(tf.layers.dense({    units: 256  }));  model.add(tf.layers.dense(    {units: 10, kernelInitializer: 'varianceScaling', activation: 'softmax'}));  return model;} 以上代码构建了一个有两个隐藏层的神经网络，第一层有32个神经元，第二层有256个神经元。    tf.sequential 构建一个序列化的网络模型，这样的网络每一层的输出连接到下一层的输入，类似一个有每一层组成的栈。不存在分支或者跳跃。    利用model.add向模型中增加一层    tf.layers.dense提供一个全联接的层。units定义了该层的神经元个数。inputShape是输入数据的形状。网络中第一层必须明确指定输入形状，其余的层默认从前面的层输入。    最后一层决定了分类器的结果，所以我们使用softmax作为激活函数，units为10，表示10的数字0-9的分类结果。   网络初始化 const model = nn_model();const LEARNING_RATE = 0.15;const optimizer = tf.train.sgd(LEARNING_RATE);model.compile({  optimizer: optimizer,  loss: 'categoricalCrossentropy',  metrics: ['accuracy'],});   初始化模型，定义学习率，优化器  调用model.compile方法，定义损失函数。  训练网络 async function train() {  const BATCH_SIZE = 16;  const TRAIN_BATCHES = 100;  const TEST_BATCH_SIZE = 100;  const TEST_ITERATION_FREQUENCY = 5;  for (let i = 0; i &lt; TRAIN_BATCHES; i++) {    const batch = data.nextTrainBatch(BATCH_SIZE);    let testBatch;    let validationData;    // Every few batches test the accuracy of the mode.    if (i % TEST_ITERATION_FREQUENCY === 0 &amp;&amp; i &gt; 0 ) {      testBatch = data.nextTestBatch(TEST_BATCH_SIZE);      validationData = [        testBatch.xs.reshape([TEST_BATCH_SIZE, 784]), testBatch.labels      ];    }    // The entire dataset doesn't fit into memory so we call fit repeatedly    // with batches.    const history = await model.fit(        batch.xs.reshape([BATCH_SIZE, 784]), batch.labels,        {batchSize: BATCH_SIZE, validationData, epochs: 1});    batch.xs.dispose();    batch.labels.dispose();    if (testBatch != null) {      testBatch.xs.dispose();      testBatch.labels.dispose();    }    await tf.nextFrame();  }}   训练的核心方法是调用model.fit(x,y,config)方法。x是训练数据，y是训练的分类标签。config是可选项。  在训练过程中，我们使用testBactch来做验证，计算准确率。结果存入model.fit的返回值中。  调用dispose方法释放tensor占用的内存  tf.nextFrame() 返回一个Promise，主要用于Web动画。 static nextFrame(): Promise&lt;void&gt; {  return new Promise&lt;void&gt;(resolve =&gt; requestAnimationFrame(() =&gt; resolve()));} &nbsp;   大家可以试一试我在codepen上的例子。 通过改变模型的层数和每一层神经元的个数，我们可以评估该模型是否有效。  Batch ：16&nbsp; &nbsp;神经元 ：32+256&nbsp; 准确率 ：0.84  Batch ：64&nbsp; &nbsp;神经元 ：32+256&nbsp; 准确率 ：0.92  Batch ：16&nbsp; &nbsp;神经元 ：32+256+256+32&nbsp; 准确率 ：0.75  Batch ：16&nbsp; &nbsp;神经元 ：32+256+256+256&nbsp; 准确率 ：0.11 我们发现网络也并非越深越好，在最后一个4层的例子中，训练的损失很高，效果很差。 在深度学习中如果定义这些超参数（hyperparameter），真的很难。 较大的batchSize效果比较小的要好，但是由于浏览器内存的限制，我们无法加载较大的Batch训练数据。 更多的发现留给大家去尝试。 神经网络的种类有很多，以后有机会我们可以继续了解。  &nbsp; 参考：   知乎：如何简单形象又有趣地讲解神经网络是什么？   阮一峰的网络日志：&nbsp;神经网络入门    Tensorflow Playground    反向传播    史上最全！27种神经网络简明图解：模型那么多，我该怎么选？    在浏览器中进行深度学习：TensorFlow.js (四）用基本模型对MNIST数据进行识别    在浏览器中进行深度学习：TensorFlow.js (三）更多的基本模型    在浏览器中进行深度学习：TensorFlow.js (二）第一个模型，线性回归    在浏览器中进行深度学习：TensorFlow.js (一）基本概念   &nbsp;</div>
                    </div>
    
        <div class='blog'>
            <h2><a href="#top" class='top'>回到顶部</a><a name="blog_1809904" href="https://my.oschina.net/taogang/blog/1809904">基于容器应用设计的原则，模式和反模式</a></h2>
            <div class='outline'>
                <div class='date'>时间：2018-05-10 02:39:32</div>
                <div class='catalog'>分类：架构设计</div>
                                                                            </div>
            <div class='content'>容器和容器编排（Kubernetes）的广泛使用，让我们可以轻松的构建基于微服务的“云原生”（Cloud Native）的应用。容器成为了云时代的新的编程单元，类似面向对象概念下的对象，J2EE中的组件或者函数式编程中的函数。 在面向对象时代，有许多著名的设计原则，模式和反模式等，例如：    SOLID&nbsp;(单一功能、开闭原则、里氏替换、接口隔离以及依赖反转）    Design Patterns: Elements of Reusable Object-Oriented Software   Anti-Pattern  在新的容器背景下，相应的原则和模式有助于帮助我们更好的构建“云原生”的应用。我们可以看到，这些原则和模式并非对之前模式的颠覆和推翻，更像是适应新环境的演进版本。 原则 单一职责原则&nbsp;SINGLE CONCERN PRINCIPLE (SCP) 与OO的单一功能相对应，每一个容器应该提供单一的职责，只关注于做好一件事。单一职责使得容器更容易重用。通常容器对应于一个进程，而该进程专注于做好一件事。  高可观测性原则 HIGH OBSERVABILITY PRINCIPLE (HOP) 容器像对象一样，应该是一个封装良好的黑盒子。但是在云的环境下，这个黑盒子应该提供良好的观测接口，使得其在云的环境下得到相应的监控和管理。这样，整个应用才能提供一致的生命周期的管理。  可观测性包含：   提供健康检查 Health Check，或者心跳  提供状态  把日志输出到标准输出(STDOUT)和标准出错(STDERR)  等等&nbsp;  生命周期确认原则 LIFE-CYCLE CONFORMANCE PRINCIPLE (LCP) 生命周期确认原则指的是容器应该提供和平台交互来处理相应的生命周期的变化。    捕获并响应Terminate (SIGTERM)信号，来尽快优雅的终止服务进程，以避免kill (SIGKILL)信号强行终止进程。例如一下的NodeJS代码。 process.on('SIGTERM', function () {  console.log("Received SIGTERM. Exiting.")  server.close(function () {    process.exit(0);  });}); &nbsp;   返回退出码 process.exit(0);   镜像不可变原则&nbsp;IMAGE IMMUTABILITY PRINCIPLE (IIP) 在运行时，配置可以不同，但是镜像应该是不可变的。  我们可以理解为镜像是个类，是容器是对象实例，类是不变的，而容器是拥有不同配置参数的镜像实例。 进程用完既丢原则 PROCESS DISPOSABILITY PRINCIPLE (PDP) 在云环境下，我们应该假定所有的容器都是临时的，它随时有可能被其它的容器实例所替代。  这也就意味着需要把容器的状态保存在容器之外。并且尽可能快速的启动和终止容器。通常越小的容器就越容易实现这一点。 自包含原则 SELF-CONTAINMENT PRINCIPLE (S-CP) 容器在构建的时候应该包含所有的依赖，也就是所说容器在运行时不应该有任何的外部依赖。  限制运行资源原则 RUNTIME CONFINEMENT PRINCIPLE (RCP) 容器的最佳实践应该是在运行时指定容器对资源配置的需求。例如需要多少的内存，CPU等等。这样做可以使得容器编排能都更有效的调度和管理资源。  模式 许多容器应用的模式和Pod的概念相关，Pod是Kubernetes为了有效的管理容器而提出的概念，它是容器的集合，我们可以理解为“超容器”（我随便发明的）。Pod包含的容器之间就好像运行在同一台机器上，这些容器共享Localhost主机地址，可以本机通信，共享卷等等。  Kubernetes 类似云上OS，提供了用容器构建云原生应用的最佳实践。我们看看这些常见的模式都有什么。 边车（侧斗）（Sidecar）  Sidecar是最常见的模式，在同一个Pod中，我们需要把不同的责任分在不同的容器中，对外部提供一个完整的功能。  这样的例子有很多，例如：   上图中的Node后端和提供缓存的Redis  Web服务器和收集日志的服务  Web服务器和负责监控服务器性能数据的服务  这样做有点类似面向对象的组合模式，好处有很多：    应用单一职责原则，每一个容器只负责专注做好一件事。  隔离，容器之间不会出现互相竞争资源，当一个次要功能（例如日志收集或者缓存）失效或者崩溃的时候，对主要功能的影响降至最小。  可以对每一个容器进行独立的生命周期管理  可以对每一个容器进行独立的弹性扩张  可以方便的替换其中一个容器  代理（大使）容器  类似于面向对象的Proxy模式，利用Pod中一个容器提供对外的访问连接。如下图中Node后端总是通过Service Discovery容器来和外部进行通信。  这样做，负责Node模块开发的只需要假定所有的通信都是来自于本机，而把通信的复杂性交给代理容器，去处理诸如负载均衡，安全，过滤请求，必要时中断通信等功能。 适配器容器  大家常常会把面向对象的Proxy模式，Bridge模式和Adapter模式搞混，因为单单从UML关系图上来看，它们都大同小异。似乎只是取了不同的名字。事实也确实如此，就像几乎所有的OO模式都是组合模式的衍生，所有容器模式都是边车模式的衍生。 在下图的例子中，如果Logging Adapter的名字不提及Adapter，我们不会认为这是个适配器模式。  其实适配器模式关注的是如果把Pod内部的不同容器的功能通过适配器统一的暴漏出来。在上图中，如果我们再多加一个容器，它同时会向卷中写入日志的化，这样就更清楚了。Logging Adapter适配不同容器用不同的接口提供的日志，并提供统一的访问接口。 容器链  类似于OO的责任链模式，把负责不同功能的容器按照依赖顺序链在一起，也是一种常见的模式。  准备就绪的Pod  通常作为服务的容器有一个启动的过程，在启动过程中，服务是不可用的。Kubernetes提供了Readiness探测功能。 readinessProbe:  httpGet:    path: /    port: 5000  timeoutSeconds: 1  periodSeconds: 5 和其它模式相比，这个更像是一个使用Kubernetes的最佳实践。 &nbsp; 反模式 构建环境和运行环境混杂在一起 应该使得用于生产的运行环境的镜像尽可能的小，避免在运行环境的镜像中包含构建时的残留。 例如下面的Dockerfile例子： FROM ubuntu:14.04RUN apt-get updateRUN apt-get install gccRUN gcc hello.c -o /hello 在这个构建的镜像中，有很多不需要也不应该出现在生产环境中的东西，例如gcc，源代码hello.c。这样的结果既不安全（直接暴漏源代码），也会有性能开销（过大的镜像体积导致加载变慢）。 Docker17.05 以后提供的multi-stage builds也可以解决这个问题。 直接使用Pod 避免直接使用Pod，用Deployment来管理Pod。利用Deployment可以很方便的对Pod进行扩展和管理。 使用latest标签 Latest标签用于标记最近的稳定版本，然而在创建容器时，尽可能避免在生产环境使用Latest标签。即使使用imagePullPolicy选项为alway。 快速失败的任务 Job是Kubernetes提供的只运行一次的容器，和service正好相反。要避免快速失败 apiVersion: batch/v1kind: Jobmetadata:  name: badspec:  template:    metadata:      name: bad    spec:      restartPolicy: Never      containers:      - name: box        image: busybox        command: ["/bin/sh", "-c", "exit 1"] 如果你尝试在你的cluster里面创建以上的Job，你可能会碰到如下的状态。 $ kubectl describe jobs Name:badNamespace:defaultImage(s):busyboxSelector:controller-uid=18a6678e-11d1-11e7-8169-525400c83acfParallelism:1Completions:1Start Time:Sat, 25 Mar 2017 20:05:41 -0700Labels:controller-uid=18a6678e-11d1-11e7-8169-525400c83acfjob-name=badPods Statuses:1 Running / 0 Succeeded / 24 FailedNo volumes.Events:  FirstSeenLastSeenCountFromSubObjectPathTypeReasonMessage  ------------------------------------------------------------  1m1m1{job-controller }NormalSuccessfulCreateCreated pod: bad-fws8g  1m1m1{job-controller }NormalSuccessfulCreateCreated pod: bad-321pk  1m1m1{job-controller }NormalSuccessfulCreateCreated pod: bad-2pxq1  1m1m1{job-controller }NormalSuccessfulCreateCreated pod: bad-kl2tj  1m1m1{job-controller }NormalSuccessfulCreateCreated pod: bad-wfw8q  1m1m1{job-controller }NormalSuccessfulCreateCreated pod: bad-lz0hq  1m1m1{job-controller }NormalSuccessfulCreateCreated pod: bad-0dck0  1m1m1{job-controller }NormalSuccessfulCreateCreated pod: bad-0lm8k  1m1m1{job-controller }NormalSuccessfulCreateCreated pod: bad-q6ctf  1m1s16{job-controller }NormalSuccessfulCreate(events with common reason combined) 因为任务快速失败。Kubernetes认为任务没能成功启动，尝试创建新的容器以恢复这个失败，导致的Cluster会在短时间创建大量的容器，这样的结果可能会消耗大量的计算资源。 在Spec中使用.spec.activeDeadlineSeconds来避免这个问题。这个参数定了等待多长时间重试失败的Job。 参考   https://www.redhat.com/cms/managed-files/cl-cloud-native-container-design-whitepaper-f8808kc-201710-v3-en.pdf  https://www.slideshare.net/luebken/container-patterns  https://github.com/luebken/container-patterns  https://kubernetes.io/blog/2015/06/the-distributed-system-toolkit-patterns/  http://docs.projectatomic.io/container-best-practices/  https://www.usenix.org/system/files/conference/hotcloud16/hotcloud16_burns.pdf  https://github.com/gravitational/workshop/blob/master/k8sprod.md</div>
                            <div class='commentList'>
                    <h3>评论列表</h3>
                                    </div>
                    </div>
    
        <div class='blog'>
            <h2><a href="#top" class='top'>回到顶部</a><a name="blog_1811573" href="https://my.oschina.net/taogang/blog/1811573">用500行纯前端代码在浏览器中构建一个Tableau</a></h2>
            <div class='outline'>
                <div class='date'>时间：2018-05-13 12:18:28</div>
                <div class='catalog'>分类：数据可视化</div>
                                                                            </div>
            <div class='content'>在Gartner最新的对商务智能软件的专业分析报告中，Tableau持续领跑。Microsoft因为PowerBI表现出色也处于领导者象限。而昔日的领导者像SAP，SAS，IBM，MicroStrategy等逐渐被拉开了差距。  Tableau因为其灵活，出色的数据表现已经成为BI领域里无可争议的领头羊。而其数据驱动的可视化和核心思想是来自于Leland Wilkinson的The Grammar Of Graphics&nbsp;，同样受到该思想影响的还有R的图形库ggplot。  在数据可视化开源领域里，大家对百度开发的echarts可谓耳熟能详，echarts经过多年的发展，其功能确实非常强大，可用出色来形容。但是蚂蚁金服开源的基于The Grammar Of Graphics的语法驱动的可视化库G2，让人眼前一亮。那我们就看看如何利用G2和500行左右的纯前端代码来实现一个的类似Tableau的数据分析功能。   演示参见&nbsp;https://codepen.io/gangtao/full/OZvedx/  代码参见&nbsp;https://gist.github.com/gangtao/e053cf9722b64ef8544afa371c2daaee&nbsp;  数据加载 第一步是加载数据：  数据加载主要用到了三个库：   axios&nbsp; 基于Promise的HTTP客户端  alasql&nbsp;基于JS的开源SQL数据库  jquery datatable&nbsp;JQuery的数据表格插件  数据通过我存放在GitHub中的csv格式的文件，以REST请求的方式来加载。下面的代码把Axios的Promise变成 async／wait方式。 // Ajax async requestconst request = {  get: url =&gt; {    return new Promise((resolve, reject) =&gt; {      axios        .get(url)        .then(response =&gt; {          resolve({ data: response.data });        })        .catch(error =&gt; {          resolve({ data: error });        });    });  }}; 封装好后，我们就可以用request.get()方法发送REST请求，获取csv文件。 let csv = await request.get(url); 这一步可能会遇到跨域请求的问题，github上的文件支持跨域。 把数据存储在一个SQL数据库中，这样做的好处是为了下一步做数据准备的时候，可以方便的利用SQL来进行查询和分析。 class SqlTable {  constructor(data) {    this.data = data;  }  async query(sql) {    // following line of code does not run in full page view due to security concern.    // const query_str = sql.replace(/(?&lt;=FROM\s+)\w+/, "CSV(?)");    const query_str = sql.replace("table", "CSV(?)");    return await alasql.promise(query_str, [this.data]);  }} SqlTable是一个对数据表的封装，把csv数据存在SQL数据库表中，提供一个query（）方法。这里要做的是把SQL查询个从&nbsp;"SELECT * FROM table" 变成&nbsp;"SELECT * FROM CSV(?)" 表示查询参数是CSV数据。因为codepen的安全性限制，运行前向查找的replace语句（这里的regex表示把前面是“FROM ”词的替换为CSV(?)的）在full page view下是不能执行的，所以我用了一个更简单的假定，用户的表名就是table，这样做有很多问题，大家如果在codepen之外的环境，可以用注释掉的代码。 然后把"SELECT * FROM table"的查询结果（JSON Array）用datatable来展示。 function sanitizeData(jsonArray) {  let newKey;  jsonArray.forEach(function(item) {    for (key in item) {      newKey = key.replace(/\s/g, "").replace(/\./g, "");      if (key != newKey) {        item[newKey] = item[key];        delete item[key];      }    }  });  return jsonArray;}function displayData(tableId, data) {  // tricky to clone array  let display_data = JSON.parse(JSON.stringify(data));  display_data = sanitizeData(display_data);  let columns = [];  for (let item in display_data[0]) {    columns.push({ data: item, title: item });  }  $("#" + tableId).DataTable({    data: display_data,    columns: columns,    destroy: true  });} 这一步有两点要注意:   数据中，如果列的名字中有包含点，空格等字符，例如Iris数据集中的Sepal.Length，datatable是无法正常显示的，这里要调用sanitizeData（）方法把列名，也就是JsonArray中Json对象的属性名中的点和空格去掉。  sanitizeData（）方法会改变输入对象，所以在传入之前做了一个深度拷贝，这里利用JSON的stringfy和parse方法可以对JSON兼容的对象有效的拷贝。  这里要注意，Iris数据集中在datatable中的列名都不显示点，但实际数据并没有改变。 数据准备 数据加载完毕，我们来到第二步的数据准备阶段。数据准备是数据科学项目最花时间的一步，通常需要对数据进行大量的清洗，变形，抽取等工作，使得数据变得可用。 在这一步我们做了两件事： 一是显示数据的一个摘要，让我们初步了解数据的概貌，为进一步的数据变形和处理做好准备。 这个是Iris数据集的摘要：  function isString(o) {    return typeof o == "string" || (typeof o == "object" &amp;&amp; o.constructor === String);}function summaryData(data) {  let summary = {};  summary.count = data.length;  summary.fields = [];  for (let p in data[0]) {    let field = {};    field.name = p;    if ( isString(data[0][p]) ) {      field.type = "string";    } else {      field.type = "number";    }    summary.fields.push(field);  }    for (let f of summary.fields) {      if ( f.type == "number" ) {        f.max = d3.max(data, x =&gt; x[f.name]);        f.min = d3.min(data, x =&gt; x[f.name]);        f.mean = d3.mean(data, x =&gt; x[f.name]);        f.median = d3.median(data, x =&gt; x[f.name]);        f.deviation = d3.deviation(data, x =&gt; x[f.name]);      } else {        f.values = Array.from(new Set(data.map(x =&gt; x[f.name])));      }  }  return summary;} 这里我们利用数据的类型判断出每一个字段是数值型还是字符型。对于字符型的字段，我们利用JS6的Set来获得所有的Unique数据。对于数值型，我们利用d3的max，min，mean，median，deviation方法计算出对应的最大值，最小值，平均数，中位数和偏差。 另一个就是利用SQL查询来对数据进行进一步的加工。  上图的例子中我们利用限制条件得到一个Iris数据的子集。 另外G2还提供了Dataset的功能：      源数据的解析，将csv, dsv,geojson 转成标准的JSON，查看Connector   加工数据，包括 filter,map,fold(补数据) 等操作，查看&nbsp;Transform   统计函数，汇总统计、百分比、封箱 等统计函数，查看&nbsp;Transform   特殊数据处理，包括 地理数据、矩形树图、桑基图、文字云 的数据处理，查看&nbsp;Transform    数据处理是一个比较大的话题，我们的目标是利用尽可能少的代码完成一个数据分析的工具，所以这一步仅仅是利用alasql提供的SQL查询来处理数据。 数据展示 数据处理好后就是我们的核心内容，数据展示了。  这一步主要是利用select2提供的选择控件构建图形语法来驱动数据展示。如上图所示，对应的G2代码图形语法为： g2chart.facet('rect', {  fields: [ 'Admit', 'Dept' ],  eachView(view) {    view.interval().position('Gender*Freq').color('Gender').label('Freq');  }}); 图形语法主要包含以下几个主要的元素： 几何标记&nbsp;Geometry 几何标记定义了使用什么样的几何图形来表征数据。G2现在支持如下这些几何标记：          geom 类型    描述              point    点，用于绘制各种点图。          path    路径，无序的点连接而成的一条线，常用于路径图的绘制。          line    线，点按照 x 轴连接成一条线，构成线图。          area    填充线图跟坐标系之间构成区域图，也可以指定上下范围。          interval    使用矩形或者弧形，用面积来表示大小关系的图形，一般构成柱状图、饼图等图表。          polygon    多边形，可以用于构建色块图、地图等图表类型。          edge    两个点之间的链接，用于构建树图和关系图中的边、流程图中的连接线。          schema    自定义图形，用于构建箱型图（或者称箱须图）、蜡烛图（或者称 K 线图、股票图）等图表。          heatmap    用于热力图的绘制。       这里要注意，intervalstack是官方支持的，但是文档没有提到，在阅读G2的API文档的时候，我也发现文档讲的不是很清楚，有很多地方没有讲清楚如何使用API。这也是开源软件值得改进的地方。 图形属性 Attributes 图形属性对应视觉编码中的不同元素，大家可以参考我的另一博客&nbsp;数据可视化中的视觉属性&nbsp;。 图形属性主要有以下几种。   position：位置，二维坐标系内映射至 x 轴、y 轴；  color：颜色，包含了色调、饱和度和亮度；  size：大小，不同的几何标记对大小的定义有差异；  shape：形状，几何标记的形状决定了某个具体图表类型的表现形式，例如点图，可以使用圆点、三角形、图片表示；线图可以有折线、曲线、点线等表现形式；  opacity：透明度，图形的透明度，这个属性从某种意义上来说可以使用颜色代替，需要使用 'rgba' 的形式，所以在 G2 中我们独立出来。  在构建语法的时候，我们把图形属性绑定一个或者多个数据字段。 坐标系 Coordinates 坐标系是将两种位置标度结合在一起组成的 2 维定位系统，描述了数据是如何映射到图形所在的平面。 G2提供了以下几种坐标系：          coordType    说明              rect    直角坐标系，目前仅支持二维，由 x, y 两个互相垂直的坐标轴构成。          polar    极坐标系，由角度和半径 2 个维度构成。          theta    一种特殊的极坐标系，半径长度固定，仅仅将数据映射到角度，常用于饼图的绘制。          helix    螺旋坐标系，基于阿基米德螺旋线。       分面 Facet 分面，将一份数据按照某个维度分隔成若干子集，然后创建一个图表的矩阵，将每一个数据子集绘制到图形矩阵的窗格中。分面其实提供了两个功能：   按照指定的维度划分数据集；  对图表进行排版。  G2支持以下的分面类型：          分面类型    说明              rect    默认类型，指定 2 个维度作为行列，形成图表的矩阵。          list    指定一个维度，可以指定一行有几列，超出自动换行。          circle    指定一个维度，沿着圆分布。          tree    指定多个维度，每个维度作为树的一级，展开多层图表。          mirror    指定一个维度，形成镜像图表。          matrix    指定一个维度，形成矩阵分面。       注意，在我的代码中，为了简化使用，只支持list和rect，当绑定一个字段的时候用list，绑定两个字段的时候用rect。 除了上面提到的元素，当然还有许多其它的元素我们没有包含和支持，例如：坐标轴，图例，提示等等。 关于图形的语法的更多内容，请参考这里。 生成图形语法的核心代码如下： function getFacet(faced, grammarScript) {  let facedType = "list";  let facedScript = ""  grammarScript = grammarScript.replace(chartScriptName,"view");  if ( faced.length == 2 ) {      facedType = "rect";  }  let facedFields = faced.join("', '")  facedScript = facedScript + `${ chartScriptName }.facet('${ facedType }', {\n`;  facedScript = facedScript + `  fields: [ '${ facedFields }' ],\n`;  facedScript = facedScript + `  eachView(view) {\n`;  facedScript = facedScript + `    ${ grammarScript };\n`;  facedScript = facedScript + `  }\n`;  facedScript = facedScript + `});\n`;  return facedScript}function getGrammar() {  let grammar = {}, grammarScript = chartScriptName + ".";  grammar.geom = $('#geomSelect').val();   grammar.coord = $('#coordSelect').val();   grammar.faced = $('#facetSelect').val();   geom_attributes.map(function(attr){    grammar[attr] = $('#' + attr + "attr").val();  });    grammarScript = grammarScript + grammar.geom + "()";  geom_attributes.map(function(attr){    if (grammar[attr].length &gt; 0) {      grammarScript = grammarScript + "." + attr + "('" + grammar[attr].join("*") + "')";     }   });    if (grammar.coord) {    grammarScript = grammarScript + ";\n " + chartScriptName + "." + "coord('" + grammar.coord + "');";  } else {    rammarScript = grammarScript + ";";  }    if ( grammar.faced ) {    if ( grammar.faced.length == 1 ||         grammar.faced.length == 2 ) {      grammarScript = getFacet(grammar.faced, grammarScript);    }   }    console.log(grammarScript)  return grammarScript;} 这里有几点要注意：   使用JS的模版字符串可以有效的构造代码片段  使用eval执行构造好的语法驱动的代码来响应select的change事件，以获得良好的交互性。在生产环境，要注意该方法的安全性隐患，因为纯前端，eval能带来的威胁比较小，生产中，可以把这个执行放在安全的沙箱中运行  你需要理解图形语法，并不是任意的组合都能驱动出有效的图形。  这里对于select2的多选，有一个小的提示，在缺省情况下，多选的顺序是固定的顺序，并不依赖选择的顺序，然而许多图形语法和字段的顺序有关，所以我们使用如下的方法来相应select的选择事件。 function updateSelect2Order(evt) {  let element = evt.params.data.element;  let $element = $(element);  $element.detach();  $(this).append($element);  $(this).trigger("change");} 这样做就是每次选中后，把当前选中的项目移到数据最后的位置。 一些例子 好了，下面我们就来看一些例子，了解一下如何使用图形语法来分析和探索数据。 Iris数据集散点图  图形语法： g2chart.point().position('Sepal.Length*Petal.Length').color('Species').size('Sepal.Width') Car数据集折线图  图形语法： g2chart.line().position('id*speed'); 切换到极坐标：  图形语法： g2chart.line().position('id*speed'); g2chart.coord('polar'); Berkeley数据柱状图  数据处理： SELECT SUM(Freq) as f , Gender FROM table GROUP BY Gender 图形语法： g2chart.interval().position('Gender*f').color('Gender').label('f'); Berkeley数据堆叠柱状图  数据处理： SELECT SUM(Freq) as f , Gender , Admit FROM table GROUP BY Gender, Admit 图形语法： g2chart.intervalStack().position('Gender*f').color('Admit') Berkeley数据饼图  数据处理： SELECT SUM(Freq) as f , Gender FROM table GROUP BY Gender 图形语法： g2chart.intervalStack().position('f').color('Gender').label('f');g2chart.coord('theta') Berkeley数据分面的应用  图形语法： g2chart.facet('rect', {  fields: [ 'Dept', 'Admit' ],  eachView(view) {    view.coord('theta');    view.intervalStack().position('Freq').color('Gender');  }}); 更多的分析图形留给大家去尝试 总结 本文分享了一个利用纯前端技术构建一个类似Tableau的BI应用的例子，整个代码统计：   JS 370&nbsp;行 JS6  HTML 69 + 9 + 5 = 83 行  CSS 21 行  总计474 行，用这么少的代码就能完成一个看上去还不错的BI工具，还算不错吧。当然这里主要是由于开源社区提供了这么多好的前端库以供应用，我要做的仅仅是让它们有效的工作在一起。这个只能算是个原型，从功能和质量上来说都不成熟，但是能在浏览器中不借助任何的服务器来实现BI的数据分析功能，应该会有很多人想要在自己的应用中嵌一个吧？ 结合我之前分享的TensorflowJS的文章，下面一步可能是加入预测功能，为数据分析加入智能，前端应用的前景，不可限量！ &nbsp; 参考   axios&nbsp; 基于Promise的HTTP客户端  alasql&nbsp;基于JS的开源SQL数据库  jquery datatable&nbsp;JQuery的数据表格插件  select2&nbsp;JQuery的选择控件插件  相关博客&nbsp;使用开源软件快速搭建数据分析平台&nbsp;  相关博客&nbsp;数据可视化中的视觉属性</div>
                    </div>
    
        <div class='blog'>
            <h2><a href="#top" class='top'>回到顶部</a><a name="blog_1812524" href="https://my.oschina.net/taogang/blog/1812524">在浏览器中进行深度学习：TensorFlow.js (六）构建一个卷积网络 Convolutional Network </a></h2>
            <div class='outline'>
                <div class='date'>时间：2018-05-15 07:02:15</div>
                <div class='catalog'>分类：机器学习</div>
                                                                            </div>
            <div class='content'>在上一篇中，我们介绍了了用TensorflowJS构建一个神经网络，然后用该模型来进行手写MINST数据的识别。和之前的基本模型比起来，模型的准确率上升的似乎不是很大。（在我的例子中，验证部分比较简单，只是一个大致的统计）甚至有些情况下，如果参数选择不当，训练效果还会更差。 卷积网络，也叫做卷积神经网络（con-volutional neural network, CNN），是一种专门用来处理具有类似网格结构的数据的神经网络。例如时间序列数据（可以认为是在时间轴上有规律地采样形成的一维网格）和图像数据（可以看作是二维的像素网格）。对于MINST手写数据来说，应用卷积网络会不会是更好的选择呢？ 先上图：  代码见Codepen 该图是我应用CNN对MINST数据进行训练的结果，准确率在97%，可以说和之前的模型来比较，提高显著。要知道，要知道在获得比较高的准确率后，要提高一点都是比较困难的。那我们就简单的看看卷积网络是什么，他为什么对于手写数据的识别做的比其他模型的更好？ CNN的原理实际上是模拟了人类的视觉神经如何识别图像。每个视觉神经只负责处理不同大小的一小块画面，在不同的神经层次处理不同的信息。 卷积和核 大家可能有用过Photoshop的经验，Photoshop提供很多不同类型的滤镜来处理图像，其实那个本质上就是应用不同的核函数对图像进行卷积的结果。 卷积操作如下图所示：  左边的矩形是输入数据，也就是我们要处理的图像的张量表示。中间的矩形是核，而右边的矩形就是卷积的结果。核函数从左至右，从上到下，每次移动一个像素扫描图像，计算出卷积和的结果矩阵。 卷积的计算过程如下图：  计算就是乘法和加法，但是上图的例子计算有个错误，看你找不找得到。下面这个图计算更简单一点：  如果你能够理解上图的数学含义你就能理解，核函数其实是一个权重，对于每一个小块的图像，不同的核对不同区域的权重不一样。  如上图的两个核，左面的对于图像中间的权重为0，上面的是负向加权，而下面的正向加权。可以想像对应于普通图像，数据分布均匀，这个加权计算的结果趋近于零，对应于水平边缘，上面没有数据而下面有数据，这个加权的值就比较大，这样我们就能够检测出水平边缘。同理右边核函数对应垂直边缘。  上图就是应用垂直，水平，垂直加水平的核，对安卓小机器人图像卷积的结果。我们可以看出对应的核函数是如何识别出边缘的。 然而在学习的时候要使用什么样的核呢？我们看一下网络结构：  每一个像素都是一个特征，每一个特征是一个输入节点。每一个卷积的结果都输入到下一层的隐藏节点。核的权重就连接了输入层和隐藏层。经过0填充的输入层可以输出不同形状的卷积结果。同时可以调整扫描的步幅（stride）。  上图中，输入为7*7，没有填充，步幅为1，输出为5*5  上图中，输入为5*5，填充1格，步幅为1，输出为5*5  上图中，输入为5*5，填充1格，步幅为2，输出为3*3  上图中，输入为2*2，填充2格，步幅为1，输出为4*4 我们可以看出来，增加填充会导致隐藏层节点数量增加，而增大步幅可以使得隐藏层的节点变少。 通过神经网络的学习，就能够确定核的权重。实际的应用，可能会有多个核，因为有许多的特征要学习。  就像我们之前看到如果要学习图像的轮廓，其实是两个不同的核的组合。 池化 池化层通常是紧跟着卷积的一层，通常是做区域的均值或者最大值操作。如下图：  如下图，池化的策略通常是取最大值或者取均值。  池化的作用类似取样，使得下一层神经网络要处理的数据极大的缩小。减少整个网络的参数，防止出现过拟合。 &nbsp; 整体结构  通常，CNN网络的由如上图所示的层次构成：   输入层 Input Layer  卷积层 Convolution&nbsp;Layer  池化层 Pooling&nbsp;Layer  全连接层 Fully Connected (Dense) Layer  分类层 Softmax Classification&nbsp;Layer  输出层 Output Layer  在了解的基本的卷积网络的概念后，我们来看看如何在TensorflowJS中实现一个CNN。 下面是模型的代码： function cnn() {  const model = tf.sequential();  model.add(tf.layers.conv2d({    inputShape: [28, 28, 1],    kernelSize: 5,    filters: 8,    strides: 1,    activation: 'relu',    kernelInitializer: 'varianceScaling'  }));  model.add(tf.layers.maxPooling2d({poolSize: [2, 2], strides: [2, 2]}));  model.add(tf.layers.conv2d({    kernelSize: 5,    filters: 16,    strides: 1,    activation: 'relu',    kernelInitializer: 'varianceScaling'  }));  model.add(tf.layers.maxPooling2d({poolSize: [2, 2], strides: [2, 2]}));  model.add(tf.layers.flatten());  model.add(tf.layers.dense(    {units: 10, kernelInitializer: 'varianceScaling', activation: 'softmax'}));  return model;}    tf.sequential() 创建一个连续的神经网络，自动创建输入层    tf.layers.conv2d 是第一层的卷积层，输入28*28*1是图像的长，高，颜色通道。核的大小是5*5，步幅是1。我们先忽略其它参数。    tf.layers.maxPooling2d是下一个池化层，就是以2*2的小窗口对卷积结果做池化。    接着又是一个卷积和一个池化层。    tf.layers.flatten() 是把之前的结果打平。    最后是一个softmax分类层 tf.layers.dense   类似这样一个结构  训练的代码如下： const model = cnn();const LEARNING_RATE = 0.15;const optimizer = tf.train.sgd(LEARNING_RATE);model.compile({  optimizer: optimizer,  loss: 'categoricalCrossentropy',  metrics: ['accuracy'],});async function train() {  const BATCH_SIZE = 16;  const TRAIN_BATCHES = 1000;  const TEST_BATCH_SIZE = 100;  const TEST_ITERATION_FREQUENCY = 5;  for (let i = 0; i &lt; TRAIN_BATCHES; i++) {    const batch = data.nextTrainBatch(BATCH_SIZE);    let testBatch;    let validationData;    // Every few batches test the accuracy of the mode.    if (i % TEST_ITERATION_FREQUENCY === 0 &amp;&amp; i &gt; 0 ) {      testBatch = data.nextTestBatch(TEST_BATCH_SIZE);      validationData = [        testBatch.xs.reshape([TEST_BATCH_SIZE, 28, 28, 1]), testBatch.labels      ];    }    // The entire dataset doesn't fit into memory so we call fit repeatedly    // with batches.    const history = await model.fit(        batch.xs.reshape([BATCH_SIZE, 28, 28, 1]), batch.labels,        {batchSize: BATCH_SIZE, validationData, epochs: 1});    const loss = history.history.loss[0];    const accuracy = history.history.acc[0];    batch.xs.dispose();    batch.labels.dispose();    if (testBatch != null) {      testBatch.xs.dispose();      testBatch.labels.dispose();    }    await tf.nextFrame();  }} 如果和之前的神经网络的训练的代码比较，这里唯一的变化就是输入数据的形状。 // For CNNbatch.xs.reshape([BATCH_SIZE, 28, 28, 1])// For NNbatch.xs.reshape([BATCH_SIZE, 784]) 这是由两个网络输入层的形状来决定的。 &nbsp; 大家在选取模型的时候可以考虑CNN的优缺点。 优点：   共享卷积核，对高维数据处理无压力  无需手动选取特征，训练好权重，即得特征  分类效果好  缺点：   需要调参，需要大样本量，训练最好要用GPU  物理含义不明确，随着 Convolution 的堆叠，Feature Map 变得越来越抽象，人类已经很难去理解了  CNN是非常流行的深度学习的模型，广泛用于图像相关的有关领域，从阿尔法狗到自动驾驶，到处都有他的身影。如果大家希望进一步了解，可以研习下面的文章。 参考   Convolutional Neural Networks - Basics  如何理解卷积神经网络中的卷积？  Visualizing parts of Convolutional Neural Networks using Keras and&nbsp;Cats  Essentials of Deep Learning: Visualizing Convolutional Neural Networks in Pytho  Undrestanding Convolutional Layers in Convolutional Neural Networks (CNNs)  &nbsp;</div>
                            <div class='commentList'>
                    <h3>评论列表</h3>
                                    </div>
                    </div>
    
        <div class='blog'>
            <h2><a href="#top" class='top'>回到顶部</a><a name="blog_1819665" href="https://my.oschina.net/taogang/blog/1819665">Pyflow : 一个基于工作流的编程模型（Flow Based Programing) 工具</a></h2>
            <div class='outline'>
                <div class='date'>时间：2018-05-28 05:16:36</div>
                <div class='catalog'>分类：架构设计</div>
                                                                            </div>
            <div class='content'>Flow Based Programing&nbsp;是由J. Paul Rodker Morrison在很早以前提出的一种编程范式。 维基百科对FBP的定义如下：   In computer programming, flow-based programming (FBP) is a programming paradigm that defines applications as networks of "black box" processes, which exchange data across predefined connections by message passing, where the connections are specified externally to the processes. These black box processes can be reconnected endlessly to form different applications without having to be changed internally. FBP is thus naturally component-oriented.  在github的这个https://github.com/samuell/awesome-fbp项目内列举了很多不同语言对该范式的实现以及一些资料，大家可以参考。 很多年前我用Python开发了一个基于流概念的数据处理工具。当时主要是想解决让不懂编程的数据工程师能够通过构建图形化的数据流来达到数据获取，变形和抽取的功能。这么多年过去了，我整理了一下代码，丰富了一下基本功能，构建了简单的运行UI，算是有个初步的雏型，看看能不能分享给社区做些贡献。 项目在这里：   https://github.com/gangtao/pyflow  码云地址&nbsp;https://gitee.com/gangtao/pyflow&nbsp;  其实利用Flow的概念在软件项目中很常见。例如：   Apache NiFi  DAG&nbsp;in spark  AWS Step Functions  Azure ML Studio  TensorBoard&nbsp;from Tensorflow  Scratch&nbsp;programing language  argo&nbsp;an open source container-native workflow engine  我么下面来看看这个项目的基本概念和如何使用吧。 Flow的基本概念 Flow的基本概念很简单，就是一个有向无环图（DAG），数据在节点间流动。    节点 Node&nbsp; 节点是组成流的主要单元，负责对流入节点的数据进行处理，并输出到后续节点进行进一步的处理。  端口 Port 每个节点拥有输入和输出端口，输入端口负责数据流入节点，输出端口负责数据流出节点。每个节点都可能拥有一个或者多个输入和输出端口。  连接 Link 一个节点的输出端口连接到另一个节点的输入端口，节点处理好的数据通过连接流入其后的节点。  Flow的实现 Pyflow对Flow的实现基本思路就是用一个Python的函数function实现一个节点，输入端口映射为函数的输入参数。输出端口映射为函数的返回值。  流中有一个节点被设置为终点节点（End Node），通过节点间的连接关系，以终点节点开始通过连接搜索所有的依赖关系（树形查找），得到一个节点运行的栈。例如上图，我们就可以得到一个 [node1，node2, node3] 这样的栈。按顺序出栈的方式执行每一个节点的功能就可以运行整个流。（注意，这是一个简单版本的Flow的实现，仍然是一个批处理，不是streaming） 需要假定每一个节点的功能是无状态的，这样就可以利用输入输出端口对计算结果进行缓存，但输入值是已经运算过的值的时候，不需要运算，直接返回已经计算过的值。 Pyflow的功能 节点 我们先介绍一下Pyflow的节点管理功能呢。Pyflow支持动态的创建和修改节点。如下图：  左边的节点列表列举出当前所有已经定义的节点。 右边包含一个节点的代码编辑面板和一个节点属性面板。 节点属性列出选中节点的ID，Title，Port等属性内容。 节点编辑面板提供对节点功能和代码的编辑。代码是节点对应的Python函数。利用函数的注释生成对应的元数据，也就是端口的属性。 上图是一个节点定义了逻辑操作And，输入端口是a和b两个布尔值，输出是a and b  上图的按钮分别对应以下功能：   增加一个新的节点  存储当前的节点  删除当前的节点  对当前的节点进行测试  例如之前And的例子，我们修改代码后，点击对该节点的功能进行测试。  流 定义好节点后，我们就可以在流管理的界面创建和运行数据流。  左边的面板是所有应定义好的节点，用户可以通过拖拽的方式把节点拖到右边的FLow面板来构建一个数据流。  当选中一个节点的时候，对应的Inspector面板会显示当前选中节点的属性，包含   ID 流中该节点的唯一标志  Specification ID 该节点对应的Spec的ID，Spec可以理解为该节点的类，而该节点在流中是该类的一个实例。  Title 节点的名字  Input Port 输入端口的信息，用户可以对没有连接的端口给出输入值。对于已经连接的端口，会给出连接信心。  Output Port 输出端口的信息。同样对于有连接的给出连接信息  Status 当前节点运行的状态  Error 运行的出错信息  Action 点击按钮，会把当前节点设置为终点节点，并运行当前的数据流。   上面的按钮分别定义了：   创建新的数据流  加载已经保存的数据流  保存当前的数据流  显示当前数据流的JSON表示  删除选中的节点  仓库 Pyflow内置一个基于Sqlite的数据仓库，用于保存所有的节点Sepc定义和数据流。用户可以使用action菜单的Dump和Load功能来加载或者导出。  Pyflow的几个例子 Pyflow给出了几个数据流的例子。 数学运算 pyflow.sample.math:Math是一个数学运算的例子。 这个例子运算 （2+3）X（4+5）／ 5  逻辑运算 pyflow.sample.xor:Xor是利用基本的逻辑操作实现异或（XOR）。数据流如下：  异或只有在两个输入不同的时候才输出真值。  Pyflow给出数据流运行的每一步的结果，让我们可以直观的理解整个数据流的处理过程。 利用多输出，我们可以得到一个简化版本的异或。  这里Self and Not节点的输出端口有两个，同时输出输入值和输入的非值。整个数据流变得更简洁。 错误处理 pyflow.sample.error:Error是一个对出错处理的演示。  该数据流演示了如果数据流中一个节点出错的情形。  由于数据流中的一个节点出错，该数据流无法正常完成，流运行的结果给出每一步的结果，出错节点之后对出错节点有依赖的节点处于Skip状态，没有运行。当出错被恢复后，已经运行成功的节点可以直接从缓存中获取之前运行成功的值。 Flow的优缺点 Flow的优点：   直观  可重用性  利用端口缓存提高效率  Flow的缺点或局限：   节点的定义很困难，如果单个节点很大，负责很多东西，则可重用性变差；如果单个节点和简单，则flow会变的很复杂。  并非所有的功能都能映射为无状态的节点，例如取当前时间，产生随机数。  不支持反馈，这个是当前实现的限制，由于要构建运行栈，不支持Flow中有环路。  总结 利用Flow的概念在编程中不是什么新鲜概念，我希望我的工具能给大家带来一些帮助。如果大家喜欢也可以和我一起来完善这个工具，这里有几个想法：   当前的运行核心不支持stream运算，想要支持，对核心改动比较大，但也不是做不到。  当前的节点是运行在一个独立的进程中，更好的做法是和容器结合，让每一个节点成为一个容器，这样更安全，也有更多的实用性。  我的代码sucks，请大家多多批评。我不是一个前端工程师，前端要实现这么多功能太累了。  Python 2 升级到 3，想想头就疼  项目已经发布到dockerhub了，大家只要运行： docker run -P naughtytao/pyflow:latest 然后在你的浏览器中访问就好了，欢迎各种反馈！ &nbsp;</div>
                            <div class='commentList'>
                    <h3>评论列表</h3>
                                    </div>
                    </div>
    
        <div class='blog'>
            <h2><a href="#top" class='top'>回到顶部</a><a name="blog_1822915" href="https://my.oschina.net/taogang/blog/1822915">用KOps在AWS上部署和管理Kubernetes</a></h2>
            <div class='outline'>
                <div class='date'>时间：2018-06-02 03:36:25</div>
                <div class='catalog'>分类：工作日志</div>
                                                                            </div>
            <div class='content'>Kubernetes作为容器编排的领导者地位已经不容置疑，可是Kubernetes还是面临这一些问题，一个是学习曲线非常陡峭，从容器到K8s你仍然要学习很多东西，另一个是部署，要在部署一套K8s并不容易，你有一些选择：   minikube&nbsp;minikube在你的本机的VM中安装一个单节点的K8s集群，但是这个只能用于本地测试和学习，不能用于真正的生产和大规模使用。  裸机 bare metal 可以自己在裸机（或者虚机）上安装，但是要自己管理物理资源，配置网络和驱动，很麻烦。如果想挑战自己的话可以去参考官方文档&nbsp;  云服务 （cloud hosted solution），各大云厂商都推出了自己的K8s的云方案。云服务优势明显，但通常云服务的master是由云厂商控制，用户自己对集群的控制比较少，而且，云服务使得用户把自己的业务绑定在某个云厂商。例如：       Google&nbsp;GKE    Azure AKS    Amazon EKS    IBM&nbsp;Cloud Kubernetes Service    阿里 容器服务      最后，我们希望使用云，但是又不想受到云厂商的限制，这个时候我们可以使用例如以下个工具在云上部署自己的K8s。例如：       Kops    Kubespray      我们今天就看看如何使用kops在AWS上部署一个K8s的cluster。   假定所有操作在Linux客户端中完成，Mac或者其它客户端自行搜索。  假定你有一个拥有对应权限的AWS account。  安装客户端 安装kubectl,kubectl是K8s的命令行客户端，Kops会使用该客户端进行k8s的配置。 curl -LO https://storage.googleapis.com/kubernetes-release/release/$(curl -s https://storage.googleapis.com/kubernetes-release/release/stable.txt)/bin/linux/amd64/kubectlchmod +x ./kubectlsudo mv ./kubectl /usr/local/bin/kubectl 安装kops curl -LO https://github.com/kubernetes/kops/releases/download/$(curl -s https://api.github.com/repos/kubernetes/kops/releases/latest | grep tag_name | cut -d '"' -f 4)/kops-linux-amd64chmod +x kops-linux-amd64sudo mv kops-linux-amd64 /usr/local/bin/kops &nbsp; 配置AWS资源 首先要安装AWS cli 实现我们需要一个AWS用户用于运行Kops，该用户需要具有以下的权限： AmazonEC2FullAccessAmazonRoute53FullAccessAmazonS3FullAccessIAMFullAccessAmazonVPCFullAccess 用CLI创建对应的用户组，用户和access key： aws iam create-group --group-name kopsaws iam attach-group-policy --policy-arn arn:aws:iam::aws:policy/AmazonEC2FullAccess --group-name kopsaws iam attach-group-policy --policy-arn arn:aws:iam::aws:policy/AmazonRoute53FullAccess --group-name kopsaws iam attach-group-policy --policy-arn arn:aws:iam::aws:policy/AmazonS3FullAccess --group-name kopsaws iam attach-group-policy --policy-arn arn:aws:iam::aws:policy/IAMFullAccess --group-name kopsaws iam attach-group-policy --policy-arn arn:aws:iam::aws:policy/AmazonVPCFullAccess --group-name kopsaws iam create-user --user-name kopsaws iam add-user-to-group --user-name kops --group-name kopsaws iam create-access-key --user-name kops 记录下最后一步创建的用户SecretAccessKey 和 AccessKeyID，并在客户端中配置，导出到环境变量 # configure the aws client to use your new IAM useraws configure           # Use your new access and secret key hereaws iam list-users      # you should see a list of all your IAM users here# Because "aws configure" doesn't export these vars for kops to use, we export them nowexport AWS_ACCESS_KEY_ID=$(aws configure get aws_access_key_id)export AWS_SECRET_ACCESS_KEY=$(aws configure get aws_secret_access_key) 用户配置好了可以配置DNS，这个是可选项，我们先略过。 Kops把K8s集群的配置存储在AWS的S3中，每一个集群的配置对应于一个S3文件，所有我们创建一个S3的bucket用于存储集群的配置。 export BUCKET=&lt;bucket-name&gt;aws s3api create-bucket \    --bucket $BUCKET \    --region us-west-2 \    --create-bucket-configuration  LocationConstraint=us-west-2aws s3api put-bucket-versioning --bucket $BUCKET  --versioning-configuration Status=Enabled 好了，准备完毕，我们可以开始创建K8s集群了。 创建集群 首先， 如果你的客户端没有ssh key，创建一个 ssh-keygen -t rsa -C "your_email@example.com" 配置环境变量，定义集群的名字和配置的url export NAME= &lt;cluster-name&gt;.k8s.localexport KOPS_STATE_STORE=s3://$BUCKET 创建集群之前，查看一下有哪些可用的AZ aws ec2 describe-availability-zones --region us-west-2 我当前在使用 us-west-2的region可用的az是这三个 {    "AvailabilityZones": [        {            "State": "available",            "Messages": [],            "RegionName": "us-west-2",            "ZoneName": "us-west-2a"        },        {            "State": "available",            "Messages": [],            "RegionName": "us-west-2",            "ZoneName": "us-west-2b"        },        {            "State": "available",            "Messages": [],            "RegionName": "us-west-2",            "ZoneName": "us-west-2c"        }    ]} 那么我们就选则在us-west-2a中创建 kops create cluster \    --zones us-west-2a \    ${NAME} 注意这一步只是生成了集群的配置文件，并存储在S3中。 可以使用kops命令，修改配置： kops edit cluster ${NAME} 如果配置没有问题，就可以部署了： kops update cluster ${NAME} --yes 缺省的情况下，kops会创建所有对应的AWS资源，包含VPC，子网，EC2，Auto Scaling Group，ELB，安全组等等。 如果需要安装在特定的子网，在创建集群时可以指定子网的id。另外，也支持跨AZ的HA配置。 集群安装好之后，需要几分钟时间启动，我们可以用kubectl来查看一下状态(Kops会自动把cluster的配置写到~/.kube/config 文件中作为缺省配置)： kubectl cluster-info 建议安装kube-dashboard，可以用UI来管理集群，Linux终端狂人自行略过。  在集群不需要的时候，可以用kops删除集群： kops delete cluster --name ${NAME} &nbsp; 扩展和暂停集群 在云上的K8s集群可以很方便的扩展，如果你的集群的计算资源都用的差不多了，你希望扩展你的集群的时候，有两种办法。 一是直接修改AWS的auto scaling group。KOps会在AWS上创建两个auto scaling group，一个用于Node，另一用于Master，通常我们只要修改Node所在的Auto Scaling Group的number就好了。  Kops的缺省设置是2，你可以把对应的数值设置成自己需要的数字。 另一个就是通过Kops来修改 kops edit ig nodes 把maxSize和minSize都设置成需要的值，然后更新 kops update cluster --yeskops rolling-update cluster 使用rolling-update可以保证在更新的时候业务不会中断。 另外，有人可能会问，我希望不用的时候能把集群暂停，这样就不会使用很多的AWS系统资源了，这要怎么办。因为Auto Scaling Group的存在，如果直接stop对应的EC2实例，Auto Scaling Group会创建新的实例的取代，所以这个方法是不管用的。其实办法很简单，只要把对应的Auto Scaling Group的数值设置为0就好了。 同样可以在AWS中直接修改Master和Node所在的Auto Scaling Group，或者在Kops中修改。 注意在Kops中修改，需要调用如下的命令来获得Master所在group的名字。 $ kops get igUsing cluster from kubectl context: staging.cluster-name.comNAMEROLEMACHINETYPEMINMAXSUBNETSmaster-us-west-2aMasterm3.medium00us-west-2anodesNodet2.medium00us-west-2a &nbsp; Kubernetes还在不断的发展中，我相信会有越来越多的好的工具来让K8s的安装变得更简单。在github的这个项目里列出了更多的选择供大家参考。 &nbsp; 参考   https://blog.alexellis.io/kubernetes-in-10-minutes/  https://medium.com/@pmvk/deploying-production-grade-kubernetes-cluster-on-bare-metal-b11c6f3beb87  https://github.com/ramitsurana/awesome-kubernetes</div>
                    </div>
    
        <div class='blog'>
            <h2><a href="#top" class='top'>回到顶部</a><a name="blog_1860421" href="https://my.oschina.net/taogang/blog/1860421">机器学习管理平台 MLFlow </a></h2>
            <div class='outline'>
                <div class='date'>时间：2018-07-21 05:11:35</div>
                <div class='catalog'>分类：机器学习</div>
                                                                            </div>
            <div class='content'>最近工作很忙，博客一直都没有更新。抽时间给大家介绍一下Databrick开源的机器学习管理平台-MLFlow。  谈起Databrick，相信即使是不熟悉机器学习和大数据的工程湿们也都有所了解，它由Spark的创始人开办的，主要为用户提供在云端使用Spark来进行机器学习的能力。那么这次Databrick开源的MLFlow究竟能提供什么样的功能呢？ 首先我们来看一看当前机器学习所遇到的挑战：   使用的软件工具繁多，一个机器学习的项目基本上数据收集和准备，模型训练，模型部署的不断迭代过程，这个过程中的每一步都有很多不同的工具和选择。单就模型训练来说，我们就有scikit-learn，pytorch，spark，tensorflow，R 等等诸多选择。它们各自具有不同的优缺点和适用场合，对于数据科学家而言，要管理和适用这么多的工具，确实非常困难。  很难跟踪和重现数据处理，代码运行，参数调试的过程和结果。  很难对模型进行产品化，部署模型很困难。  当数据规模增长的时候，很难扩展和伸缩。  MLFlow的目标是希望能够解决这些问题，希望支持任何的机器学习库，可以在本机或者云端运行，并解决可伸缩性的问题 MLFlow采用了开源的策略，基于API和模块化的设计。他主要有以下三个功能模块：   Tracking 跟踪和记录  Project 项目管理  Models 模型管理  我们分别来看看这三块： Tracking  MLFlow的追踪功能类似APM，记录应用运行过程中所产生的各种数据。主要记录参数（Parameters），指标（Metrics），模型的持久化对象（Artifact）等内容。这一部分实际上功能的实现很简单。大家可以参考它的代码&nbsp;， 例如，对于参数的记录代码： def log_param(self, key, value):    # TODO: prevent keys from containing funky values like ".."    fn = os.path.join(self.run_dir, "parameters", key)    dir_util.mkpath(os.path.dirname(fn))    with open(fn, "w") as f:        f.write("%s\n" % value) 我想说，要解决追踪的问题，业内已经有很多成熟的工具，各种APM产品，甚至Splunk，ELK都是不错的选择。 Project  项目管理主要是为了解决机器学习中诸多不同工具的问题。其实解决的方法也很简单，就是用一些元数据来描述项目，并把项目用一个合理的目录结构来管理，结构如下：  例如上图的例子，把一个conda的python项目用一个yaml文件来描述，记录了名字是什么，参数是什么，有哪些依赖，运行命令是什么，然后就可以用mlflow的API或者命令行客户端来调用。 对于这个问题，我想说的是，为什么不适用容器呢？容器是一个很好的封装各种不用运行环境和运行工具的选择。比起上面的方案，我认为要强许多，何况，还要支持云端，容器才是最佳实践！ 项目管理的代码在这里&nbsp;，功能也不多。 MLFlow可以直接运行在github上的项目，也就是用github作为项目管理的仓库。 这里的亮点是可以运行拥有多个步骤的工作流，每一个步骤都是一个项目，类似一个数据处理管道（data pipeline）。利用Tracking API，不同项目步骤之间可以传递数据和模型（Artifact）。这也许是为什么该项目叫MLFlow吧。 Models  MLFlow利用类似对项目管理的相同哲学管理模型，使用元数据来描述不同工具所产生的不同模型。  上图是一个模型的例子。模型支持不同的口味（Flavors），包含Python，Scikit-Learn，Tensorflow和Spark ML。用户也可以定制自己个性化的口味（感觉像是在做菜）。 模型这比部分还支持了部署的功能，也就是说可以把模型部署到不同的平台，这个功能还是很好的。主要支持本地部署，Microsoft AzureML，AWS Sagemaker和Apache Spark。 当然，MLFlow并不能很好的解决不同工具所产生的模型不兼容的问题，你并不能把一个tensorflow的模型部署到Saprk上。如果有人对这一部分感兴趣，可以去关注一下这个项目：Mleap 另外MLFlow使用Flask提供web服务，有UI功能。当前还在Alpha阶段。希望它的未来能有好的发展，给数据科学家们提供一个好用的平台。 参考   https://mlflow.org/docs/latest/index.html#  https://github.com/databricks/mlflow  https://databricks.com/blog/2018/06/05/introducing-mlflow-an-open-source-machine-learning-platform.html  https://www.oreilly.com/ideas/mlflow-a-platform-for-managing-the-machine-learning-lifecycle  https://www.jianshu.com/p/2ed60a1dc764  https://juejin.im/post/5b1775086fb9a01e7b4de8f0  &nbsp;</div>
                            <div class='commentList'>
                    <h3>评论列表</h3>
                                    </div>
                    </div>
    
        <div class='blog'>
            <h2><a href="#top" class='top'>回到顶部</a><a name="blog_1940597" href="https://my.oschina.net/taogang/blog/1940597">读书笔记：A Philosophy of Software Design （一）</a></h2>
            <div class='outline'>
                <div class='date'>时间：2018-09-01 13:11:36</div>
                <div class='catalog'>分类：架构设计</div>
                                                                            </div>
            <div class='content'>今天一位同事在斯坦福的博士生导师John Ousterhout&nbsp;（注，Tcl语言的设计者）来公司做了他的新书《A Philosophy of Software Design》的演讲，介绍了他对于软件设计的思考。这里我把本书的读书笔记和心得分享给大家，欢迎大家来和我交流探讨。大家也可以去看作者在google演讲时的视频和他演讲的slides复杂性的本质软件设计应该简单，避免复杂，关于复杂性的定义，作者认为主要有两个量度系统是不是难以理解系统是不是难以修改关于复杂性的症状：当新增特性时，需要修改大量的代码当需要完成一个功能时，开发人员需要了解许多知识当新增/修改功能时，不能明显的知道要修改那些代码引起复杂性的原因：依赖和晦涩。最后，复杂性不是突然出现的，它是随着时间和系统的演进逐渐增加的。我的解读：这本书讲的是软件设计的哲学，哲学要解决的是最根本的问题。作者认为软件设计要解决的最根本的问题就是避免复杂性，依赖和晦涩是造成软件负责的主要原因。依赖很多时候是无法避免的，但是应该尽可能的减少依赖，去除不必要的依赖。软件设计应该容易理解，晦涩是引起复杂性增加的另一个原因。这个核心观点是这本书的主旨，借用老爱的话“Simple，but not simpler！”我曾经就职某存储巨头，其中有一块代码因为是收购的产品，代码已经非常陈旧了，因为没有人能看懂，所以也就没有人敢修改。你看，这个产品不是也卖的挺好的。&nbsp;仅仅可工作的代码还远远不够在第二章，作者提出了“战术性编程”和“战略性编程”的对立。“战术性编程”最求以最快的速度完成可工作的功能。这看上去无可厚非。但是这种行为往往会增加系统的复杂性。引发大量的技术债。可以说这种做法以牺牲长远利益来获得眼前的利益。“战略性编程”不仅仅要求可工作的代码，以好的设计为重，未来的功能投资，认为现阶段在设计上的投入会在将来获得回报。好的设计是有代价的，问题是你愿意投入多少？我的解读：很有趣的是，我司之前的产品的负责人在公司推行大规模的敏捷（LeSS），当时有一个顾问给我们上课，他也说设计要尽可能简单，但是不要为了未来做设计。以最小的代价实现可用的功能。以John的观点，这样做无疑会增加系统变复杂的可能性。我比较认同John这里的观点，好的设计是有价值的，投入在软件设计上的，对功能毫无影响的东西，是有价值的。但是如何取舍和权衡，投入多少是需要开发团队达成共识。&nbsp;软件有它的生命周期，为了未来的投入也不是越多越好。模块要有深度深度其实是对模块封装的度量，模块应该提供尽可能简单的接口和尽可能强大的功能。这样的模块称之为深度模块。我的解读：这一部分没有什么新东西，传统的面向对象和如今的微服务架构都是对这一哲学的应用。好的封装可以减少依赖，简单的接口可以避免晦涩。也就是减少了复杂性。信息的隐藏和泄漏关于信息的隐藏和泄漏，这一部分对于熟悉面向对象的猿们来说不是新东西。基于SOLID，这就是Open，软件应该是对于扩展开放的，但是对于修改封闭的。信息隐藏使得修改变的封闭。具有通用功能的模块更具深度更通用功能的接口意味着更高层级的抽象，隐藏更多的实现细节，按照John的观点，也就更具深度。那么如何在通用接口和特殊接口之间做权衡呢？能够实现所需功能的最简单接口是什么？该接口会被用于那些不同场景？该接口对于我的当前是否容易使用？我的解读：通用的接口和之前的“战略性编程”是一致的，更通用的接口在面对未来可能发生的需求变化的时候，更容易使用。这里的艺术在于能够找到需求到软件接口之间的最佳映射。抽象到哪一个层级，是主要问题。不同的层，不同的抽象软件系统通常有不同的层次组成，每一层都通过和它之上和之下的层的接口来交互。每一层都具有自己不同的抽象。例如典型的数据库，服务器和客户端模型中，数据库层的抽象是数据表和关系，服务器层是应用对象和应用逻辑而客户端的抽象是用户接口视图和交互。如果你发现不同的层具有相同的抽象，那也许你的分层有问题。把复杂性向下移在软件分层的鄙视链中，最高层是用户，接着的一层的UI工程师，然后是后台工程师，数据库工程师，等等。用户是上帝不能得罪，如果一定要在某个层次处理复杂性，那么这个层次越低越好，反正苦逼程序员也不会抱怨，对得，就是这个道理。合并还是分离“天下大事，分久必合，合久必分”。软件设计中经常要问的问题就是这两个功能模块是合并好，还是分开好？不论是合并还是分离，目标都是降低复杂性，那么把系统分离成更多的小的单元模块，每一个模块都更简单，系统的复杂性会降低么？答案是不一定：复杂性可能来源于系统模块的数量更多的模块也许意味着需要额外的代码来管理和协调更多的模块可能带来许多依赖更多的模块可能带来重复的代码，而重复的代码是恶魔在以下的情况下，需要考虑合并：模块之间共享信息合并后的接口更简单合并后减少了重复的代码确保错误终结异常和错误处理是造成软件复杂的罪魁祸首之一。程序员往往错误的认为处理和上报越多的错误，就越好。这也就导致了过度防御性的编程。而很多时候，程序员捕获了异常并不知道该如何处理，干脆往上层扔，这就违背了封装原则。用户一脸懵逼，“你叫我干啥？”降低复杂度的一个原则就是尽可能减少需要处理的异常可能性。而最佳实践就是确保错误终结，例如删除一个并不存在的文件，与其上报文件不存在的异常，不如什么都不做。确保文件不存在就好了，上层逻辑不但不会被影响，还会因为不需要处理额外的异常而变得简单。今天就先分享到这里，后面有空，我会继续分享本书的后半部分，祝大家开学愉快！&nbsp;</div>
                    </div>
    
        <div class='blog'>
            <h2><a href="#top" class='top'>回到顶部</a><a name="blog_1940954" href="https://my.oschina.net/taogang/blog/1940954">读书笔记：A Philosophy of Software Design （二）</a></h2>
            <div class='outline'>
                <div class='date'>时间：2018-09-02 04:02:37</div>
                <div class='catalog'>分类：编程语言</div>
                                                                            </div>
            <div class='content'>接着上次的分享设计两次这里“设计两次”的意思是无论设计一个类，模块还是功能，在设计的时候仔细思考，除了当前的方案，还有那些其它的选择。在众多设计中比较，列出各自的优缺点，然后选出最佳方案。就是对于设计方案，都有两个或者两个以上的选择。对于大牛而言，也许设计方案显而易见，于是觉得没有必要在不同方案中做遴选。然而这并不是一个好的习惯，这说明，你没有在处理更困难的问题，问题对于你而言太简单了。这不是一个好的现象，因为上坡路总是很难走。当你面对困难的问题的时候，通过对不同设计方案的学习和思考，你会成长到更高的一个层次。我的解读：在管理理论上有一个叫彼得原理，就是“在一个等级制度中，每个人趋向于上升到他所不能胜任的地位”。程序员也面临同样的问题，当你的经验和资历不断的提高，你总会遇到你所不能胜任的问题，这个时候就需要通过不断的学习，提高自己。当然也有可能所处的环境无法给你更具挑战的问题。这个时候你就需要考虑，你的下一站在哪里？为什么要写注释困扰程序员的两大世界性难题：别人的代码没有注释别人让我给我的代码写注释程序员通常有各种理由不写注释：好的代码是自解释的没时间写注释很快就会和代码不一致，造成误解我读的其他人的注释都毫无意义我的解读：其实开发过软件的工程师都能理解写注释的重要性和意义，这并不需要很多的解释。但是“懒惰”是原罪之一，我就是不想写呀不想写。关于软件开发的七宗罪，请阅读AntiPatterns&nbsp;注释应当用于描述代码中不易理解的部分如果你一定要对于显而易见的部分增加注释，那么可能你是按代码行数收取工资吧，当然，注释也是算行数的。选择命名给变量，类，模块，文件起名字很难，真的很难。好的命名能使得软件设计更容易理解，差的命名更容易产生Bug。我就被坑过。还是在某存储公司的时候，负责开发一个软件升级的规则模块，根据不同的规则决定能不能升级。当时我的代码release之后，发现客户不能升级了。于是我们在代码中找Bug，后来发现，原因是我的代码判断“hardware”字段来决定目标硬件类型是否匹配，而应该是另一个和“hardware”命名很像的另一个字段来决定要升级的硬件的类型。更糟糕的是，因为这个字段实在是比真正应该判断的字段看上去更合理，进行代码审查的人都没能看出这个问题。而当时没有测试环境能够实际匹配到这个硬件类型，这个问题也没能在测试环节中发现。注释先行在实现过程中，把接口和注释先准备好。修改现有代码对于修改代码，同样面临着“战术性编程”和“战略性编程”的挑战，是以最少的修改完成任务，还是以重新设计使得系统更合理的角度进行长线投资，需要仔细思考。我的解读：随便改一些不相关的代码，你可能会发现Bug神奇的消失了，软件开发需要运气，祈祷有的时候真的管用。一致性一致性在软件设计里很重要，包括：命名代码风格接口设计模式常量可以使用以下的方法来保证一致性：文档利用工具/代码审查来强制入乡随俗不要随便改变命名约定代码应当显而易见怎么定义代码是不是显而易见，就是带代码审查的时候，如果有人认为这的代码不是容易理解，那么这个代码应该就是有问题的。也许这个代码对你来说很直观，但是代码不是写给自己看的。应该让团队里的其他成员也能读懂你的代码。有一些使的代码不易理解的元素：事件驱动模式 - 因为不知道事件流控制的顺序范型 - 也许运行时才知道类型，造成阅读的困难我的解读：最早曾在一家通信企业做管理软件开发，几年后被要求修改自己多年前写的代码，读了好久，愣是没看懂。软件开发的趋势John对软件开发重的一些趋势和问题做了总结：面向对象，对于继承，基于接口的继承要优于基于实现的继承敏捷，敏捷的一个潜在问题是导致“战术性编程”为主导，导致系统的复杂性增加单元测试测试驱动，测试驱动的问题是关注功能，而非找到最佳设计设计模式，设计模式的问题可能导致过度应用Getter/Seeting， 这个模式可能是冗余的，也许不如直接暴露成员更简单为性能做设计关于如何在复杂性和性能之间的权衡，通常更简单的代码运行的更快。当然很有可能更复杂和晦涩的代码性能更高，例如汇编对比Python。设计的时候需要考虑的是为了获得性能的提升，代价是什么？这样的代价是不是值得？在为了性能做出修改之前，先进行测量。针对关键路径，找到影响性能的核心单元，做出性能改进的设计。&nbsp;这本书的核心是关于“复杂性”的，软件无疑是一个非常复杂的领域。对于导致复杂的原因，我觉得John的观点没有问题，但是实际上还有很多更深层的原因。软件开发和人息息相关，离开人来讲纯软件的东西，其实并不复杂，软件开发中引起复杂性的更多原因是更为复杂的人，团队，组织，和组织关系。这并不是对该书的否定，这本书对于程序员来说还是很好的一本书，值得一读。&nbsp;</div>
                    </div>
    
        <div class='blog'>
            <h2><a href="#top" class='top'>回到顶部</a><a name="blog_2032324" href="https://my.oschina.net/taogang/blog/2032324">在浏览器中进行深度学习：TensorFlow.js (七）递归神经网络 （RNN）</a></h2>
            <div class='outline'>
                <div class='date'>时间：2018-09-09 12:59:45</div>
                <div class='catalog'>分类：机器学习</div>
                                                                            </div>
            <div class='content'>介绍 上一篇博客我们讨论了CNN，卷积神经网络。CNN广泛应用于图像相关的深度学习场景中。然而CNN也有一些限制：   很难应用于序列数据  输入数据和输出数据都是固定长度  不理解上下文  这些问题就可以由RNN来处理了。 神经网络除了CNN之外的另一个常见的类别是RNN，递归/循环神经网络。这里的R其实是两种神经网络，Recurrent：时间递归 ， Recusive：结构递归。时间递归神经网络的神经元间连接构成有向图，而结构递归神经网络利用相似的神经网络结构递归构造更为复杂的深度网络。我们大部分时间讲的RNN指的是前一种，时间递归神经网络。  &nbsp; RNN的结构如上图所示，为了解决上下文的问题，RNN网路中，输入Xt，输出ht，而且输出ht回反馈回处理单元A。上图右边是随时间展开的序列图。tn时间的输出hn反馈成为tn+1时间的输入，hn和Xn+1一起成为tn+1时间的输入。这样也就保留了上下文对模型的影响，以更好的针对时间序列建模。 如下图所示，RNN可以支持不同的输入输出序列。  RNN有一些变体，常见的是LSTM和GRU LSTM即Long Short Memory Network，长短时记忆网络。它其实是属于RNN的一种变种，可以说它是为了克服RNN无法很好处理远距离依赖而提出的。 GRU即Gated Recurrent Unit，是LSTM的一个变体。GRU保持了LSTM的效果同时又使结构更加简单，所以它也非常流行。  RNN可以有效的应用在以下的领域中：   音乐作曲  图像捕捉  语音识别  时序异常处理  股价预测  文本翻译  &nbsp; 例子：用RNN实现加法运算 我们这里介绍一个利用RNN来实现加法运算的例子，源代码在这里，或者去我的Codepen运行我的例子。这个例子最早源自keras。 科学世界的论证（reasoning）方式有两种，演绎（deduction）和归纳（induction）。 所谓的演绎就是根据已有的理论，通过逻辑推导，得出结论。经典的就是欧几里得的几何原理，利用基本的公设和公理演绎出了整个欧氏几何的大厦。而机器学习则是典型的归纳法，数据先行，现有观测数据，然后利用数学建模，找到最能够解释当前观察数据的公式。这就像是理论物理学家和实验物理学家，理论物理学家利用演绎，根据理论推出万物运行的道理，实验物理学家通过实验数据，反推理论，证实或者否定理论。当然两种方法是相辅相成的，都是科学的利器。 好了我们回到加法的例子，这里我们要用机器学习的方法来教会计算机加法，记得用归纳而不是演绎。因为计算机是很擅长演绎的，加法的演绎是所有计算的基础之一，定义0，1，2=1+1，然后演绎出所有的加法。这里用归纳，然计算机算法通过已有的加法例子数据找到如何计算加法。这样做当然不是最有效的，但是很有趣。 我们来看例子吧。 首先是一个需要一个字符表的类来管理字符到张量的映射： class CharacterTable {  /**   * Constructor of CharacterTable.   * @param chars A string that contains the characters that can appear   *   in the input.   */  constructor(chars) {    this.chars = chars;    this.charIndices = {};    this.indicesChar = {};    this.size = this.chars.length;    for (let i = 0; i &lt; this.size; ++i) {      const char = this.chars[i];      if (this.charIndices[char] != null) {        throw new Error(`Duplicate character '${char}'`);      }      this.charIndices[this.chars[i]] = i;      this.indicesChar[i] = this.chars[i];    }  }  /**   * Convert a string into a one-hot encoded tensor.   *   * @param str The input string.   * @param numRows Number of rows of the output tensor.   * @returns The one-hot encoded 2D tensor.   * @throws If `str` contains any characters outside the `CharacterTable`'s   *   vocabulary.   */  encode(str, numRows) {    const buf = tf.buffer([numRows, this.size]);    for (let i = 0; i &lt; str.length; ++i) {      const char = str[i];      if (this.charIndices[char] == null) {        throw new Error(`Unknown character: '${char}'`);      }      buf.set(1, i, this.charIndices[char]);    }    return buf.toTensor().as2D(numRows, this.size);  }  encodeBatch(strings, numRows) {    const numExamples = strings.length;    const buf = tf.buffer([numExamples, numRows, this.size]);    for (let n = 0; n &lt; numExamples; ++n) {      const str = strings[n];      for (let i = 0; i &lt; str.length; ++i) {        const char = str[i];        if (this.charIndices[char] == null) {          throw new Error(`Unknown character: '${char}'`);        }        buf.set(1, n, i, this.charIndices[char]);      }    }    return buf.toTensor().as3D(numExamples, numRows, this.size);  }  /**   * Convert a 2D tensor into a string with the CharacterTable's vocabulary.   *   * @param x Input 2D tensor.   * @param calcArgmax Whether to perform `argMax` operation on `x` before   *   indexing into the `CharacterTable`'s vocabulary.   * @returns The decoded string.   */  decode(x, calcArgmax = true) {    return tf.tidy(() =&gt; {      if (calcArgmax) {        x = x.argMax(1);      }      const xData = x.dataSync(); // TODO(cais): Performance implication?      let output = "";      for (const index of Array.from(xData)) {        output += this.indicesChar[index];      }      return output;    });  }} 这个类存储了加法运算所能用到的所有字符，“0123456789+ ”，其中空格是占位符，两位数的2会变成“ 2”。 为了实现字符到索引的双向映射， 这个类保存了两个表，charIndices是字符到索引，indicesChar是索引到字符。 encode方法把一个加法字符串映射为一个one hot的tensor： this.charTable.encode("1+2",3).print()；Tensor    [[0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0],     [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]]this.charTable.encode("3",1).print()Tensor     [[0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0],] 例如对于“1+2”等于“3”，输入和输出的张量如上所示。 decode是上述encode方法的逆向操作，把张量映射为字符串。 然后进行数据生成： function generateData(digits, numExamples, invert) {  const digitArray = ["0", "1", "2", "3", "4", "5", "6", "7", "8", "9"];  const arraySize = digitArray.length;  const output = [];  const maxLen = digits + 1 + digits;  const f = () =&gt; {    let str = "";    while (str.length &lt; digits) {      const index = Math.floor(Math.random() * arraySize);      str += digitArray[index];    }    return Number.parseInt(str);  };  const seen = new Set();  while (output.length &lt; numExamples) {    const a = f();    const b = f();    const sorted = b &gt; a ? [a, b] : [b, a];    const key = sorted[0] + "`" + sorted[1];    if (seen.has(key)) {      continue;    }    seen.add(key);    // Pad the data with spaces such that it is always maxLen.    const q = `${a}+${b}`;    const query = q + " ".repeat(maxLen - q.length);    let ans = (a + b).toString();    // Answer can be of maximum size `digits + 1`.    ans += " ".repeat(digits + 1 - ans.length);    if (invert) {      throw new Error("invert is not implemented yet");    }    output.push([query, ans]);  }  return output;} 生成测试数据的方法，输入是加法的位数和生成多少例子。对于两位数的加法，输入补齐为5个字符，输出补齐到3个字符，空位用空格。 generateData(2,10,false);["24+38", "62 "]["2+0  ", "2  "]["86+62", "148"]["36+91", "127"]["66+51", "117"]["47+40", "87 "]["97+96", "193"]["98+83", "181"]["45+30", "75 "]["88+75", "163"] 下一步需要把生成的数据转化成张量： function convertDataToTensors(data, charTable, digits) {  const maxLen = digits + 1 + digits;  const questions = data.map(datum =&gt; datum[0]);  const answers = data.map(datum =&gt; datum[1]);  return [    charTable.encodeBatch(questions, maxLen),    charTable.encodeBatch(answers, digits + 1)  ];} 生成的数据是一个2个元素的列表，第一个元素是问题张量，第二个元素是答案张量。 数据生成好了，下一步就是创建神经网络模型： function createAndCompileModel(  layers,  hiddenSize,  rnnType,  digits,  vocabularySize) {  const maxLen = digits + 1 + digits;  const model = tf.sequential();  switch (rnnType) {    case "SimpleRNN":      model.add(        tf.layers.simpleRNN({          units: hiddenSize,          recurrentInitializer: "glorotNormal",          inputShape: [maxLen, vocabularySize]        })      );      break;    case "GRU":      model.add(        tf.layers.gru({          units: hiddenSize,          recurrentInitializer: "glorotNormal",          inputShape: [maxLen, vocabularySize]        })      );      break;    case "LSTM":      model.add(        tf.layers.lstm({          units: hiddenSize,          recurrentInitializer: "glorotNormal",          inputShape: [maxLen, vocabularySize]        })      );      break;    default:      throw new Error(`Unsupported RNN type: '${rnnType}'`);  }  model.add(tf.layers.repeatVector({ n: digits + 1 }));  switch (rnnType) {    case "SimpleRNN":      model.add(        tf.layers.simpleRNN({          units: hiddenSize,          recurrentInitializer: "glorotNormal",          returnSequences: true        })      );      break;    case "GRU":      model.add(        tf.layers.gru({          units: hiddenSize,          recurrentInitializer: "glorotNormal",          returnSequences: true        })      );      break;    case "LSTM":      model.add(        tf.layers.lstm({          units: hiddenSize,          recurrentInitializer: "glorotNormal",          returnSequences: true        })      );      break;    default:      throw new Error(`Unsupported RNN type: '${rnnType}'`);  }  model.add(    tf.layers.timeDistributed({      layer: tf.layers.dense({ units: vocabularySize })    })  );  model.add(tf.layers.activation({ activation: "softmax" }));  model.compile({    loss: "categoricalCrossentropy",    optimizer: "adam",    metrics: ["accuracy"]  });  return model;} 这里的几个主要的参数是：   rnnType， RNN的网络类型，这里有三种，SimpleRNN，GRU和LSTM  hiddenSize，隐藏层的Size，决定了隐藏层神经单元的规模，  digits，参与加法运算的数位  vocabularySize， 字符表的大小，我们的例子里应该是12， 也就是sizeof（“0123456789+ ”）  网络的构成如下图, 图中 digits=2，hiddenSize=128：  repeatVector层把第一个RNN层的输入重复digits+1次，增加一个维数，输出适配到要预测的size上。这里是构建RNN网络的一个需要设计的点。 之后跟着的是另一个RNN层。 然后是一个有12（vocabularySize）个单元全联接层，使用timeDistributed对RNN的输出打包，得到的输出是的形状为 [digits+1,12] 。TimeDistributed层的作用就是把Dense层应用到128个具体的向量上，对每一个向量进行了一个Dense操作。RNN之所以能够进行多对多的映射，也是利用了个这个功能。 最后是一个激活activate层，使用softmax。因为这个网络本质上是一个分类，也就是把所有的输入分类到 digit+1 * 12 的分类。 表示的digit+1位的数字。也就是说两个n位数字的加法，结果是n+1位数字。 最后一步，使用“Adam”算法作为优化器，交叉熵作为损失函数，编译整个模型。 模型构建好了，接下来就可以进行训练了。 训练的代码如下： class AdditionRNN {  constructor(digits, trainingSize, rnnType, layers, hiddenSize) {    // Prepare training data.    const chars = '0123456789+ ';    this.charTable = new CharacterTable(chars);    console.log('Generating training data');    const data = generateData(digits, trainingSize, false);    const split = Math.floor(trainingSize * 0.9);    this.trainData = data.slice(0, split);    this.testData = data.slice(split);    [this.trainXs, this.trainYs] =        convertDataToTensors(this.trainData, this.charTable, digits);    [this.testXs, this.testYs] =        convertDataToTensors(this.testData, this.charTable, digits);    this.model = createAndCompileModel(        layers, hiddenSize, rnnType, digits, chars.length);  }    async train(iterations, batchSize, numTestExamples) {    console.log("training started!");    const lossValues = [];    const accuracyValues = [];    const examplesPerSecValues = [];    for (let i = 0; i &lt; iterations; ++i) {      console.log("training iter " + i);      const beginMs = performance.now();      const history = await this.model.fit(this.trainXs, this.trainYs, {        epochs: 1,        batchSize,        validationData: [this.testXs, this.testYs],        yieldEvery: 'epoch'      });      const elapsedMs = performance.now() - beginMs;      const examplesPerSec = this.testXs.shape[0] / (elapsedMs / 1000);      const trainLoss = history.history['loss'][0];      const trainAccuracy = history.history['acc'][0];      const valLoss = history.history['val_loss'][0];      const valAccuracy = history.history['val_acc'][0];            document.getElementById('trainStatus').textContent =          `Iteration ${i}: train loss = ${trainLoss.toFixed(6)}; ` +          `train accuracy = ${trainAccuracy.toFixed(6)}; ` +          `validation loss = ${valLoss.toFixed(6)}; ` +          `validation accuracy = ${valAccuracy.toFixed(6)} ` +          `(${examplesPerSec.toFixed(1)} examples/s)`;      lossValues.push({'epoch': i, 'loss': trainLoss, 'set': 'train'});      lossValues.push({'epoch': i, 'loss': valLoss, 'set': 'validation'});      accuracyValues.push(          {'epoch': i, 'accuracy': trainAccuracy, 'set': 'train'});      accuracyValues.push(          {'epoch': i, 'accuracy': valAccuracy, 'set': 'validation'});      examplesPerSecValues.push({'epoch': i, 'examples/s': examplesPerSec});    }  }} AdditionRNN类实现了模型训练的主要逻辑。 在构造函数重生成训练数据，其中百分之九十的数据用于训练，百分之十用于测试验证。 在训练中，循环调用model.fit方法进行训练。 训练好完毕，我们就可以使用该模型进行预测了。 const input = demo.charTable.encode("10+20",5).expandDims(0);const result = model.predict(input);result.print()console.log("10+20 = " + demo.charTable.decode(result.as2D(result.shape[1], result.shape[2])));Tensor    [[[0.0010424, 0.0037433, 0.2403527, 0.4702294, 0.2035268, 0.0607058, 0.0166195, 0.0021113, 0.0012174, 0.0000351, 0.0000088, 0.0004075],      [0.3456545, 0.0999702, 0.1198046, 0.0623895, 0.0079124, 0.0325381, 0.2000451, 0.0856998, 0.0255273, 0.0050597, 0.000007 , 0.0153919],      [0.0002507, 0.0000023, 0.0000445, 0.0002062, 0.0000298, 0.0000679, 0.0000946, 0.0000056, 7e-7     , 2e-7     , 1e-7     , 0.9992974]]]10+20 = 40  使用charTable的encode方法把“10+20”编码转换为Tensor，因为输入为多个数据，所以用expandDims方法把它增加一个维度，变成只有一个数据的Batch 对于预测结果，只要看每一个Tensor行中最大的数据，就能找到对应的预测数据了。例如上面的例子对应的结果是：“30空格”。当然这次模型的训练数据比较小，没能正确预测也很正常。 最后我们看看这个RNN网络到底好不好用。使用digits=2，hiddenSize=128，trainIterations=300，batchSize=128  在这个例子中，当训练数据达到2000的时候，LSTM和GRU都能取得比较好的训练结果。2000意味着大概20%的两位数加法的数据。也就是说当掌握了大概20%的数据后，我们就能够比较有把握的预测其它的两位数的加法了。当训练数据是100的时候（1%），SimpleRNN也居然有43%的准确率，可以说是相当不错的模型了。 好了，但是为什么呢？为什么RNN可以用来预测加法呢？这个和时间序列又有什么关系呢？如果你和我有同样的疑问，请阅读这两篇论文：LEARNING TO EXECUTE，Sequence to Sequence Learning with Neural Networks&nbsp; 参考：   Recurrent Neural Networks in DL4J&nbsp;  The Unreasonable Effectiveness of Recurrent Neural Networks  Fundamentals of Deep Learning – Introduction to Recurrent Neural Networks   keras中TimeDistributed的用法    When and How to use TimeDistributedDense   如何在长短期记忆(LSTM)网络中利用TimeDistributed层   人人都能看懂的GRU&nbsp;  LSTM神经网络 和 GRU神经网络&nbsp;  在线图表制作&nbsp;http://charts.udpwork.com/&nbsp;</div>
                    </div>
    
        <div class='blog'>
            <h2><a href="#top" class='top'>回到顶部</a><a name="blog_2052152" href="https://my.oschina.net/taogang/blog/2052152">轻松扩展你的机器学习能力 ： Kubeflow</a></h2>
            <div class='outline'>
                <div class='date'>时间：2018-09-17 02:40:34</div>
                <div class='catalog'>分类：机器学习</div>
                                                                            </div>
            <div class='content'>提起机器学习，尤其是深度学习，大家可能会对诸如Tensorflow，Pytorch，Caffee的工具耳熟能详。但其实在实际的机器学习的生命周期中，训练模型（上述工具主要解决的问题）只是整个机器学习生命周期的很小一部分。  数据如何准备？模型训练好了如何部署？如何上云？如何上规模Scale？等等挑战随之而来。随着机器学习的广泛应用，许多工具响应而生，以解决模型部署的问题。例如：   Oracle 的&nbsp;graphpipe  Databricks 的&nbsp;mlflow  Google的&nbsp;kubeflow  我们今天就来看一看Google推出的Kubeflow。Kubeflow，顾名思义，是Kubernetes + Tensorflow，是Google为了支持自家的Tensorflow的部署而开发出的开源平台，当然它同时也支持Pytorch和基于Python的SKlearn等其它机器学习的引擎。与其它的产品相比较，因为是基于强大的Kubernetes之上构建，Kubeflow的未来和生态系统更值得看好。  Kukeflow主要提供在生产系统中简单的大规模部署机器学习的模型的功能，利用Kubernetes，它可以做到：   简单，可重复，可移植的部署  利用微服务提供松耦合的部署和管理  按需扩大规模  Kubeflow是基于K8S的机器学习工具集，它提供一系列的脚本和配置，来管理K8S的组件。Kubeflow基于K8s的微服务架构，其核心组件包括：   Jupyterhub&nbsp;&nbsp;多租户Nootbook服务  Tensorflow/Pytorch/MPI/MXnet/Chainer&nbsp; 主要的机器学习引擎  Seldon&nbsp;提供在K8s上对于机器学习模型的部署  Argo&nbsp;基于K8s的工作流引擎  Ambassador&nbsp; API Gateway  Istio&nbsp;提供微服务的管理，Telemetry收集  Ksonnet&nbsp; K8s部署工具  基于K8s，扩展其它能力非常方便，Kubeflow提供的其它扩展包括：   Pachyderm&nbsp;基于容器和K8s的数据流水线 （git for data）  Weaveworks flux&nbsp;基于git的配置管理  ... ...   我们可以看出，基于K8s，Kubeflow利用已有的生态系统来构微服务，可以说充分体现了微服务的高度扩展性。 我们下面就来看看Kubeflow是如何整合了这些组件，来提供机器学习模型部署的功能的。 JupyterHub Jupyter Notebook是深受数据科学家喜爱的开发工具，它提供出色的交互和实时反馈。JupyterHub提供一个使用Juypter Notebook的多用户使用环境，它包含以下组件：   多用户Hub  可配置的HTTP代理  多个但用户Notebook server   运行以下的命令通过port-forward访问jyputer hub kubectl port-forward tf-hub-0 8000:8000 -n &lt;ns&gt; 第一次访问，可以创建一个notebook的实例。创建的实例可以选择不同的镜像，可以实现对GPU的支持。同时需要选择配置资源的参数。 创建好的jupyterlab&nbsp;（JupyterLab是新一代的Juypter Notebook）的界面如下：  不过我还是比较习惯传统的notebook界面。Lab的优点是可以开Console，这个不错。（Lab也支持打开传统的notebook界面） Kubeflow在notebook镜像中集成了Tensorboard，可以方便的对tensflow的程序进行可视化和调试。 在jyputerlab的Console中，输入下面的命令开启Tensorboard： tensorboard --logdir &lt;logdir&gt; $ tensorboard --logdir /tmp/logs2018-09-15 20:30:21.186275: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMAW0915 20:30:21.204606 Reloader tf_logging.py:121] Found more than one graph event per run, or there was a metagraph containing a graph_def, as well as one or more graph events.  Overwriting the graph with the newest event.W0915 20:30:21.204929 Reloader tf_logging.py:121] Found more than one metagraph event per run. Overwriting the metagraph with the newest event.W0915 20:30:21.205569 Reloader tf_logging.py:121] Found more than one graph event per run, or there was a metagraph containing a graph_def, as well as one or more graph events.  Overwriting the graph with the newest event.TensorBoard 1.8.0 at http://jupyter-admin:6006 (Press CTRL+C to quit) 访问tensorboard也需要port-forward，这里user是创建notebook的用户名，kubeflow为为一个实例创建一个Pod。缺省的tensorboard的端口是6006。 kubectl port-forward jupyter-&lt;user&gt; 6006:6006 -n &lt;ns&gt;  Tensorflow 训练 为了支持在Kubernete中进行分布式的Tensorflow的训练，Kubeflow开发了K8s的CDR，TFJob&nbsp;（tf-operater）。  如上图所示，分布式的Tensorflow支持0到多个以下的进程：   Chief&nbsp;&nbsp;负责协调训练任务  Ps&nbsp;Parameter servers，参数服务器，为模型提供分布式的数据存储  Worker&nbsp;负责实际训练模型的任务. 在某些情况下 worker 0 可以充当Chief的责任.  Evaluator 负责在训练过程中进行性能评估  下面的yaml配置是Kubeflow提供的一个CNN Benchmarks的例子。 ---apiVersion: kubeflow.org/v1alpha2kind: TFJobmetadata:  labels:    ksonnet.io/component: mycnnjob  name: mycnnjob  namespace: kubeflowspec:  tfReplicaSpecs:    Ps:      template:        spec:          containers:          - args:            - python            - tf_cnn_benchmarks.py            - --batch_size=32            - --model=resnet50            - --variable_update=parameter_server            - --flush_stdout=true            - --num_gpus=1            - --local_parameter_device=cpu            - --device=cpu            - --data_format=NHWC            image: gcr.io/kubeflow/tf-benchmarks-cpu:v20171202-bdab599-dirty-284af3            name: tensorflow            workingDir: /opt/tf-benchmarks/scripts/tf_cnn_benchmarks          restartPolicy: OnFailure      tfReplicaType: PS    Worker:      replicas: 1      template:        spec:          containers:          - args:            - python            - tf_cnn_benchmarks.py            - --batch_size=32            - --model=resnet50            - --variable_update=parameter_server            - --flush_stdout=true            - --num_gpus=1            - --local_parameter_device=cpu            - --device=cpu            - --data_format=NHWC            image: gcr.io/kubeflow/tf-benchmarks-cpu:v20171202-bdab599-dirty-284af3            name: tensorflow            workingDir: /opt/tf-benchmarks/scripts/tf_cnn_benchmarks          restartPolicy: OnFailure 在Kubeflow中运行这个例子，会创建一个TFjob。可以使用Kubectl来管理，监控这个Job的运行。 # 监控当前状态kubectl get -o yaml tfjobs &lt;jobname&gt; -n &lt;ns&gt;# 查看事件kubectl describe tfjobs &lt;jobname&gt; -n &lt;ns&gt;# 查看运行日志kubectl logs mycnnjob-[ps|worker]-0 -n &lt;ns&gt; Tensoflow 服务（Serving） Serving就是指当模型训练好了以后，提供一个稳定的接口，供用户调用，来应用该模型。 基于Tensorflow的Serving功能，Kubeflow提供一个Tensorflow模型服务器（model server）的Ksonnet模块来提供模型服务的功能。  Kukeflow支持使用S3，Google Cloud或者NFS来存贮模型。并支持利用Istio来收集Telemetry。 其它机器学习引擎的支持 虽说Tensorflow是自家的机器学习引擎，但是Google的Kubeflow也提供了对其它不同引擎的支持，包含：   Pytorch&nbsp;&nbsp;PyTorch是由Facebook的人工智能研究小组开发，基于Torch的开源Python机器学习库。  MXnet&nbsp;Apache MXNet是一个现代化的开源深度学习软件框架，用于训练和部署深度神经网络。它具有可扩展性，允许快速模型培训，并支持灵活的编程模型和多种编程语言 MXNet库是可移植的，可以扩展到多个GPU和多台机器。  Chainer&nbsp;&nbsp;Chainer是一个开源深度学习框架，纯粹用Python编写，基于Numpy和CuPy Python库。 该项目由日本风险投资公司Preferred Networks与IBM，英特尔，微软和Nvidia合作开发。 Chainer因其早期采用“按运行定义”方案以及其在大规模系统上的性能而闻名。Kubeflow对Chainer的支持要到下一个版本。现在还在Beta。  MPI&nbsp; 使用MPI来训练Tensorflow。这部分看到的资料比较少。  这些都是用Kubernetes CDRs的形式来支持的，用户只要利用KS创建对应的组件来管理就好了。 Seldon Serving 既然要支持不同的机器学习引擎，当然也不能只提供基于Tensforflow的模型服务，为了提供其它模型服务的能力，Kubeflow集成了Seldon。 Seldon Core是基于K8s的开源的机器学习模型部署平台。 机器学习部署面临许多挑战。 Seldon Core希望帮助应对这些挑战。它的高级目标是：   允许数据科学家使用任何机器学习工具包或编程语言创建模型。我们计划最初涵盖以下工具/语言：       基于Python的模型包括           Tensorflow模型      Sklearn模特          Spark模型    H2O模型    R模型      在部署时通过REST和gRPC自动公开机器学习模型，以便轻松集成到需要预测的业务应用程序中。  允许将复杂的运行时推理图部署为微服务。这些图可以包括：       模型 - 可执行机器学习模型的运行时推理    路由器 - 将API请求路由到子图。示例：AB测试，多武装强盗。    组合器 - 结合子图的响应。示例：模型集合    变形器&nbsp;- 转换请求或响应。示例：转换要素请求。      处理已部署模型的完整生命周期管理：       更新运行时图表，无需停机    缩放    监控    安全      除了提供单模型服务的功能，Seldon还支持AB测试，异常检测等等。  模型部署好了之后，通过API Gateway暴露的endpoint来访问和使用模型。 http://&lt;ambassadorEndpoint&gt;/seldon/&lt;deploymentName&gt;/api/v0.1/predictions  &nbsp; Argo argo是一个开源的基于容器的工作流引擎，并实现为一个K8S的CRD。   用容器实现工作流的每一个步骤  用DAG的形式描述多个任务之间的关系的依赖  支持机器学习和数据处理中的计算密集型任务  无需复杂配置就可以在容器中运行CICD  用容器来实现工作流已经不是什么新鲜事了，codeship就是用容器来实现CICD的每一步。所以Argo很适合CICD。 下图就是一个Argo工作流的例子：  机器学习同样可以抽象为一个或者多个工作流。Kubeflow继承了Argo来作为其机器学习的工作流引擎。 可以通过Kubectl proxy来访问Kubeflow中的Argo UI。&nbsp;http://localhost:8001/api/v1/namespaces/kubeflow/services/argo-ui/proxy/workflows 现阶段，并没有实际的Argo工作流来运行机器学习的例子。但是Kubeflow在使用Argo来做自己的CICD系统。  &nbsp; Pachyderm  Pychyderm是容器化的数据池，提供像git一样的数据版本系统管理，并提供一个数据流水线，来构建你的数据科学项目。 &nbsp; 总结 Kubeflow利用Google自家的两大利器Kubernete和Tensorflow，强强联手，来提供一个数据科学的工具箱和部署平台。我们可以看到他有很多优点：   云优化 - 基于K8s，可以说，所有功能都很容易的在云上扩展。诸如多租户功能，动态扩展，对AWS/GCP的支持等等  利用微服务架构，扩展性强，基于容器，加入心得组件非常容易  出色的DevOps和CICD支持，使用Ksonnet/argo，部署和管理组件和CICD都变得非常轻松  多核心支持，除了我们本文提到的深度学习引擎，Kubeflow很容易扩展新的引擎，例如Caffe2正在开发中。  GPU支持  同时我们也可以看到Kubeflow的一些问题：   组件比较多，缺乏协调，更像是一推工具集合。希望能有一个整合流畅的工作流，能统一各个步骤。  文档还需改善  当然，kubeflow的当前版本是0.2.5，我相信，未来Kubeflow会有很好的发展。 &nbsp; 参考：   kubeflow  open mpi  Kubeflow: Cloud-native machine learning with Kubernetes  Bringing Your Data Pipeline Into The Machine Learning Era  Introducing Argo — A Container-Native Workflow Engine for Kubernetes  Introducing Seldon Deploy  jupyterhub 文档  MPI AND SCALABLE DISTRIBUTED MACHINE LEARNING  Chainer 使复杂神经网络变的简单  pachyderm 文档  https://github.com/fnproject/fn-helm/issues/21</div>
                    </div>
    
        <div class='blog'>
            <h2><a href="#top" class='top'>回到顶部</a><a name="blog_2222908" href="https://my.oschina.net/taogang/blog/2222908">谈谈机器学习模型的部署</a></h2>
            <div class='outline'>
                <div class='date'>时间：2018-10-05 03:00:53</div>
                <div class='catalog'>分类：机器学习</div>
                                                                            </div>
            <div class='content'>随着机器学习的广泛应用，如何高效的把训练好的机器学习的模型部署到生产环境，正在被越来越多的工具所支持。我们今天就来看一看不同的工具是如何解决这个问题的。  上图的过程是一个数据科学项目所要经历的典型的过程。从数据采集开始，经历数据分析，数据变形，数据验证，数据拆分，训练，模型创建，模型验证，大规模训练，模型发布，到提供服务，监控和日志。诸多的机器学习工具如Scikt-Learn，Spark, Tensorflow, MXnet, PyTorch提供给数据科学家们不同的选择，同时也给模型的部署带来了不同的挑战。 我们先来简单的看一看机器学习的模型是如何部署，它又会遇到那些挑战。 模型持久化 模型部署一般就是把训练的模型持久化，然后运行服务器加载模型，并提供REST或其它形式的服务接口。我们以RandomForestClassification为例，看一下Sklearn，Spark和Tensorflow是如何持久化模型。 Sklearn 我们使用Iris数据集，利用RandomForestClassifier分类。 from sklearn.ensemble import RandomForestClassifierfrom sklearn.datasets import load_irisfrom sklearn.model_selection import train_test_splitfrom sklearn.metrics import classification_reportfrom sklearn.externals import joblibdata = load_iris()X, y = data["data"], data["target"]X_train, X_test, y_train, y_test = train_test_split(    X, y, test_size=0.33, random_state=42)clf = RandomForestClassifier(max_depth=2, random_state=0)clf.fit(X_train, y_train)print(clf.feature_importances_)print(classification_report(y_test, clf.predict(    X_test), target_names=data["target_names"]))joblib.dump(clf, 'classification.pkl') 训练的代码如上。这里模型导出的代码在最后一句。joblib.dump()，参考这里。Sklearn的模型到处本质上是利用Python的Pickle机制。Python的函数进行序列化，也就是说把训练好的Transformer函数序列化并存为文件。 要加载模型也很简单，只要调用joblib.load()就好了。 from sklearn.externals import joblibfrom sklearn.datasets import load_irisfrom sklearn.metrics import classification_reportdata = load_iris()X, y = data["data"], data["target"]clf = joblib.load('classification.pkl')print(clf.feature_importances_)print(classification_report(y, clf.predict(    X), target_names=data["target_names"])) Sklearn对Pickle做了一下封装和优化，但这并不能解决Pickle本身的一些限制，例如：   版本兼容问题，不同的Python，Pickle，Sklearn的版本，生成的序列化文件并不兼容  安全性问题，例如序列化的文件中被人注入恶意代码  扩展问题，你自己写了一个扩展类，无法序列化，或者你在Python中调用了C函数  模型的管理，如果我生成了不同版本的模型，该如何管理  Spark Spark的Pipeline和Model都支持Save到文件，然后可以很方便的在另一个Context中加载。 训练的代码如下： from pyspark.ml import Pipelinefrom pyspark.ml.classification import RandomForestClassifierfrom pyspark.ml.feature import IndexToString, StringIndexer, VectorIndexerfrom pyspark.ml.evaluation import MulticlassClassificationEvaluatorfrom pyspark.ml.linalg import Vectorsfrom pyspark.ml.feature import VectorAssemblerfrom pyspark.sql.types import DoubleTypefrom pyspark import SparkFilesfrom pyspark import SparkContexturl = "https://server/iris.csv"spark.sparkContext.addFile(url)# Load and parse the data file, converting it to a DataFrame.data = spark.read.csv(SparkFiles.get("iris.csv"), header=True)data = data.withColumn("sepal_length", data["sepal_length"].cast(DoubleType()))data = data.withColumn("sepal_width", data["sepal_width"].cast(DoubleType()))data = data.withColumn("petal_width", data["petal_width"].cast(DoubleType()))data = data.withColumn("petal_length", data["petal_length"].cast(DoubleType()))#data.show()data.printSchema()assembler = VectorAssembler(    inputCols=["sepal_length", "sepal_width", "petal_width", "petal_length"],    outputCol="features")output = assembler.transform(data)# Index labels, adding metadata to the label column.# Fit on whole dataset to include all labels in index.labelIndexer = StringIndexer(inputCol="species", outputCol="indexedLabel").fit(output)# Automatically identify categorical features, and index them.# Set maxCategories so features with &gt; 4 distinct values are treated as continuous.featureIndexer =\    VectorIndexer(inputCol="features", outputCol="indexedFeatures", maxCategories=4).fit(output)# Split the data into training and test sets (30% held out for testing)(trainingData, testData) = data.randomSplit([0.7, 0.3])# Train a RandomForest model.rf = RandomForestClassifier(labelCol="indexedLabel", featuresCol="indexedFeatures", numTrees=10)# Convert indexed labels back to original labels.labelConverter = IndexToString(inputCol="prediction", outputCol="predictedLabel",                               labels=labelIndexer.labels)# Chain indexers and forest in a Pipelinepipeline = Pipeline(stages=[assembler, labelIndexer, featureIndexer, rf, labelConverter])# Train model.  This also runs the indexers.model = pipeline.fit(trainingData)# Make predictions.predictions = model.transform(testData)# Select example rows to display.predictions.select("predictedLabel", "species", "features").show(5)# Select (prediction, true label) and compute test errorevaluator = MulticlassClassificationEvaluator(    labelCol="indexedLabel", predictionCol="prediction", metricName="accuracy")accuracy = evaluator.evaluate(predictions)print("Test Error = %g" % (1.0 - accuracy))rfModel = model.stages[3]print(rfModel)  # summary onlyfilebase="hdfs://server:9000/tmp"pipeline.write().overwrite().save("{}/classification-pipeline".format(filebase))model.write().overwrite().save("{}/classification-model".format(filebase)) 模型加载的代码如下： %pysparkfrom pyspark.ml import Pipelinefrom pyspark.ml import PipelineModelfrom pyspark import SparkFilesurl = "https://server/iris.csv"spark.sparkContext.addFile(url)# Load and parse the data file, converting it to a DataFrame.data = spark.read.csv(SparkFiles.get("iris.csv"), header=True)data = data.withColumn("sepal_length", data["sepal_length"].cast(DoubleType()))data = data.withColumn("sepal_width", data["sepal_width"].cast(DoubleType()))data = data.withColumn("petal_width", data["petal_width"].cast(DoubleType()))data = data.withColumn("petal_length", data["petal_length"].cast(DoubleType()))filebase="hdfs://server:9000/tmp/"pipeline = Pipeline.read().load("{}/classification-pipeline".format(filebase))model = PipelineModel.read().load("{}/classification-model".format(filebase))# Make predictions.predictions = model.transform(data)# Select example rows to display.predictions.select("predictedLabel", "species", "features").show(5)# Select (prediction, true label) and compute test errorevaluator = MulticlassClassificationEvaluator(    labelCol="indexedLabel", predictionCol="prediction", metricName="accuracy")accuracy = evaluator.evaluate(predictions)print("Test Error = %g" % (1.0 - accuracy)) 调用model的toDebugString方法可以看到分类器的内部细节。 RandomForestClassificationModel (uid=rfc_225ef4968bf9) with 10 trees  Tree 0 (weight 1.0):    If (feature 3 &lt;= 1.9)     Predict: 1.0    Else (feature 3 &gt; 1.9)     If (feature 3 &lt;= 4.7)      Predict: 2.0     Else (feature 3 &gt; 4.7)      If (feature 3 &lt;= 5.1)       If (feature 1 &lt;= 2.5)        Predict: 0.0       Else (feature 1 &gt; 2.5)        If (feature 1 &lt;= 2.7)         Predict: 2.0        Else (feature 1 &gt; 2.7)         Predict: 0.0      Else (feature 3 &gt; 5.1)       Predict: 0.0  Tree 1 (weight 1.0):    If (feature 3 &lt;= 1.9)     Predict: 1.0    Else (feature 3 &gt; 1.9)     If (feature 3 &lt;= 4.9)      If (feature 2 &lt;= 1.6)       Predict: 2.0      Else (feature 2 &gt; 1.6)       If (feature 0 &lt;= 4.9)        Predict: 0.0       Else (feature 0 &gt; 4.9)        If (feature 0 &lt;= 5.9)         Predict: 2.0        Else (feature 0 &gt; 5.9)         Predict: 0.0     Else (feature 3 &gt; 4.9)      If (feature 1 &lt;= 3.0)       If (feature 3 &lt;= 5.1)        If (feature 2 &lt;= 1.7)         Predict: 0.0        Else (feature 2 &gt; 1.7)         Predict: 0.0       Else (feature 3 &gt; 5.1)        Predict: 0.0      Else (feature 1 &gt; 3.0)       Predict: 0.0  Tree 2 (weight 1.0):    If (feature 3 &lt;= 1.9)     Predict: 1.0    Else (feature 3 &gt; 1.9)     If (feature 3 &lt;= 5.0)      If (feature 2 &lt;= 1.6)       Predict: 2.0      Else (feature 2 &gt; 1.6)       If (feature 1 &lt;= 2.5)        Predict: 0.0       Else (feature 1 &gt; 2.5)        Predict: 2.0     Else (feature 3 &gt; 5.0)      If (feature 0 &lt;= 6.0)       If (feature 1 &lt;= 2.7)        If (feature 0 &lt;= 5.8)         Predict: 0.0        Else (feature 0 &gt; 5.8)         Predict: 2.0       Else (feature 1 &gt; 2.7)        Predict: 0.0      Else (feature 0 &gt; 6.0)       Predict: 0.0  Tree 3 (weight 1.0):    If (feature 3 &lt;= 1.9)     Predict: 1.0    Else (feature 3 &gt; 1.9)     If (feature 3 &lt;= 4.9)      If (feature 2 &lt;= 1.5)       Predict: 2.0      Else (feature 2 &gt; 1.5)       If (feature 2 &lt;= 1.7)        Predict: 0.0       Else (feature 2 &gt; 1.7)        Predict: 2.0     Else (feature 3 &gt; 4.9)      If (feature 3 &lt;= 5.1)       If (feature 0 &lt;= 6.5)        If (feature 0 &lt;= 5.9)         Predict: 0.0        Else (feature 0 &gt; 5.9)         Predict: 0.0       Else (feature 0 &gt; 6.5)        Predict: 2.0      Else (feature 3 &gt; 5.1)       Predict: 0.0  Tree 4 (weight 1.0):    If (feature 2 &lt;= 0.5)     Predict: 1.0    Else (feature 2 &gt; 0.5)     If (feature 2 &lt;= 1.5)      If (feature 2 &lt;= 1.4)       Predict: 2.0      Else (feature 2 &gt; 1.4)       If (feature 3 &lt;= 4.9)        Predict: 2.0       Else (feature 3 &gt; 4.9)        Predict: 0.0     Else (feature 2 &gt; 1.5)      If (feature 2 &lt;= 1.8)       If (feature 3 &lt;= 5.0)        If (feature 0 &lt;= 4.9)         Predict: 0.0        Else (feature 0 &gt; 4.9)         Predict: 2.0       Else (feature 3 &gt; 5.0)        Predict: 0.0      Else (feature 2 &gt; 1.8)       Predict: 0.0  Tree 5 (weight 1.0):    If (feature 2 &lt;= 0.5)     Predict: 1.0    Else (feature 2 &gt; 0.5)     If (feature 2 &lt;= 1.6)      If (feature 2 &lt;= 1.3)       Predict: 2.0      Else (feature 2 &gt; 1.3)       If (feature 3 &lt;= 4.9)        Predict: 2.0       Else (feature 3 &gt; 4.9)        Predict: 0.0     Else (feature 2 &gt; 1.6)      If (feature 3 &lt;= 4.8)       If (feature 2 &lt;= 1.7)        Predict: 0.0       Else (feature 2 &gt; 1.7)        Predict: 2.0      Else (feature 3 &gt; 4.8)       Predict: 0.0  Tree 6 (weight 1.0):    If (feature 3 &lt;= 1.9)     Predict: 1.0    Else (feature 3 &gt; 1.9)     If (feature 3 &lt;= 4.9)      If (feature 2 &lt;= 1.6)       Predict: 2.0      Else (feature 2 &gt; 1.6)       If (feature 1 &lt;= 2.8)        Predict: 0.0       Else (feature 1 &gt; 2.8)        Predict: 2.0     Else (feature 3 &gt; 4.9)      If (feature 1 &lt;= 2.7)       If (feature 2 &lt;= 1.6)        If (feature 3 &lt;= 5.0)         Predict: 0.0        Else (feature 3 &gt; 5.0)         Predict: 2.0       Else (feature 2 &gt; 1.6)        Predict: 0.0      Else (feature 1 &gt; 2.7)       Predict: 0.0  Tree 7 (weight 1.0):    If (feature 0 &lt;= 5.4)     If (feature 2 &lt;= 0.5)      Predict: 1.0     Else (feature 2 &gt; 0.5)      Predict: 2.0    Else (feature 0 &gt; 5.4)     If (feature 2 &lt;= 1.7)      If (feature 3 &lt;= 1.5)       Predict: 1.0      Else (feature 3 &gt; 1.5)       If (feature 0 &lt;= 6.9)        If (feature 3 &lt;= 5.0)         Predict: 2.0        Else (feature 3 &gt; 5.0)         Predict: 0.0       Else (feature 0 &gt; 6.9)        Predict: 0.0     Else (feature 2 &gt; 1.7)      If (feature 0 &lt;= 5.9)       If (feature 2 &lt;= 1.8)        Predict: 2.0       Else (feature 2 &gt; 1.8)        Predict: 0.0      Else (feature 0 &gt; 5.9)       Predict: 0.0  Tree 8 (weight 1.0):    If (feature 3 &lt;= 1.7)     Predict: 1.0    Else (feature 3 &gt; 1.7)     If (feature 3 &lt;= 5.1)      If (feature 2 &lt;= 1.6)       If (feature 2 &lt;= 1.4)        Predict: 2.0       Else (feature 2 &gt; 1.4)        If (feature 1 &lt;= 2.2)         Predict: 0.0        Else (feature 1 &gt; 2.2)         Predict: 2.0      Else (feature 2 &gt; 1.6)       If (feature 1 &lt;= 2.5)        Predict: 0.0       Else (feature 1 &gt; 2.5)        If (feature 3 &lt;= 5.0)         Predict: 2.0        Else (feature 3 &gt; 5.0)         Predict: 0.0     Else (feature 3 &gt; 5.1)      Predict: 0.0  Tree 9 (weight 1.0):    If (feature 2 &lt;= 0.5)     Predict: 1.0    Else (feature 2 &gt; 0.5)     If (feature 0 &lt;= 6.1)      If (feature 3 &lt;= 4.8)       If (feature 0 &lt;= 4.9)        If (feature 2 &lt;= 1.0)         Predict: 2.0        Else (feature 2 &gt; 1.0)         Predict: 0.0       Else (feature 0 &gt; 4.9)        Predict: 2.0      Else (feature 3 &gt; 4.8)       Predict: 0.0     Else (feature 0 &gt; 6.1)      If (feature 3 &lt;= 4.9)       If (feature 1 &lt;= 2.8)        If (feature 0 &lt;= 6.2)         Predict: 0.0        Else (feature 0 &gt; 6.2)         Predict: 2.0       Else (feature 1 &gt; 2.8)        Predict: 2.0      Else (feature 3 &gt; 4.9)       Predict: 0.0 下图是Spark存储的Piple模型的目录结构：  我们可以看到，它包含了元数据Pipeline的五个阶段的数据，这里的文件都是二进制的数据，只有Spark自己可以加载。 Tensorflow 最后我们来看一下Tensorflow。Tensorflow提供了tf.train.Saver来导出他的模型到元图（MetaGraph）。 from __future__ import print_functionimport tensorflow as tffrom tensorflow.contrib.tensor_forest.python import tensor_forestfrom tensorflow.python.ops import resourcesfrom sklearn.datasets import load_irisfrom sklearn.model_selection import train_test_splitimport numpy as np# Ignore all GPUs, tf random forest does not benefit from it.import osos.environ["CUDA_VISIBLE_DEVICES"] = ""data = load_iris()dX, dy = data["data"], data["target"]X_train, X_test, y_train, y_test = train_test_split(    dX, dy, test_size=0.33, random_state=42)# Parametersnum_steps = 500  # Total steps to trainbatch_size = 10  # The number of samples per batchnum_classes = 3  # The 10 digitsnum_features = 4  # Each image is 28x28 pixelsnum_trees = 10max_nodes = 100# Input and Target dataX = tf.placeholder(tf.float32, shape=[None, num_features])# For random forest, labels must be integers (the class id)Y = tf.placeholder(tf.int32, shape=[None])# Random Forest Parametershparams = tensor_forest.ForestHParams(num_classes=num_classes,                                      num_features=num_features,                                      num_trees=num_trees,                                      max_nodes=max_nodes).fill()# Build the Random Forestforest_graph = tensor_forest.RandomForestGraphs(hparams)# Get training graph and losstrain_op = forest_graph.training_graph(X, Y)loss_op = forest_graph.training_loss(X, Y)# Measure the accuracyinfer_op, _, _ = forest_graph.inference_graph(X)correct_prediction = tf.equal(tf.argmax(infer_op, 1), tf.cast(Y, tf.int64))accuracy_op = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))# Initialize the variables (i.e. assign their default value) and forest resourcesinit_vars = tf.group(tf.global_variables_initializer(),                     resources.initialize_resources(resources.shared_resources()))def next_batch(size):    index = range(len(X_train))    index_batch = np.random.choice(index, size)    return X_train[index_batch], y_train[index_batch]# Start TensorFlow sessionsess = tf.Session()# Run the initializersess.run(init_vars)saver = tf.train.Saver()# Trainingfor i in range(1, num_steps + 1):    # Prepare Data    # Get the next batch of MNIST data (only images are needed, not labels)    batch_x, batch_y = next_batch(batch_size)    _, l = sess.run([train_op, loss_op], feed_dict={X: batch_x, Y: batch_y})    if i % 50 == 0 or i == 1:        acc = sess.run(accuracy_op, feed_dict={X: batch_x, Y: batch_y})        print('Step %i, Loss: %f, Acc: %f' % (i, l, acc))# Test Modelprint("Test Accuracy:", sess.run(    accuracy_op, feed_dict={X: X_test, Y: y_test}))# Print the tensors related to this modelprint(accuracy_op)print(infer_op)print(X)print(Y)# save the model to a check point filesave_path = saver.save(sess, "/tmp/model.ckpt") 导出的模型会包含以下文件：  其中checkpoint是元数据，包含其它文件的路径信息。还包含了一个Pickle文件和其它几个checkpiont文件。可以看出，Tensorflow也利用了Python的Pickle机制来存储模型，并在这之外加入了额外的元数据。 模型加载的代码如下： from __future__ import print_functionimport tensorflow as tffrom sklearn.datasets import load_irisfrom sklearn.model_selection import train_test_splitfrom sklearn.metrics import classification_report# note: this has to be imported in case to support forest graphfrom tensorflow.contrib.tensor_forest.python import tensor_forestimport osos.environ["CUDA_VISIBLE_DEVICES"] = ""saver = tf.train.import_meta_graph('/tmp/model.ckpt.meta')data = load_iris()dX, dy = data["data"], data["target"]graph = tf.get_default_graph()with tf.Session() as sess:    new_saver = tf.train.import_meta_graph('/tmp/model.ckpt.meta')    new_saver.restore(sess, '/tmp/model.ckpt')    #input = graph.get_operation_by_name("train")    # print(graph.as_graph_def())    load_infer_op = graph.get_tensor_by_name('probabilities:0')    accuracy_op = graph.get_tensor_by_name('Mean_1:0')    X = graph.get_tensor_by_name('Placeholder:0')    Y = graph.get_tensor_by_name('Placeholder_1:0')    print("Test Accuracy:", sess.run(accuracy_op, feed_dict={X: dX, Y: dy}))    result = sess.run(load_infer_op, feed_dict={X: dX})    prediction_result = [i.argmax() for i in result]    print(classification_report(dy, prediction_result,                                target_names=data["target_names"])) 这里要注意的是，RandomForest不是tensforflow的核心包，所以在模型加载的时候必须tensorflow.contrib.tensor_forest.python.tensor_forest, 否则模型是无法成功加载的。因为不加载的话tensor_forest中定义的一些属性会缺失。 另外就是Tensorflow也可以存储计算图，调用tf.train.write_graph（）方法可以把图定义存储下来。当然也可以在TesnsorBoard中展示该图。 tf.train.write_graph(sess.graph_def, '/tmp', 'train.pbtxt')%cat /tmp/train.pbtxt &nbsp; 好了，我们看到，Sklearn，Spark和Tensorflow都提供了自己的模型持久化的方法，那么简单来说，只要使用一个web服务器例如Flask，加一些模型加载和管理的方法，然后暴露REST API就可以提供预测服务了，是不是很简单呢？ 其实要在生产环境下提供服务，还需要面对很多其它的挑战，例如：   在云上如何扩展和伸缩  如何进行性能调优  如何管理模型的版本  安全性  如何持续集成和持续部署  如何支持AB测试  为了解决模型部署的挑战，不同的组织开发了一些开源的工具，例如：Clipper，Seldon，MFlow，MLeap，Oracle Graphpipe，MXnet model server&nbsp;等等，我们就选其中几个看个究竟。 Clipper Clipper是由UC Berkeley&nbsp;RISE Lab&nbsp;开发的， 在用户应用和机器学习模型之间的一个提供预测服务的系统，通过解耦合用户应用和机器学习系统的方式，简化部署流程。 它有以下功能：   利用简单标准化的REST接口来简化机器学习系统的集成，支持主要的机器学习框架。  使用开发模型相同的库和环境简化模型部署  利用可适配的Batching，缓存等技术改善吞吐量  通过智能选择和合并模型来改善预测的准确率  Clipper的架构如下图：  Clipper使用了容器和微服务技术来构架架构。使用Redis来管理配置，Prometheus来进行监控。Clipper支持使用Kubernetes或者本地的Docker来管理容器。 Clipper支持以下几种模型：   纯Python函数  PyShark  PyTorch  Tensorflow  MXnet  自定义  Clipper模型部署的基本过程如下，大家可以参考我的这个notebook   创建Clipper集群（使用K8s或者本地Docker）  创建一个应用  训练模型  调用Clipper提供的模型部署方法部署模型，这里不同的工具需要调用不同的部署方法。部署时，会把训练好的Estimator利用CloudPickle之久化，本地构建一个容器镜像，部署到Docker或者K8s。  把模型和应用关联到一起，相当于发布模型。然后就可以调用对应的REST API来做预测了。  我试着把之前的三种工具的RomdomForest的例子用Clipper发布到我的Kubernetes集群，踩到了以下的坑坑：   我本地的Cloudpickle的版本太新，导致模型不能反序列化，参考这个Issue  Tensorflow在Pickle的时候失败，应该是调用了C的code  我的K8s运行在AWS上，我在K8S上使用内部IP失败，clipper连接一直在使用外部的域名，导致无法部署PySpark的模型。  总之，除了Sklearn成功部署之外，Tensorflow和Spark都失败了。大家可以参考这里的例子。 Seldon Seldon是一家创办于伦敦的公司，致力于提供对于基于开源软件的机器学习系统的控制。Seldon Core是该公司开源的提供在Kubernetes上部署机器学习模型的工具。它拥有以下功能：   Python/Spark/H2O/R 的模型支持  REST API和gRPC接口  部署基于Model/Routers/Combiner/Transformers的图的微服务  利用K8S来提供扩展，安全性，监控等等DevOps的功能   Seldon的使用过程如上图，   首先在K8s上安装Seldon Core，Seldon利用ksonnet，以CRD的形式安装seldon core  利用S2i（s2i是openshift开源的一款工具，用于把代码构建成容器镜像），构建运行时模型容器，并注册到容器注册表  编写你的运行图，并提交到K8s来部署你的模型  Seldon支持基于四种基本单元，Model，Transformer， Router， Combiner来构建你的运行图，并按照该图在K8s创建对应的资源和实例，来获得AB测试，模型ensemble的功能。 例如下图的几个例子： AB 测试  模型ensemble  复杂图  图模式是Seldon最大的亮点，可以训练不同的模型，然后利用图来组合出不同的运行时，非常方便。更多的例子参考这里。 笔者尝试在K8S上利用Seldon部署之前提到的三种工具生成的模型，都获得了成功（代码在这里）。这里分享一下遇到的几个问题：   Seldon支持Java的Python，然而用运行PySpark，这两个都需要，所以我不得不自己构建了一个镜像，手工在Python镜像上安装Java。  因为使用CDR的原因，我没有找到有效改变容器的liveness和readiness的设置，因为Spark初始化模型在Hadoop上，加载模型需要时间，总是readiness超时导致容器无法正常启动，K8s不断的重启容器。所以我只好修改代码，让模型加载变成Lazy Load，但是这样第一次REST Call会比较耗时，但是容器和服务总算是能够正常启动。  在我之前的一篇介绍Kubeflow的文章中，大家可以了解到，Kubeflow就是使用Seldon来管理模型部署的。 MLflow MLflow是Databricks开发的开源系统，用于管理机器学习的端到端的生命周期。我之前写过一篇介绍该工具的文章。 MLflow提供跟踪，项目管理和模型管理的功能。使用MLFlow来提供一个基于Sklearn的模型服务非常简单， from __future__ import print_functionfrom sklearn.ensemble import RandomForestClassifierfrom sklearn.datasets import load_irisfrom sklearn.model_selection import train_test_splitfrom sklearn.metrics import classification_reportimport mlflowimport mlflow.sklearnif __name__ == "__main__":    data = load_iris()    X, y = data["data"], data["target"]    X_train, X_test, y_train, y_test = train_test_split(        X, y, test_size=0.33, random_state=42)    clf = RandomForestClassifier(max_depth=2, random_state=0)    clf.fit(X_train, y_train)    print(clf.feature_importances_)    print(classification_report(y_test, clf.predict(        X_test), target_names=data["target_names"]))    mlflow.sklearn.log_model(clf, "model")    print("Model saved in run %s" % mlflow.active_run().info.run_uuid) 调用mlflow.sklearn.log_model(), MLflow创建以下的目录来管理模型：  我们看到在artifacts目录下有Python的pickle文件和另一个元数据文件，MLModel。 artifact_path: modelflavors:  python_function:    data: model.pkl    loader_module: mlflow.sklearn    python_version: 2.7.10  sklearn:    pickled_model: model.pkl    sklearn_version: 0.20.0run_id: 44ae85c084904b4ea5bad5aa42c9ce05utc_time_created: '2018-10-02 23:38:49.786871' 使用 mlflow sklearn serve -m model 就可以很方便的提供基于sklearn的模型服务了。 虽然MLFlow也号称支持Spark和Tensorflow，但是他们都是基于Python来做，我尝试使用，但是文档和例子比较少，所以没能成功。但原理上都是使用Pickle➕元数据的方式。大家有兴趣的可以尝试一下。 关于部署功能，MLFlow的一个亮点是和Sagemaker，AzureML的支持。 MLeap MLeap的目标是提供一个在Spark和Sklearn之间可移植的模型格式，和运行引擎。它包含：   基于JSON的序列化  运行引擎  Benchmark  MLeap的架构如下图：  这是一个使用MLeap导出Sklearn模型的例子： # Initialize MLeap libraries before Scikit/Pandasimport mleap.sklearn.preprocessing.dataimport mleap.sklearn.pipelinefrom mleap.sklearn.ensemble import forestfrom mleap.sklearn.preprocessing.data import FeatureExtractor# Import Scikit Transformer(s)import pandas as pdfrom sklearn.pipeline import Pipelinefrom sklearn.ensemble import RandomForestClassifierdata = pd.read_csv('https://raw.githubusercontent.com/mwaskom/seaborn-data/master/iris.csv')input_features = ['sepal_length', 'sepal_width', 'petal_length', 'petal_width']output_vector_name = 'extracted_features' # Used only for serialization purposesoutput_features = [x for x in input_features]feature_extractor_tf = FeatureExtractor(input_scalars=input_features,                                        output_vector=output_vector_name,                                        output_vector_items=output_features)classification_tf = RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',                       max_depth=2, max_features='auto', max_leaf_nodes=None,                       min_impurity_decrease=0.0, min_impurity_split=None,                       min_samples_leaf=1, min_samples_split=2,                       min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=1,                       oob_score=False, random_state=0, verbose=0, warm_start=False)classification_tf.mlinit(input_features="features", prediction_column='species',feature_names="features")rf_pipeline = Pipeline([(feature_extractor_tf.name, feature_extractor_tf),                        (classification_tf.name, classification_tf)])rf_pipeline.mlinit()rf_pipeline.fit(data[input_features],data['species'])rf_pipeline.serialize_to_bundle('./', 'mleap-scikit-rf-pipeline', init=True) 导出的模型结构如下图所示：  这个是randonforest的模型json {   "attributes": {      "num_features": {         "long": 4      },       "trees": {         "type": "list",          "string": [            "tree0",             "tree1",             "tree2",             "tree3",             "tree4",             "tree5",             "tree6",             "tree7",             "tree8",             "tree9"         ]      },       "tree_weights": {         "double": [            1.0,             1.0,             1.0,             1.0,             1.0,             1.0,             1.0,             1.0,             1.0,             1.0         ],          "type": "list"      }   },    "op": "random_forest_classifier"} 我们可以看出MLeap把模型完全序列化成与代码无关的JSON文件，这样就可以在不同的运行时工具Spark/Sklearn之间做到可移植。 MLeap对模型提供服务，不需要依赖任何Sklearn或者Spark的代码。只要启动MLeap的Server，然后提交模型就好了。 docker run -p 65327:65327 -v /tmp/models:/models combustml/mleap-serving:0.9.0-SNAPSHOTcurl -XPUT -H "content-type: application/json" \-d '{"path":"/models/yourmodel.zip"}' \http://localhost:65327/model 下面的代码用Scala在Spark 上训练一个同样的Randonforest分类模型，并利用MLeap持久化模型。 import org.apache.spark.ml.feature.VectorAssemblerimport org.apache.spark.ml.linalg.Vectorsimport org.apache.spark.sql.types.{IntegerType, DoubleType}import org.apache.spark.ml.Pipelineimport org.apache.spark.ml.classification.{RandomForestClassificationModel, RandomForestClassifier}import org.apache.spark.ml.evaluation.MulticlassClassificationEvaluatorimport org.apache.spark.ml.feature.{IndexToString, StringIndexer, VectorIndexer}import ml.combust.bundle.BundleFileimport ml.combust.bundle.serializer.SerializationFormatimport ml.combust.mleap.spark.SparkSupport._import resource._import org.apache.spark.SparkFilesspark.sparkContext.addFile("https://s3-us-west-2.amazonaws.com/mlapi-samples/demo/data/input/iris.csv")val data = spark.read.format("csv").option("header", "true").load(SparkFiles.get("iris.csv"))//data.show()//data.printSchema()// Transform, convert string coloumn to number// this transform is not part of the pipelineval featureDf = data.select(data("sepal_length").cast(DoubleType).as("sepal_length"),                            data("sepal_width").cast(DoubleType).as("sepal_width"),                            data("petal_width").cast(DoubleType).as("petal_width"),                            data("petal_length").cast(DoubleType).as("petal_length"),                            data("species") )// assember the featuresval assembler = new VectorAssembler()  .setInputCols(Array("sepal_length", "sepal_width", "petal_width", "petal_length"))  .setOutputCol("features")  val output = assembler.transform(featureDf)// create lable and featuresval labelIndexer = new StringIndexer()  .setInputCol("species")  .setOutputCol("indexedLabel")  .fit(output)val featureIndexer = new VectorIndexer()  .setInputCol("features")  .setOutputCol("indexedFeatures")  .setMaxCategories(4)  .fit(output)  // Split the data into training and test sets (30% held out for testing).val Array(trainingData, testData) = featureDf.randomSplit(Array(0.7, 0.3))// Train a RandomForest model.val rf = new RandomForestClassifier()  .setLabelCol("indexedLabel")  .setFeaturesCol("indexedFeatures")  .setNumTrees(10)// Convert indexed labels back to original labels.val labelConverter = new IndexToString()  .setInputCol("prediction")  .setOutputCol("predictedLabel")  .setLabels(labelIndexer.labels)// Chain indexers and forest in a Pipeline.val pipeline = new Pipeline()  .setStages(Array(assembler, labelIndexer, featureIndexer, rf, labelConverter))// Train model. This also runs the indexers.val model = pipeline.fit(trainingData)// Make predictions.val predictions = model.transform(testData)// Select example rows to display.predictions.select("predictedLabel", "species", "features").show(5)// Select (prediction, true label) and compute test error.val evaluator = new MulticlassClassificationEvaluator()  .setLabelCol("indexedLabel")  .setPredictionCol("prediction")  .setMetricName("accuracy")val accuracy = evaluator.evaluate(predictions)println("Test Error = " + (1.0 - accuracy))val rfModel = model.stages(3).asInstanceOf[RandomForestClassificationModel]println("Learned classification forest model:\n" + rfModel.toDebugString)val pipelineModel = SparkUtil.createPipelineModel(Array(model))for(bundle &lt;- managed(BundleFile("file:/tmp/mleap-examples/rf"))) {  pipelineModel.writeBundle.format(SerializationFormat.Json).save(bundle)} 导出的模型和之前的Sklearn具有相同的格式。 MLeap的问题在于要支持所有的算法，对于每一个算法都要实现对应的序列化，这也使得它的需要很多的开发来支持客户自定义的算法。对于常用算法的支持，大家可以参考这里。 &nbsp; 其它 除了以上几个，还有一些我们没有涉及，有兴趣的读者可以自行搜索。    Model Server for Apache MXNet&nbsp;(MMS)    DeepDetect    Oracle GraphPipe   总结 Seldon Core和K8S结合的很好，它提供的运行图的方式非常强大，它也是我实验中唯一一个能够成功部署Sklearn，Spark和Tensorflow三种模型的工具，非常推荐！ Clipper提供基于K8s和Docker的模型部署，它的模型版本管理做得不错，但是代码不太稳定，小问题不少，基于CloudPickle也有不少的限制，只能支持Python也是个问题。推荐给数据科学家有比较多的本地交互的情况。 MLFlow能够提供很方便的基于Python的模型服务，但是缺乏和容器的结合。但是它能够支持和Sagemaker，AzureML等云的支持。推荐给已经在使用这些云的玩家。 MLeap的特色是支持模型的可交互性，也就是说我可以把sklearn训练的模型导出在Spark上运行，这的功能很有吸引力，但是要支持全部的算法，它还有很长的路要走。关于机器学习模型标准化的问题，大家也可以关注PMML。现阶段各个工具对PMML的支持比较有限，随着深度学习的广泛应用，PMML何去何从还未可知。 下表是对以上几个工具的简单总结，供大家参考          &nbsp;     Model Persistent      ML Tools      Kubernetest Integration      Version      License      Implementation            Seldon Core      S2i + Pickle      Tensorflow, SKlearn, Keras, R, H2O, Nodejs, PMML      Yes      0.3.2      Apache      Docker + K8s CRD            Clipper      Pickle      Python, PySpark, PyTorch, Tensorflow, MXnet, Customer Container      Yes      0.3.0      Apache      CPP / Python            MLFlow      Directory + Metadata      Python, H2O, Kera, MLeap, PyTorch, Sklearn, Spark, Tensorflow, R      No      Alpha      Apache      Python            MLeap     &nbsp;JSON     Spark,Sklearn, Tensorflow      No      0.12.0      Apache      Scala/Java        &nbsp; 参考    https://github.com/hiveml/simple-ml-serving    https://medium.com/@vikati/the-rise-of-the-model-servers-9395522b6c58    https://qconsp.com/system/files/presentation-slides/qconsp18-deployingml-may18-npentreath.pdf    https://www.slideshare.net/dscrankshaw/veloxampcamp5-final    https://github.com/jpmml/jpmml-sklearn    https://www.ibm.com/developerworks/cn/opensource/ind-PMML1/index.html    https://cmry.github.io/notes/serialize    https://cmry.github.io/notes/serialize-sk</div>
                    </div>
    
        <div class='blog'>
            <h2><a href="#top" class='top'>回到顶部</a><a name="blog_2248506" href="https://my.oschina.net/taogang/blog/2248506">在浏览器中进行深度学习：TensorFlow.js (八）生成对抗网络 （GAN）</a></h2>
            <div class='outline'>
                <div class='date'>时间：2018-10-18 03:05:22</div>
                <div class='catalog'>分类：机器学习</div>
                                                                            </div>
            <div class='content'>Generative&nbsp;Adversarial&nbsp;Network 是深度学习中非常有趣的一种方法。GAN最早源自Ian Goodfellow的这篇论文。LeCun对GAN给出了极高的评价：   “There are many interesting recent development in deep learning…The most important one, in my opinion, is adversarial training (also called GAN for Generative Adversarial Networks). This, and the variations that are now being proposed is the most interesting idea in the last 10 years in ML, in my opinion.”&nbsp;–&nbsp;Yann LeCun  那么我们就看看GAN究竟是怎么回事吧：  如上图所示，GAN包含两个互相对抗的网络：G（Generator）和D（Discriminator）。正如它的名字所暗示的那样，它们的功能分别是：   Generator是一个生成器的网络，它接收一个随机的噪声，通过这个噪声生成图片，记做G(z)。  Discriminator是一个鉴别器网络，判别一张图片或者一个输入是不是“真实的”。它的输入x是数据或者图片，输出D（x）代表x为真实图片的概率，如果为1，就代表100%是真实的图片，而输出为0，就代表不可能是真实的图片。  在训练过程中，生成网络G的目标就是尽量生成真实的图片去欺骗判别网络D。而D的目标就是尽量把G生成的图片和真实的图片分别开来。这样，G和D构成了一个动态的“博弈过程”。在最理想的状态下，G可以生成足以“以假乱真”的图片G(z)。对于D来说，它难以判定G生成的图片究竟是不是真实的，因此D(G(z)) = 0.5。 最后，我们就可以使用生成器和随机输入来生成不同的数据或者图片了。 上面的描述大家可能都能理解，但是把它变成数学语言，可能你就蒙B了。  如上图所示，x是输入，z是随机噪声。D(x)是鉴别器的判定数据为真的概率，D(G(z))是判定生成数据为真的概率。生成器希望这个D(G(z))越大越好，这个时候整个表达式的值应该变小。而鉴别器的目的是能够有效区分真实数据和假数据，所以D(x)应该趋向于变大，D(G(z))趋向于变小，整个表达式就变大。也就是说训练过程，生成器和辨别器互相对抗，一个使上述表达式变小，另一个使其变大，最后训练趋向于平衡，而生成器这时候应该生成真假难辨的数据，这就是我们的最终目的。  上图是GAN算法训练的具体过程，这里我们不做过多的解释，直接运行一个例子。  我们用MINST数据集来看看如何使用TensorflowJS来训练一个GAN，模拟生成手写数字。 代码见我的codepen function gen(xs) {  const l1 = tf.leakyRelu(xs.matMul(G1w).add(G1b));  const l2 = tf.leakyRelu(l1.matMul(G2w).add(G2b));  const l3 = tf.tanh(l2.matMul(G3w).add(G3b));  return l3;}function disReal(xs) {  const l1 = tf.leakyRelu(xs.matMul(D1w).add(D1b));  const l2 = tf.leakyRelu(l1.matMul(D2w).add(D2b));  const logits = l2.matMul(D3w).add(D3b);  const output = tf.sigmoid(logits);  return [logits, output];}function disFake(xs) {  return disReal(gen(xs));} GAN的两个网络分别用gen和disReal创建。gen是生成器网络，disReal是辨别器的网络。disFake是把生成数据用辨别器来辨别。这里的网络使用leakyrelu。使得输出在-inf到+inf，利用sigmoid映射到【0，1】，这是辨别器模型输出一个0-1之间的概率。  &nbsp; 通常我们会创建一个比生成器更复杂的鉴别器网络使得鉴别器有足够的分辨能力。但在这个例子里，两个网络的复杂程度类似。 计算损失的函数使用 tf.sigmoidCrossEntropyWithLogits，值得注意的是，在最新的0.13版本中，这个交叉熵被移除了，你需要自己实现该方法。 训练过程如下： async function trainBatch(realBatch, fakeBatch) {  const dcost = dOptimizer.minimize(    () =&gt; {      const [logitsReal, outputReal] = disReal(realBatch);      const [logitsFake, outputFake] = disFake(fakeBatch);      const lossReal = sigmoidCrossEntropyWithLogits(ONES_PRIME, logitsReal);      const lossFake = sigmoidCrossEntropyWithLogits(ZEROS, logitsFake);      return lossReal.add(lossFake).mean();    },    true,    [D1w, D1b, D2w, D2b, D3w, D3b]  );  await tf.nextFrame();  const gcost = gOptimizer.minimize(    () =&gt; {      const [logitsFake, outputFake] = disFake(fakeBatch);      const lossFake = sigmoidCrossEntropyWithLogits(ONES, logitsFake);      return lossFake.mean();    },    true,    [G1w, G1b, G2w, G2b, G3w, G3b]  );  await tf.nextFrame();  return [dcost, gcost];} 训练使用了两个optimizer，   第一步，计算实际数据的辨别结果和1的交叉熵，以及生成器生成数据的辨别结果和0的交叉熵。也就是说，我们希望辨别器尽可能的判断出生成数据都是假的而实际数据都是真的。使得这两个交叉熵的均值最小。  第二步开始对抗，要让生成数据尽可能被判别为真。  下图是某个训练过程的损失：  这个是经过1000个迭代后的生成图：  大家可以尝试调整学习率，增加网络复杂度，加大迭代次数来获得更好的生成模型。 GAN的学习其实还是比较复杂的，参数和损失选择都不容易，好在有一些现成的工具可以使用，另外推荐大家去https://poloclub.github.io/ganlab/，提供了很直观的GAN学习的过程。这个也是用TensorflowJS来实现的。 参考：   https://www.msra.cn/zh-cn/news/features/gan-20170511  https://zhuanlan.zhihu.com/p/24767059  http://blog.aylien.com/introduction-generative-adversarial-networks-code-tensorflow/  https://github.com/carpedm20/DCGAN-tensorflow  https://blog.openai.com/generative-models/  https://zhuanlan.zhihu.com/p/45200767  https://blog.csdn.net/heyc861221/article/details/80127148</div>
                    </div>
    
        <div class='blog'>
            <h2><a href="#top" class='top'>回到顶部</a><a name="blog_2990742" href="https://my.oschina.net/taogang/blog/2990742">重构机器学习算法的知识体系 - 《终极算法》读书笔记</a></h2>
            <div class='outline'>
                <div class='date'>时间：2018-12-20 05:31:15</div>
                <div class='catalog'>分类：机器学习</div>
                                                                            </div>
            <div class='content'>最近有幸从图书馆借阅了Pedro Domingos的《The Master Alogrithm》一书，这本书的中文翻译叫《终极算法》，台湾版本叫《大演算》。英文原版的豆瓣评分是8.4，而中文翻译只有7.2。豆瓣用户对于该书的中文翻译吐槽不少，我没有读中文版本，但是对于原版，我个人认为非常值得阅读，无论你是否工作在机器学习领域，本书都能带给你足够的启发。在之前学习机器学习的算法的时候，我经常有很多的疑惑，为什么机器学习的算法这么多，虽然整体上可以按照，有监督，无监督，回归，分类，聚类，降维等来归类算法，但是同一大类中算法差别又比较大，感觉体系结构混乱而嘈杂。多明戈斯把机器学习的算法分为五个大的部落，系统的阐述有这五个部落的思想和之间的联系和脉络。对于我而言就像吹散了迷雾，帮助我重构了我的机器学习算法的知识体系 。同时本书没有深奥的数学公式，非常适合对数学不太感冒，又希望了解机器学习的读者。这里献上我的读书笔记。序章在序章中，作者提出了机器学习算法的五大部落：符号主义：把学习看成是基于哲学，心理学和逻辑的逆向演绎。连接主义：受神经科学和物理学的启发，对人类大脑进行逆向工程。进化主义：引用生物科学和基因科学，在计算机上模拟进化贝叶斯主义：认为学习是基于统计学的概率推理的一种形式类比主义：受心理学和数学优化的影响，通过推测证据之间的类似性来学习。第一章 机器学习的变革我们生活在一个算法的时代，算法面临最大的挑战是复杂性（这里推荐一本书 《Complexity》，豆瓣评分9.1），无论是时间还是空间的复杂度。机器学习和普通算法的区别在于，学习算法是用来生成算法的算法。机器学习就像种植，算法是种子，数据是土壤，学习的过程就是庄稼生长的过程，而我们这些写算法的程序猿果不其然就是码农了。同时，机器学习又是我们手中的利剑，用于杀戮复杂性怪兽。工业革命使得手工劳动自动化，而信息革命使得大脑的工作自动化。而机器学习自动化自动化本身，如果做不到这一点，程序猿就会是系统的瓶颈。那么问题来了，程序猿为什么要和自己过不去？成为瓶颈不好么？第二章 终极算法在这一章中，作者提出了本书的基本假说：所有的知识，无论过去，当下和未来，都可以利用某个单一，通用的学习算法中从数据中获取。作者称其为终极算法或者主算法 -&nbsp;Master Algorithm有以下的的论点来证明终极算法的存在：从神经科学上来说，通过实验发现，科学家们通过改变神经元的“接受域”或它对应的视觉域触发了可塑性。从进化论的角度来说，生命的无限多种形式，都是一个简单机制，自然选择的结果。从物理学的角度来看，物理学由简单的数学公式主导。从统计学的角度，贝叶斯理论可以把数据变成知识。从计算机科学的角度，计算机本身的存在是一个很好的迹象表明存在终极算法。在这一章中，作者正式引入了五个部落的概念，而终极算法必须能够有效的同时解决这五个问题。第三章 休谟的归纳推理哲学中的经典问题是关于理性主义和经验主义，理性主义者喜欢在行动之前把所有的计划都做好，而经验主义者通过不断的试错来找到方向。对于程序猿来说这就像是传统的瀑布流式软件工程之于敏捷式的开发。大卫休谟是经验主义哲学家，休谟的怀疑论实际上是对于“因果性”的怀疑，休谟想要说明的是：相关性无法保证因果关系。我们看到一个东西，但是我们无法通过这个东西证明一个普遍存在的真理。英国哲学家、数学家、思想家伯特兰·罗素提出过一个著名的火鸡问题（Russell's Turkey）：在火鸡饲养场里，一只火鸡发现，每天上午9点钟主人给它喂食。它并不马上做出结论，而是慢慢观察，一直收集了有关上午9点给它喂食这一事实的大量观察证据：雨天和晴天，热天和冷天，星期三和星期四，各种各样的情况。最后，它得出了下面的结论：“主人总是在上午9点钟给我喂食。”可是，事情并不像它所想象的那样简单和乐观：在圣诞节前一天的9点，主人没有给它喂食，而是把它宰杀。在休谟之后的250年，“没有免费的午餐”定理由美国斯坦福大学的Wolpert和Macready教授提出，简称NFL定理（和美国橄榄球联盟并没有鸟关系）。 在机器学习算法中的体现为在没有实际背景下，没有一种算法比随机胡猜的效果好。该定律类似帕斯卡的赌注，帕斯卡假定所有人类对上帝存在或不存在下注。由于上帝可能确实存在，并假设这情况下信者和不信者会分别得无限的收益或损失，一个理性的人应该相信上帝存在。如果上帝实际上不存在，这样信者将只会有限的损失；而如果上帝存在，信者得永生。但实际上这个赌注你可以把上帝换成任意的神，安拉，土地，白骨精，效果是一样的。把‘上帝’换成‘学习算法’，把‘永生’换成‘精准预测’就是NFL定律了。Tom Mitchell, 是机器学习领域符号主义的领袖,他称之为“无用的无偏见学习”。在我们的日常生活中，‘偏见’是个贬义词，然而在机器学习中‘偏见’是非常重要的，没有它你将无法学习。实际上在人类的认知中，我们也一直在使用偏见。我们称这种偏见为“知识”。我曾经在一家存储公司开发一种基于规则的引擎，该组件的核心是一组组的规则，每一个规则可以简化为一个条件和一个操作，也就是再满足某个条件的时候做某个事情。基于规则的学习也是类似的，它给出在在某个数据输入条件下的输出是什么。一个好的学习算法就像是在幻觉（overfit）和失明（underfit）之间左右摇摆。人类也不能免疫overfitting。阿里士多德认为力使物体保持运动就是一个overfitting的例子。算法学习是你的数据量和你的假说的数量之间的竞赛，更多数据可以有效的减少假说，但是如果你一开始就有很多假说，很有可能最后还是会留下来一些不太好的假说。这里我觉得可以参考奥卡姆剃刀，我的另一篇博客。但是在机器学习里，该假说会有误导。更简单的理论也许是underfitting。归纳是逆向演绎。对于符号主义的学习算法的典型是决策树。决策树确保每一个实例都有唯一一个对应的规则。如果某位男士想追求一名倾心的女生，已经有过几次约会记录，但并不是次次都成功，他面临一个不知所措的局面，继续约不约，怎么约？当然，影响能否约会成功的因素：周末与非周末、吃饭或泡吧、天气冷或暖、电视节目好看或不好看，也就是有2*2*2*2=16种可能性，导致她说“yes” or “no”的不同结果。这就需要策略，需要一棵树，一棵决策树。符号主义认为所有的智能都可以通过符号的方式来推导。早期的人工智能使用被称作基于知识的系统，曾经迅速发展，但是很快就遇到了瓶颈。因为从专家身上抽取知识成为规则实在是太难了，即耗时耗力，又容易出错。尽管决策树很流行，逆向演绎更接近终极算法。逆向演绎就像一位超级科学家系统的检查所有的证据，思考可能的归纳假说，并用新的证据来构建进一步的假说。当然所有的这一切都是由计算机来完成。以符号主义这的角度来看它简单而优雅。但是它的问题是，可能的归纳假说数量实在太大了，而且实际问题大多是不是非黑即白的问题，有很多的灰色地带，这时候规则也就不那么管用了。连接主义对符号主义就非常不满，根据他们的观点，能通过逻辑规则定义的概念仅仅是冰山一角，表面之下还有很多的东西是推理无法完成的。第四章 大脑如何学习连接主义认为知识存在于神经元之间的连接中。不想符号主义认为的概念和符号存在一对一的映射，在连接主义中，概念和神经元是多对多的关系，每一个概念分布在多个神经元连接中，而每一个神经元也要参与构建多个概念。另一个区别，在符号主义中学习是穿行的，而在连接主义中，学习是并行的。人类的大脑可以进行大量的并行运算，数十亿的神经元同时工作，但是每一个都很慢，因为神经元每秒只能触发上千次。（爱因斯坦，牛顿们可能超过频，会快一些）1943年，心理学家Warren McCulloch和数理逻辑学家Walter Pitts在合作的《A logical calculus of the ideas immanent in nervous activity》论文中提出并给出了人工神经网络的概念及人工神经元的数学模型，从而开创了人工神经网络研究的时代。感知器是Frank Rosenblatt在1957年所发明的一种人工神经网络。它可以被视为一种最简单形式的前馈神经网络，是一种二元线性分类器。我们看到上图中的感知器和符号主义的规则是不是有一点点像，它是一个由神经元对于输入数据求和再由激活函数判定输出的数学规则。但是感知器不能解决简单的异或（XOR）等线性不可分问题。不像逻辑门，神经元更像是电压频率转换器。可以用S曲线来描述：我们生活中的许多现象都可以用S曲线来描述，当我们面对指数增长的时候，你需要想一想，他什么时候会变成S曲线，想一想统治IT多年的摩尔定律吧。通过反向传播算法，连接主义迎来了他们的春天，多层感知器可以有效的进行学习了。但是这里有一个致命的问题，就是局部的最小值。在高维空间中，学习很容易陷入局部最小值。虽然很多时候，这个值并不算太坏。而更严重的问题是，符号主义的规则和推理是我们人类很容易理解的，然而鬼才知道这个神经网络究竟在干些什么。没有人能偶解释它。神经网络的调优更像是在碰运气。第五章 进化，大自然的学习算法进化主义的学习算法是遗传算法，它是计算数学中用于解决最优化的搜索算法，最初是借鉴了进化生物学中的一些现象而发展起来的，这些现象包括遗传、突变、自然选择以及杂交等。遗传算法的核心是杂交和变异：当遗传算法达到局部的极值的时候，他会停留在那里很长一段时间，但是由于变异和杂交的存在，再加上一定运气，他有机会越过局部极值达到全局最优解。相比较神经网络的调优，这里程序猿只需静静的祈祷，等着变异就好了。在神经网络中学习的是各个神经元的权重，而网络结构是固定的，而遗传算法能够通过进化来学习结构，这个是连接主义做不到的。终极算法既不是遗传算法也不是反向传播，但是它必须包含这两个关键元素，结构的学习和权重的学习。连接主义和进化主义都是对自然的模拟，通过模拟大脑或者自然选择来进行学习。然而符号主义和贝叶斯主义不满足于并不完美的自然，大脑经常会做出错误的决定而，大自然也不总是给出最优解。这就是科学和哲学的冲突，科学追求的是真理，“这就是真想”，而哲学给出的是方法，“就该这么搞”。第六章 在神圣的贝叶斯教堂里对于贝叶斯主义者来说，学习只不过是贝叶斯定理的另一个应用，把模型看成是假说，把数据看成是证据：你得到越多的数据，某些模型就变得更可能为真，直到某一个模型明显胜出。在拉普拉斯的一个思想实验中，如果太阳过去升起n天，那么明天太阳照样升起的概率是(n+1)/(n+2)。过度悲观的人和过度乐观的人都得不到这个数。前者不信均匀分布的假设，我想他们得到的概率下限该是1/2。而后者不相信整个概率模型的假设，因此得到概率为1。朴素贝叶斯其实和感知器算法紧密关联。感知器对权重求和而朴素贝叶斯对概率求积，但是乘法做对数运算就是加法。所有这两个其实可以看成是如果-那么的规则模型。隐马尔可夫模型（Hidden Markov Model，HMM）是统计模型，它用来描述一个含有隐含未知参数的马尔可夫过程。其难点是从可观察的参数中确定该过程的隐含参数。然后利用这些参数来作进一步的分析。当状态和观察都是连续变量的时候，HMM就变成了卡曼滤波器。贝叶斯网络(Bayesian network)，又称信念网络(Belief Network)，或有向无环图模型(directed acyclic graphical model)，是一种概率图模型，于1985年由Judea Pearl首先提出。它是一种模拟人类推理过程中因果关系的不确定性处理模型，其网络拓朴结构是一个有向无环图(DAG)。我们将有因果关系（或非条件独立）的变量或命题用箭头来连接（换言之，连接两个节点的箭头代表此两个随机变量是具有因果关系，或非条件独立）。若两个节点间以一个单箭头连接在一起，表示其中一个节点是“因(parents)”，另一个是“果(children)”，两节点就会产生一个条件概率值。 例如，假设节点E直接影响到节点H，即E→H，则用从E指向H的箭头建立结点E到结点H的有向弧(E,H)，权值(即连接强度)用条件概率P(H|E)来表示，如下图所示：把某个研究系统中涉及的随机变量，根据是否条件独立绘制在一个有向图中，就形成了贝叶斯网络。朴素贝叶斯，马尔可夫链和隐马尔可夫模型都可以看成是贝叶斯网络的特例。对贝叶斯主义者来说，并不存在真相，你可以有一个对于假说的先验分布，验证数据后变为后验分布，仅此而已。在以上的理论中，学习都是又一个明确的模型，可以是一组规则，多层感知器，基因算法，或者贝叶斯网络。但是他们都有一个限制，当数据量非常小的时候，他们都无法很好的工作。然而类比主义可以从很少的数据中学习，因为他不需要建立模型。第七章 像什么就是什么类比主义的算法核心就是基于已有的数据判定输入数据和已有数据最接近，或者说最像的分类。近邻算法是最简单也最高效的学习算法。对于近邻算法而言，其实并没有一个学习的过程，因为没有模型，数据来了直接算相似度就好了，这也是类比主义和其它几个部落的一个大的区别。近邻算法的一个问题就是如果有很多不相关的特性，分类会很糟糕因为算法会找和这些不相关特性的相似处。另外在高维空间，计算相似会变得很困难。实际上，没有一个学习算法可以免疫高维度带来的诅咒，维度灾难是机器学习中仅次于过拟合（overfitting）大问题。支持向量机使用支持向量和权重来产生最大可能的边界。如下图所示，找到一个超平面使得Margin最大。SVM其实也可以看成是感知器，但是它相比较多层感知器的主要优点是SVM的权重有唯一的最优解，而不像神经网络存在许多的局部最优解。但是神经网络有更丰富的表现能力。类比主义的一个问题是，两个事物相像，并不一定表明它们就是一个类别。从分子结构看，甲烷（也就是沼气）和甲醇就差了一个氧原子。然而这两种分子的物理和化学特性都截然不同。所有我们之前提到的算法都需要老师来教它们，它们并不能自发的学习。最于终极算法而言，这可不行。第八章 无师自通这一章中，作者从无监督学习的聚类和主成分分析开讲。这两个都是属于无师自通型的算法。当你需要学习一个统计模型但是又缺乏关键信息的时候，你可以使用EM，最大似然估计。K均值算法和EM算法有些类似。它们都是把实体分配到某一个聚类，然后更新聚类的属性。实际上，K均值是EM算法在所有的特性都符合正态分布时的一个特例。之前的算法有一个问题就是计算最大回报，然而在现实中，我们并以一定每一步都需要最大回报来得到最终的胜利，有时候倒车是为了更好的前进。这里就需要引入强化学习。第九章 所有的拼图一目了然在这一章里，作者希望像秦始皇同一六国一样统一机器学习所有的五个部落。许多重要的科技都是因为发明了统一的机制，例如电作为统一的能源，互联网作为统一的通信机制。物理学家一直在找寻大一统的理论统一所有的力。作者也希望终极算法能统一所有的学习算法。把不同的算法合并为一个并不难，我们称之为元学习。现在的 AI 系统可以通过大量时间和经验从头学习一项复杂技能。但是，我们如果想使智能体掌握多种技能、适应多种环境，则不应该从头开始在每一个环境中训练每一项技能，而是需要智能体通过对以往经验的再利用来学习如何学习多项新任务，因此我们不应该独立地训练每一个新任务。这种学习如何学习的方法，是通往可持续学习多项新任务的多面智能体的必经之路。引导聚集算法（Bagging）通过对训练集随机采样，并应用在同一个学习算法上，然后通过投票表决来决定最终结果。另一个元学习算法是Boosting，不同于合并不同的学习器，Boosting重复在数据上应用同一分类器，然后通过在训练集上分配不同的权重，用新的模型来纠正之前的错误，每一个被错误分类的样本都会被增加权重。&nbsp;最终，如上图所示，所有的部落都可以用三个关键元素来描述，表示层，评估单元（代价Cost）和优化器。表示层定义学习器如何表示模型，对于符号主义来说是逻辑和规则，对于连接主义是神经网络，对于进化主义是基因算法，对于贝叶斯是图模型（贝叶斯网络或者马尔可夫网络），对于类比主义来说是用于某些权重的特例（SVM）评估单元就是记分函数，用于评价一个模型究竟是好还是不好。对于符号主义来说是准确度或者信息增益。对于连接主义是连续误差，例如均方差。对于贝叶斯来说是后验概率。对于进化主义来说是适应度。对于类比主义来说是Margin或者距离。同时所有的部落都会考虑到一些通用的属性例如模型是否简洁。优化器是如何找到得分最高的模型，并返回该模型的算法。对于符号主义是逆向归纳。对于连接主义是梯度下降。对于进化主义是基因搜索包含突变和杂交。对于贝叶斯来说有些特殊，它并不搜索最佳模型，它使用概率推测计算所有模型的概率。对于类比主义使用有限制条件的优化方法。我们可以看出各个部落之间存在着这样或者那样的联系，每一个符号主义的规则可以看成是一个特殊的神经元，而SVM可以看成是一个感知器，进化主义的适应度和连接主义的均方差也并没有本质上的区别。第十章 建立在机器学习之上的世界最后一章其实非常有趣，作者展望了机器学习对人类社会的影响。我们在和不同的应用在交互，其实是在训练一个能够理解我们的模型，如果你讨厌google，facebook，那么你要做的就是拼了老命去点击那些你不喜欢的广告，因为这样google，facebook就会训练出一个错误的模型，哈哈！在不远的将来，没个人都会有自己的一个数字拷贝，它就是基于你的数据训练出的你的模型，它帮你选择购买什么样的产品，它帮你决定和谁约会，它帮你寻找合适的工作机会，它决定你早餐应该出什么，它决定你的病要吃什么药。随着机器能够做越来越多的工作，失业率将被就业率取代，国家富强的标志不是看你提高了多少就业，而是看你是不是应用了更多的科技来解放人力。It’s Difficult to Make Predictions, Especially About the FutureThe best way to predict the future is to invent it. -- Alan Kay总结如果你觉得本读书笔记内容凌乱，那绝对是我的错误，和原书无关，这是一本五星推荐的书，对于我而言，它就像是一串串珠，把凌乱的机器学习算法穿在一条线索上，这是一本我希望早点能读到的书，欢迎大家和我交流！参考Machine learning evolution (infographic)机器学习的五大流派Leveraging technology for a better tomorrow “machine learnig”【 机器学习 】机器学习的五岳剑派 （中文）"The Five Tribes of Machine Learning (And What You Can Learn from Each)," Pedro DomingosThe Five Tribes of Machine LearningMIT研究揭示大脑可塑性基本规则怎么理解 P 问题和 NP 问题？罗素的火鸡问题：互联网归纳法简化论述没有免费午餐定理（NFL）聊聊因果推理、反事实逻辑和Do-calculus拆解Ｉ《终极算法》符号学派如何通俗易懂地解释遗传算法？有什么例子？一文搞懂HMM（隐马尔可夫模型）隐马尔可夫模型HMM如何用通俗的语言解释卡尔曼滤波器？概率图模型之贝叶斯网络支持向量机(SVM)是什么意思？莫烦强化学习强化学习 (Reinforcement Learning)从零开始，了解元学习与模型无关的元学习，UC Berkeley提出一种可推广到各类任务的元学习方法</div>
                    </div>
    
        <div class='blog'>
            <h2><a href="#top" class='top'>回到顶部</a><a name="blog_3011686" href="https://my.oschina.net/taogang/blog/3011686">自动机器学习简述（AutoML）</a></h2>
            <div class='outline'>
                <div class='date'>时间：2019-02-19 02:46:21</div>
                <div class='catalog'>分类：机器学习</div>
                                                                            </div>
            <div class='content'>为什么需要自动机器学习对于机器学习的新用户而言，使用机器学习算法的一个主要的障碍就是算法的性能受许多的设计决策影响。随着深度学习的流行，工程师需要选择相应的神经网络架构，训练过程，正则化方法，超参数，等等，所有的这些都对算法的性能有很大的影响。于是深度学习工程师也被戏称为调参工程师。自动机器学习（AutoML）的目标就是使用自动化的数据驱动方式来做出上述的决策。用户只要提供数据，自动机器学习系统自动的决定最佳的方案。领域专家不再需要苦恼于学习各种机器学习的算法。自动机器学习不光包括大家熟知的算法选择，超参数优化，和神经网络架构搜索，还覆盖机器学习工作流的每一步：自动准备数据自动特征选择自动选择算法超参数优化自动流水线/工作流构建神经网络架构搜索自动模型选择和集成学习超参数优化&nbsp;Hyper-parameter Optimization学习器模型中一般有两类参数，一类是可以从数据中学习估计得到，还有一类参数时无法从数据中估计，只能靠人的经验进行设计指定，后者成为超参数。比如，支持向量机里面的C, Kernal, game；朴素贝叶斯里面的alpha等。超参数优化有很多方法：最常见的类型是黑盒优化 （black-box function optimization）。所谓黑盒优化，就是将决策网络当作是一个黑盒来进行优化，仅关心输入和输出，而忽略其内部机制。决策网络通常是可以参数化的，这时候我们进行优化首先要考虑的是收敛性。以下的几类方法都是属于黑盒优化：网格搜索 （grid search）Grid search大家都应该比较熟悉，是一种通过遍历给定的参数组合来优化模型表现的方法。网格搜索的问题是很容易发生维度灾难，优点是很容易并行。随机搜索 （random search）随机搜索是利用随机数求极小点而求得函数近似的最优解的方法。很多时候，随机搜索比网格搜索效果要更好，但是我们可以从上图看出，它们都不能保证找到最优解。贝叶斯优化贝叶斯优化是一种迭代的优化算法，包含两个主要的元素，输入数据假设的模型和一个采集函数用来来决定下一步要评估哪一个点。每一步迭代，都使用所有的观测数据fit模型，然后利用激活函数预测模型的概率分布，决定如何利用参数点，权衡是Explaoration还是Exploitation。相对于其它的黑盒优化算法，激活函数的计算量要少很多，这也是为什么贝叶斯优化被认为是更好的超参数调优的算法。黑盒优化的一些工具：hyperopthyperopt&nbsp;是一个Python库，可以用来寻找实数,离散值,条件维度等搜索空间的最佳值。Google VizierGoogle的内部的机器学习系统 Google Vizier能够利用迁移学习等技术自动优化其他机器学习系统的超参数advisorGoogle Vizier的开源实现。katib&nbsp;基于Kubernetes的超参数优化工具由于优化目标具有不连续、不可导等数学性质，所以一些搜索和非梯度优化算法被用来求解该问题，包括我们上面提到的这些黑盒算法。此类算法通过采样和对采样的评价进行搜索，往往需要大量对采样的评价才能获得比较好的结果。然而，在自动机器学习任务中评价往往通过 k 折交叉验证获得，在大数据集的机器学习任务上，获得一个评价的时间代价巨大。这也影响了优化算法在自动机器学习问题上的效果。所以一些减少评价代价的方法被提出来，其中多保真度优化（multi-fidelity methods）就是其中的一种。这里的技术包括：基于学习曲线来决定是否要提前终止训练，探索-利用困境（exploration exploitation）的多臂老虎机算法 （Multi-armed bandit）等等。另外还有一些研究是基于梯度下降的优化。超参数优化面临许多挑战：对于大规模的模型或者复杂的机器学习流水线而言，需要评估的空间规模非常大配置空间很复杂无法或者很难利用损失函数的梯度变化训练集合的规模太小很容易过拟合相关参考https://docs.aws.amazon.com/sagemaker/latest/dg/automatic-model-tuning.htmlhttp://numbbo.github.io/workshops/index.htmlhttps://www.blog.google/products/google-cloud/cloud-automl-making-ai-accessible-every-business/https://scikit-learn.org/stable/modules/grid_search.htmlhttps://yq.aliyun.com/articles/68266https://arxiv.org/abs/1807.02811http://gpss.cc/gpmc17/slides/LancasterMasterclass_1.pdfhttps://yq.aliyun.com/articles/661786https://blog.csdn.net/xbinworld/article/details/79372777元学习&nbsp;Meta Learning元学习也就是‘学习如何学习’，通过对现有的学习任务之间的性能差异进行系统的观测，然后学习已有的经验和元数据，用于更好的执行新的学习任务。这样做可以极大的该静机器学习流水线或者神经网络架构的设计，也可以用数据驱动的方式取代手工作坊似的算法工程工作。从某种意义上来说，元学习覆盖了超参数优化，因为元数据的学习包含了：超参数，流水线的构成，神经网络架构，模型构成，元特征等等。机器学习的算法我们又称为‘学习器’，学习器就是假定一个模型，该模型拥有很多未知参数，利用训练数据和优化算法来找到最适合这些训练数据的参数，生成一个新的算法，或者参数已知的模型，并利用该模型/算法来预测新的未知数据。如果说世界上只有一个模型，那么问题就简单了，问题是模型有很多，不同的模型拥有不同的超参数，我们往往还会把模型和算法组装在一起构成复合模型和机器学习的流水线，这个时候，我就需要知道解决不同的问题要构建那些不同的模型。元学习就在这个时候，我们可以把超参数，流水线，神经网络架构这些都看成是一个新的模型的未知参数，把不同学习任务的性能指标看成是输入数据，这样我们就可以利用优化算法来找到性能最好的那组参数。这个模式可以一直嵌套，也就是说，你可以有‘元元元学习‘，当然我希望你不要走得太远，找不到回来的路。元学习的方法包括：通过模型评估来学习通过任务的属性，元特征来学习以下列出了一些常见的元特征从现有的模型中学习，包括：迁移学习利用RNN在学习过程中修改自己的权重元学习的一个很大的挑战就是如果通过很少的训练数据来学习一个复杂的模型，这就是one-shot或者few-shot的问题。像人类的学习一样，每次学习无论成功失败，我们都收获一定的经验，人类很少从头学习。在构建自动学习的时候，我们也应该充分利用已有的每一次的学习经验，逐步的改进，使得新的学习更加有效。相关参考：https://en.wikipedia.org/wiki/Meta_learning_(computer_science)https://lilianweng.github.io/lil-log/2018/11/30/meta-learning.htmlhttps://towardsdatascience.com/whats-new-in-deep-learning-research-understanding-meta-learning-91fef1295660https://medium.com/huggingface/from-zero-to-research-an-introduction-to-meta-learning-8e16e677f78ahttps://bair.berkeley.edu/blog/2017/07/18/learning-to-learn/神经网络架构搜索 Neural Architecture Search提起AutoML，其实大多数人都是因为Google的AutoML系统才知道这个故事的。随着深度学习的流行，神经网络的架构变得越来越复杂，越来越多的手工工程也随之而来。神经网络架构搜索就是为了解决这个问题。NAS主要包含三个部分：搜索空间&nbsp;search space搜索策略 search strategy性能估计策略 performance estimation strategy相关参考https://medium.com/aifrontiers/understand-automl-and-neural-architecture-search-4260a0942116https://towardsdatascience.com/everything-you-need-to-know-about-automl-and-neural-architecture-search-8db1863682bfhttps://ai.googleblog.com/2017/11/automl-for-large-scale-image.htmlhttps://www.fast.ai/2018/07/16/auto-ml2/自动化特征工程自动化特征工程可以帮助数据科学家基于数据集自动创建能够最好的用于训练的特征。Featuretools是一个开源库，用来实现自动化特征工程。它是一个很好的工具，旨在加快特征生成的过程，从而让大家有更多的时间专注于构建机器学习模型的其他方面。换句话说，它使你的数据处于“等待机器学习”的状态。Featuretools程序包中的三个主要组件：实体（Entities）深度特征综合（Deep Feature Synthesis ，DFS）特征基元（Feature primitives）一个Entity可以视作是一个Pandas的数据框的表示，多个实体的集合称为Entityset。深度特征综合（DFS）与深度学习无关，不用担心。实际上，DFS是一种特征工程方法，是Featuretools的主干。它支持从单个或者多个数据框中构造新特征。DFS通过将特征基元应用于Entityset的实体关系来构造新特征。这些特征基元是手动生成特征时常用的方法。例如，基元“mean”将在聚合级别上找到变量的平均值。&nbsp;参考：https://towardsdatascience.com/automated-feature-engineering-in-python-99baf11cc219https://towardsdatascience.com/why-automated-feature-engineering-will-change-the-way-you-do-machine-learning-5c15bf188b96https://www.kaggle.com/willkoehrsen/automated-feature-engineering-tutorialhttps://medium.com/data-from-the-trenches/automatic-feature-engineering-an-event-driven-approach-b2ca09d166fhttps://medium.com/@rrfd/simple-automatic-feature-engineering-using-featuretools-in-python-for-classification-b1308040e183https://blog.datarobot.com/automated-feature-engineering其它自动机器学习工具集以下列出一些开源的自动机器学习工具空大家参考，选择。Auto-Sklearn&nbsp;AutoKerasTPOTH2O AutoML&nbsp;Python auto_ml参考：自动机器学习（AutoML）最新综述自动机器学习工具全景图：精选22种框架，解放炼丹师开源自动机器学习(AutoML)框架盘点揭秘 | 谷歌自动化机器学习真的那么神吗？分享一篇比较全面的AutoML综述</div>
                            <div class='commentList'>
                    <h3>评论列表</h3>
                                    </div>
                    </div>
    
        <div class='blog'>
            <h2><a href="#top" class='top'>回到顶部</a><a name="blog_3014119" href="https://my.oschina.net/taogang/blog/3014119">是时候把你的Python2应用迁移到Python3了</a></h2>
            <div class='outline'>
                <div class='date'>时间：2019-02-24 14:27:33</div>
                <div class='catalog'>分类：编程语言</div>
                                                                            </div>
            <div class='content'>到2020年一月1日，Python2.7将不再受到官方维护，小伙伴，程序猿，工程狮们，是时候将你们的Python2迁移到Python3了。因为距这一天只有10个月了！  许多的开源项目已经准备好离python2而去了：  上面的列表只是其中一些，包括了许多我常用的机器学习的库，Tensorflow，Pandas，Scikit-learn，Numpy等等，看看有没有你常用的呢？ Python2 VS Python3 那么我么就先来看看Python2/3的主要差异吧。 Python3引入了很多和Python2不兼容的关键字和功能，其中一些可以通过Python2内置的__future__模块来实现前向兼容，也就是说可以让你的Python2的代码在Python3的解释器中运行。如果你计划要支持Python3，那么你可以在你的Python2的代码中先使用该模块。 from __future__ import division 该模块支持的Python3的新特性如下的功能：   PEP 3105:&nbsp;Make print a function  PEP 238:&nbsp;Changing the Division Operator  PEP 3112:&nbsp;Bytes literals in Python 3000 （unicode）  PEP 328:&nbsp;Imports: Multi-Line and Absolute/Relative  print print从2的statement变成了3的一个函数调用，在Python3调用print，必须使用函数的方式来调用，这个可能是最广为人知的Python3的变化了。 # Python2 print is a statementprint 'Hello, World!'print('Hello, World!')print "text", ; print 'print more text on the same line' Hello, World!Hello, World!text print more text on the same line # Python3 print is a functionprint('Hello, World!')print("some text,", end="")print(' print more text on the same line') Hello, World!some text, print more text on the same line 除法运算 Python3除法运算的改动比较危险，因为改动的是计算的行为，语法上是完全一样的，也就是说，同样的代码在2和3的环境下可能会返回不同的结果。 #python2 Divisionprint '3 / 2 =', 3 / 2print '3 // 2 =', 3 // 2print '3 / 2.0 =', 3 / 2.0print '3 // 2.0 =', 3 // 2.0 3 / 2 = 13 // 2 = 13 / 2.0 = 1.53 // 2.0 = 1.0 # Python3 divisionprint('3 / 2 =', 3 / 2)print('3 // 2 =', 3 // 2)print('3 / 2.0 =', 3 / 2.0)print('3 // 2.0 =', 3 // 2.0) 3 / 2 = 1.53 // 2 = 13 / 2.0 = 1.53 // 2.0 = 1.0 unicode Python2的unicode，str，bytearray是三个不同的类型，而在3中，bytes和bytesarray都成为了类。Python3提供了对unicode的直接支持。 # Python2 unicode/str/bytearray are 3 typesprint type(unicode('this is like a python3 str type'))print type(b'byte type does not exist')print 'they are really' + b' the same'print type(bytearray(b'bytearray oddly does exist though')) &lt;type 'unicode'&gt;&lt;type 'str'&gt;they are really the same&lt;type 'bytearray'&gt; # Python3 bytes and bytearray are two classedprint(type(b' bytes for storing data'))print(type(bytearray(b'bytearrays')))print('strings are now utf-8 \u4F60\u597D\u4E16\u754C!') &lt;class 'bytes'&gt;&lt;class 'bytearray'&gt;strings are now utf-8 你好世界! import Python2和Python3对于import的主要区别在于：   Python2默认相对路径import，Python3默认绝对路径import  Python2需要在文件夹下创建&nbsp;__init__.py文件才能把这个文件目录作为包来导入。在Python3.3以后，所有的目录都被看作是包，而无需__init__.py文件。  Python 2, 支持在函数哪调用&nbsp;from &lt;module&gt; import *&nbsp;. Python 3, 像&nbsp;from &lt;module&gt; import *&nbsp;这样的语法只能在模块级别调用，不能在函数内调用  range and xrange Python2 range返回一个list，而Python3 range返回一个range类的对象。 # Python2 rangeprint range(0,10,1)print type(range(0,10,1)) [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]&lt;type 'list'&gt; # Python3 range return a objectprint(range(0,10,1))print(type(range(0,10,1)))print(list(range(0,10,1))) range(0, 10)&lt;class 'range'&gt;[0, 1, 2, 3, 4, 5, 6, 7, 8, 9] xrange在Python2中经常被使用，我们知道xrange是一个生成器。因为在3中range的行为和xrange一样，所以xrange在Python3中被delete了。 迭代器的next()方法 在Python2中，可以调用next函数或者迭代器对象的.next()方法，在Python3中，.next()方法被删除了。 # Python2 nextmy_generator = (letter for letter in 'abcdefg')print(next(my_generator))print my_generator.next() ab # Python3 nextmy_generator = (letter for letter in 'abcdefg')print(next(my_generator)) a For-loop variables 看栗子 # Python2 Loop i = 1print 'before: i =', iprint 'comprehension: ', [i for i in range(5)]print 'after: i =', i before: i = 1comprehension:  [0, 1, 2, 3, 4]after: i = 4 在Python2中，列推导表达式和它之外的代码拥有相同的命名空间，也就是说，for loop中的变量i和之前的全局变量i其实是一个。这个可能是一个危险的行为。 # Python3 loop i = 1print('before: i =', i)print('comprehension:', [i for i in range(5)])print('after: i =', i) before: i = 1comprehension: [0, 1, 2, 3, 4]after: i = 1 Python3的列推导表达式拥有独立的命名空间，不会污染到全局变量。 比较不可比较的类型 Python2，你可以对任何类型进行比较 # Python2 unordered typeprint "[1, 2] &gt; 'foo' = ", [1, 2] &gt; 'foo'print "(1, 2) &gt; 'foo' = ", (1, 2) &gt; 'foo'print "[1, 2] &gt; (1, 2) = ", [1, 2] &gt; (1, 2) [1, 2] &gt; 'foo' =  False(1, 2) &gt; 'foo' =  True[1, 2] &gt; (1, 2) =  False 鬼才知道为什么由1/2组成的列表比‘foo’要大，而同样的集合就小。Python3会返回Type Error TypeError: unorderable types: list() &gt; str() &nbsp; 四舍五入 Python2 采用的是四舍五入 # Python2 roundingprint round(15.5)print round(16.5) 16.017.0 Python3采用国际最新的“银行家规则”，该规则并不是简单“四舍六入五取偶”，而是只有在精确的 0.5 的情况下才会取偶，否则还是要四舍五入的。 # Python3 banker's roundingprint(round(15.5))print(round(16.5)) 1616 Classes IPython&nbsp;2 支持两种类型的类 “old-style” 和&nbsp;“new-style”或者“classic-style”. 而Python3只支持“new-style”&nbsp; Type Annotation 类型标注 Python2的类型标注是利用注释来实现的 # Python2 Type Hintdef embezzle(self, account, funds=1000000, *fake_receipts):    # type: (str, int, *str) -&gt; None    """Embezzle funds from account using fake receipts."""    &lt;code goes here&gt; Python3.5开始支持原生的typing # Python3 Typingdef embezzle(self, account: str, funds :int =1000000, *fake_receipts : *str): -&gt; None    """Embezzle funds from account using fake receipts."""    &lt;code goes here&gt; 以上是一些主要和常见的差异，更多的细节大家可以在后面的参考中去找。 迁移指南 当你决定要迁移到Python3，那么你要做的是：   确定你的所有的依赖支持Python3  运行测试，确保你的所有的代码都能通过测试  进行代码迁移，这里你可能会用到一些工具       six&nbsp;Python2 和 3 兼容库    利用&nbsp;futurize&nbsp;或者&nbsp;modernize&nbsp; Futurize就是在Python2中使用未来的Python3的功能，而Modernize正相反，使用Python2/3的兼容子集，利用six来提供兼容性。你可以选择任何一种方式。    使用pylint&nbsp;来分析代码中和Python3不兼容的地方。      当你组件迁移到Python3，你可能会使用tox来保证兼容2，tox支持创建不同的python环境来进行测试。  开始使用类型检查  我们组的小伙伴已经兴高采烈，迫不及待的把代码都赶到Python3的一边了，你有没有计划呢？希望这篇文章能够帮助你下个决定吧！ 参考   本文的Python2演示代码  本文的Python3演示代码  选择一个 Python 解释器（3 vs 2）  What’s New In Python 3.0  The Conservative Python 3 Porting Guide  Language differences and workarounds  10 awesome features of Python that you can't use because you refuse to upgrade to Python 3  “不科学”的“四舍五入” ？  &nbsp;</div>
                    </div>
    
        <div class='blog'>
            <h2><a href="#top" class='top'>回到顶部</a><a name="blog_3023996" href="https://my.oschina.net/taogang/blog/3023996">在浏览器中进行深度学习：TensorFlow.js (九）训练词向量 Word Embedding</a></h2>
            <div class='outline'>
                <div class='date'>时间：2019-03-18 15:41:54</div>
                <div class='catalog'>分类：机器学习</div>
                                                                            </div>
            <div class='content'>词向量，英文名叫Word Embedding，在自然语言处理中，用于抽取语言模型中的特征，简单来说，就是把单词用一个向量来表示。最著名的Word Embedding模型应该是托马斯·米科洛夫（Tomas Mikolov）在Google带领的研究团队创造的Word2vec。 词向量的训练原理就是为了构建一个语言模型，我们假定一个词的出现概率是由它的上下问来决定的，那么我们找来很多的语素来训练这个模型，也就是通过上下文来预测某个词语出现的概率。  如上图所示，词嵌入向量的训练主要有两种模式：   连续词袋 CBOW， 在这个方法中，我们用出现在该单词的上下文的词来预测该单词出现的概率，如上图就是该单词的前两个和后两个。然后我们可以扫描全部的训练语素（所有的句子），对于每一次出现的词都找到对于的上下文的4个词，这样我们就可以构建一个训练集合来训练词向量了。  Skip-Gram和CBOW正好相反，它是用该单词来预测前后的4个上下文的单词。注意这里和上面的4个都是例子，你可以选择上下文的长度。  那么训练出来的词向量它的含义是什么呢？  词向量是该单词映射到一个n维空间的表示，首先，所有的单词只有在表示为数学上的向量后在能参与神经网络的运算，其次，单词在空间中的位置反映了词与词之间的关系，距离相近的词可能意味着它们有相近的含义，或者经常一出现。 用神经网络构建语言模型的时候，Embedding常常是作为第一个层出现的，它就是从文本中提取数字化的特征。那么我们今天就看看如何利用TensorflowJS在训练一个词向量嵌入模型吧。 倒入文本 首先倒入我的文本，这里我的文本很简单，你可以替换任何你想要训练的文本 const sentence = "Mary and Samantha arrived at the bus station early but waited until noon for the bus."; 抽取单词和编码 然后，抽取文本中所有的单词序列，在自然语言处理中，Tokenize是意味着把文本变成序列，我这个例子中的单词的抽取用了很简单的regular expression，实际的应用中，你可以使用不同的自然语言处理库提供的Tokenize方法。TensorflowJS中并没有提供Tokenize的方法。（Tensorflow 中由提供 https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/keras/preprocessing/text/Tokenizer） const tokenize = words =&gt; {  return words.match(/[^\s\.]+/g);}// tokenizeconst tokens = tokenize(sentence); 单词序列如下： ["Mary", "and", "Samantha", "arrived", "at", "the", "bus", "station", "early", "but", "waited", "until", "noon", "for", "the", "bus"] 下一步我们要对所有的单词编码，也就是用数字来表示每一个单词 const encode = tokens =&gt; {  let encoding_map = {};  let decoding_map = {};  let index = 0;  tokens.map( token =&gt; {    if( !encoding_map.hasOwnProperty(token) ) {      const pair = {};      const unpair = {};      pair[token] = index;      unpair[index] = token;      encoding_map = {...encoding_map, ...pair};       decoding_map = {...decoding_map, ...unpair};       index++;    }  })  return {    map: encoding_map,    count: index,    encode: function(word) {      return encoding_map[word];    },    decode: function(index) {      return decoding_map[index];    }  };}const encoding = encode(tokens);const vocab_size = encoding.count; 编码的方式很简单，我们统计每一个出现的单词，然后给每一个单词一个对应的数字。我们使用了两个map，一个存放从单词到数字索引的映射，另一个存放相反的从索引到单词的映射。这个例子中，一种出现了14个单词，那么索引的数字就是从0到13。 准备训练数据 下一步，我们来准备训练数据： const to_one_hot = (index, size) =&gt; {  return tf.oneHot(index, size);} // training dataconst data = [];const window_size = 2 + 1;for ( let i = 0; i &lt; tokens.length; i ++ ) {  const token = tokens[i];  for ( let j = i - window_size; j &lt; i + window_size; j ++) {    if ( j &gt;= 0 &amp;&amp; j !=i &amp;&amp; j &lt; tokens.length) {      data.push( [ token, tokens[j]] )    }  }}const x_train_data = [];const y_train_data = [];data.map( pair =&gt; {  x = to_one_hot(encoding.encode(pair[0]), vocab_size);  y = to_one_hot(encoding.encode(pair[1]), vocab_size);  x_train_data.push(x);  y_train_data.push(y);})  const x_train = tf.stack(x_train_data);const y_train = tf.stack(y_train_data);console.log(x_train.shape);console.log(y_train.shape); one_hot encoding是一种常用的编码方式，例如，对于索引为2的单词，它的one_hot encoding 就是[0,0,1&nbsp;.... 0], 就是索引位是1其它都是0 的向量，向量的长度和所有单词的数量相等。这里我们定义的上下文滑动窗口的大小为2，对于每一个词，找到它的前后出现的4个单词构成4对，用该词作为训练的输入，上下文的四个词作为目标。（注意在文首尾出的词上下文不足四个） 0: (2) ["Mary", "and"]1: (2) ["Mary", "Samantha"]2: (2) ["and", "Mary"]3: (2) ["and", "Samantha"]4: (2) ["and", "arrived"]5: (2) ["Samantha", "Mary"]6: (2) ["Samantha", "and"]7: (2) ["Samantha", "arrived"]8: (2) ["Samantha", "at"]9: (2) ["arrived", "Mary"]10: (2) ["arrived", "and"]... ... 训练集合如上图所示，第一个词是训练的输入，第二次的训练的目标。我们这里采用的方法类似Skip-Gram，因为上下文是预测对象。 模型构建和训练 训练集合准备好，就可以用开始构建模型了。 const build_model = (input_size,output_size) =&gt; {  const model = tf.sequential();  model.add(tf.layers.dense({     units: 2, inputShape: output_size, name:'embedding'  }));  model.add(tf.layers.dense(    {units: output_size, kernelInitializer: 'varianceScaling', activation: 'softmax'}));  return model;}const model = build_model(vocab_size, vocab_size);  model.compile({  optimizer: tf.train.adam(),  loss: tf.losses.softmaxCrossEntropy,  metrics: ['accuracy'],}); 我们的模型很简单，是一个两层的神经网络，第一层就是我们要训练的嵌入层，第二层是一个激活函数为Softmax的Dense层。因为我们的目标是预测究竟是哪一个单词，其实就是一个分类问题。这里要注意得是我是用的嵌入层的unit是2，也就是说训练的向量的长度是2，实际用户可以选择任何长度的词向量空间，这里我用2是为了便于下面的词向量的可视化，省去了降维的操作。 训练的过程也很简单： const batchSize = 16;const epochs = 500;model.fit(x_train, y_train, {  batchSize,  epochs,  shuffle: true,}); 可视化词向量 训练完成后，我们可以利用该模型的embeding层来生成每一个单词的嵌入向量。然后在二维空间中展示。 // visualize embedding layerconst embedding_layer = model.getLayer('embedding');const vis_model = tf.sequential();vis_model.add(embedding_layer);const vis_result = vis_model.predict(predict_inputs).arraySync();console.log(vis_result);const viz_data = [];for ( let i = 0; i &lt; vocab_size; i ++ ) {  const word = encoding.decode(i);  const pos = vis_result[i];  console.log(word,pos);  viz_data.push( { label:word, x:pos[0], y :pos[1]});}const chart = new G2.Chart({  container: 'chart',  width: 600,  height: 600});chart.source(viz_data);chart.point().position('x*y').label('label');chart.render(); 生成的词向量的例子如下： "Mary" [-0.04273216053843498, -0.18541619181632996]"and" [0.09561611711978912, -0.29422900080680847]"Samantha" [0.08887559175491333, 0.019271137192845345]"arrived" [-0.47705259919166565, -0.024428391829133034] 可视化关系如下图：  &nbsp; 总结 词向量嵌入常常是自然语言处理的第一步操作，用于提取文本特征。我们演示了如何训练一个模型来构建词向量。当然实际操作中，你可以直接使用https://js.tensorflow.org/api/latest/#layers.embedding&nbsp;来构建你的文本模型，本文是为了演示词向量的基本原理。代码参见https://codepen.io/gangtao/full/jJqbQb &nbsp; 参考    在浏览器中进行深度学习：TensorFlow.js (八）生成对抗网络 （GAN）   在浏览器中进行深度学习：TensorFlow.js (七）递归神经网络 （RNN）   在浏览器中进行深度学习：TensorFlow.js (六）构建一个卷积网络 Convolutional Network    在浏览器中进行深度学习：TensorFlow.js (五）构建一个神经网络    在浏览器中进行深度学习：TensorFlow.js (四）用基本模型对MNIST数据进行识别    在浏览器中进行深度学习：TensorFlow.js (三）更多的基本模型    在浏览器中进行深度学习：TensorFlow.js (二）第一个模型，线性回归    在浏览器中进行深度学习：TensorFlow.js (一）基本概念</div>
                    </div>
    </div>
</body>
</html>
    

<!-- Generated by oschina (init:0[ms],page:272[ms],ip:184.94.121.185) //-->